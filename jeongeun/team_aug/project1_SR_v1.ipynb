{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1rJaVvestnBEi1RJwsmvsk0ZPrvQb8TD2","authorship_tag":"ABX9TyPzUxVkLDWk8ScZ3O+rOJRr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# nltk 다운로드 (처음 한 번만 실행)\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mXx8n2nFl0gG","executionInfo":{"status":"ok","timestamp":1727073315404,"user_tz":-540,"elapsed":1235,"user":{"displayName":"김정은","userId":"15845624418781515931"}},"outputId":"fcc8d565-7bb6-4238-8086-156575b31952"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uQW0xjpj4To"},"outputs":[],"source":["import pandas as pd\n","import random\n","from nltk import word_tokenize, pos_tag\n","from nltk.corpus import wordnet\n","\n","# CSV 파일 읽기\n","df = pd.read_csv('train.csv')\n","\n","def get_synonym(word):\n","    synonyms = []\n","    for syn in wordnet.synsets(word):\n","        for lemma in syn.lemmas():\n","            synonyms.append(lemma.name())\n","    return list(set(synonyms))\n","\n","def augment_sentence(sentence):\n","    words = word_tokenize(sentence)\n","    augmented_words = []\n","\n","    for word in words:\n","        if random.random() < 0.3:  # 30% 확률로 단어를 대체\n","            synonyms = get_synonym(word)\n","            if synonyms:\n","                augmented_words.append(random.choice(synonyms))\n","            else:\n","                augmented_words.append(word)\n","        else:\n","            augmented_words.append(word)\n","\n","    return ' '.join(augmented_words)\n","\n","# 데이터 증강\n","augmented_samples = df.copy()\n","augmented_samples['sentence_1'] = augmented_samples['sentence_1'].apply(augment_sentence)\n","augmented_samples['sentence_2'] = augmented_samples['sentence_2'].apply(augment_sentence)\n","\n","# 원본 데이터와 증강 데이터 결합\n","augmented_df = pd.concat([df, augmented_samples], ignore_index=True)\n","\n","# 새로운 CSV 파일로 저장\n","augmented_df.to_csv('SR_train_1.csv', index=False)"]},{"cell_type":"code","source":["import pandas as pd\n","import random\n","from nltk import word_tokenize, pos_tag\n","from nltk.corpus import wordnet\n","\n","# CSV 파일 읽기\n","df = pd.read_csv('dev.csv')\n","\n","def get_synonym(word):\n","    synonyms = []\n","    for syn in wordnet.synsets(word):\n","        for lemma in syn.lemmas():\n","            synonyms.append(lemma.name())\n","    return list(set(synonyms))\n","\n","def augment_sentence(sentence):\n","    words = word_tokenize(sentence)\n","    augmented_words = []\n","\n","    for word in words:\n","        if random.random() < 0.3:  # 30% 확률로 단어를 대체\n","            synonyms = get_synonym(word)\n","            if synonyms:\n","                augmented_words.append(random.choice(synonyms))\n","            else:\n","                augmented_words.append(word)\n","        else:\n","            augmented_words.append(word)\n","\n","    return ' '.join(augmented_words)\n","\n","# 데이터 증강\n","augmented_samples = df.copy()\n","augmented_samples['sentence_1'] = augmented_samples['sentence_1'].apply(augment_sentence)\n","augmented_samples['sentence_2'] = augmented_samples['sentence_2'].apply(augment_sentence)\n","\n","# 원본 데이터와 증강 데이터 결합\n","augmented_df = pd.concat([df, augmented_samples], ignore_index=True)\n","\n","# 새로운 CSV 파일로 저장\n","augmented_df.to_csv('SR_dev_1.csv', index=False)"],"metadata":{"id":"c2qg4oWpj8m2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import requests\n","\n","def get_papago_translation(text, source_lang='ko', target_lang='ko'):\n","    url = \"https://openapi.naver.com/v1/papago/n2mt\"\n","    headers = {\n","        \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n","        \"X-Naver-Client-Id\": \"YOUR_CLIENT_ID\",\n","        \"X-Naver-Client-Secret\": \"YOUR_CLIENT_SECRET\"\n","    }\n","    data = {\n","        \"source\": source_lang,\n","        \"target\": target_lang,\n","        \"text\": text\n","    }\n","    response = requests.post(url, headers=headers, data=data)\n","    if response.status_code == 200:\n","        return response.json().get('message').get('result').get('translatedText')\n","    else:\n","        return None\n","\n","# 예시 문장\n","text = \"행복하다\"\n","translated_text = get_papago_translation(text)\n","print(translated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0v5-uBKq-Azf","executionInfo":{"status":"ok","timestamp":1727081129309,"user_tz":-540,"elapsed":1041,"user":{"displayName":"김정은","userId":"15845624418781515931"}},"outputId":"20430ce6-cd17-4f53-9755-2e14145ae19b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["None\n"]}]},{"cell_type":"code","source":["import requests\n","\n","def kakao_translate(text, src_lang='kr', target_lang='kr'):\n","    # 카카오 API URL 및 헤더 설정\n","    url = \"https://kapi.kakao.com/v2/translation/translate\"\n","    headers = {\n","        \"Authorization\": \"KakaoAK YOUR_REST_API_KEY\"  # REST API 키 입력\n","    }\n","    data = {\n","        \"src_lang\": src_lang,\n","        \"target_lang\": target_lang,\n","        \"query\": text\n","    }\n","\n","    # POST 요청을 통해 API 호출\n","    response = requests.post(url, headers=headers, data=data)\n","\n","    if response.status_code == 200:\n","        result = response.json()\n","        translated_text = result['translated_text'][0][0]\n","        return translated_text\n","    else:\n","        return \"API 호출 실패: \" + str(response.status_code)\n","\n","# 예시 문장\n","text = \"나는 자동차를 운전하고 있다\"\n","translated_text = kakao_translate(text)\n","print(translated_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZflKQC9GEIPB","executionInfo":{"status":"ok","timestamp":1727081255841,"user_tz":-540,"elapsed":1096,"user":{"displayName":"김정은","userId":"15845624418781515931"}},"outputId":"f0bdbccc-1640-44bb-a0bf-c887d8baa8dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["API 호출 실패: 404\n"]}]},{"cell_type":"code","source":["pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PYRN-HSZPZU7","executionInfo":{"status":"ok","timestamp":1727084237379,"user_tz":-540,"elapsed":16238,"user":{"displayName":"김정은","userId":"15845624418781515931"}},"outputId":"71ea7c7b-4874-4058-c308-02abcb2aa432"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n","Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["import random\n","from konlpy.tag import Okt\n","from nltk.corpus import wordnet\n","\n","okt = Okt()\n","\n","# RD: Random Deletion\n","def random_deletion(words, p=0.1):\n","    if len(words) == 1:  # 단어가 하나뿐일 때는 삭제하지 않음\n","        return words\n","    return [word for word in words if random.uniform(0, 1) > p]\n","\n","# RS: Random Swap\n","def random_swap(words, n=1):\n","    words = words.copy()\n","    for _ in range(n):\n","        idx1, idx2 = random.sample(range(len(words)), 2)\n","        words[idx1], words[idx2] = words[idx2], words[idx1]\n","    return words\n","\n","# SR: Synonym Replacement\n","def synonym_replacement(words, n=1):\n","    new_words = words.copy()\n","    for _ in range(n):\n","        word_to_replace = random.choice(new_words)\n","        synonyms = get_synonyms(word_to_replace)\n","        if len(synonyms) > 0:\n","            synonym = random.choice(synonyms)\n","            new_words = [synonym if word == word_to_replace else word for word in new_words]\n","    return new_words\n","\n","def get_synonyms(word):\n","    # 예시로 고정된 동의어 목록을 사용합니다. (더 나은 방법은 지식 기반을 사용하는 것)\n","    synonyms_dict = {\n","        '간단한': ['쉬운', '간편한'],\n","        '예시': ['사례', '예'],\n","        '문장': ['문구', '문장'],\n","        '텍스트': ['글', '문서'],\n","        '증강': ['증대', '확장']\n","    }\n","    return synonyms_dict.get(word, [])\n","\n","# RI: Random Insertion\n","def random_insertion(words, n=1):\n","    new_words = words.copy()\n","    for _ in range(n):\n","        new_word = get_random_word(new_words)\n","        random_idx = random.randint(0, len(new_words))\n","        new_words.insert(random_idx, new_word)\n","    return new_words\n","\n","def get_random_word(words):\n","    random_word = random.choice(words)\n","    synonyms = get_synonyms(random_word)\n","    if len(synonyms) > 0:\n","        return random.choice(synonyms)\n","    else:\n","        return random_word\n","\n","# 테스트용 예시 문장\n","sentence = \"이것은 텍스트 증강을 위한 간단한 예시 문장입니다\".split()\n","\n","# 기법별 적용 예시\n","print(\"Original:\", sentence)\n","print(\"Random Deletion:\", random_deletion(sentence))\n","print(\"Random Swap:\", random_swap(sentence, n=2))\n","print(\"Synonym Replacement:\", synonym_replacement(sentence, n=2))\n","print(\"Random Insertion:\", random_insertion(sentence, n=2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pj_sLOTxFeUb","executionInfo":{"status":"ok","timestamp":1727084246182,"user_tz":-540,"elapsed":4676,"user":{"displayName":"김정은","userId":"15845624418781515931"}},"outputId":"14f1c026-7bdb-4662-d6eb-0e8b0a4a6fea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original: ['이것은', '텍스트', '증강을', '위한', '간단한', '예시', '문장입니다']\n","Random Deletion: ['이것은', '텍스트', '증강을', '위한', '예시', '문장입니다']\n","Random Swap: ['문장입니다', '위한', '증강을', '텍스트', '간단한', '예시', '이것은']\n","Synonym Replacement: ['이것은', '텍스트', '증강을', '위한', '간편한', '예시', '문장입니다']\n","Random Insertion: ['이것은', '문장입니다', '텍스트', '증강을', '위한', '간단한', '예시', '이것은', '문장입니다']\n"]}]},{"cell_type":"code","source":["import requests\n","\n","def get_synonyms_from_naver(word):\n","    client_id = 'YOUR_CLIENT_ID'\n","    client_secret = 'YOUR_CLIENT_SECRET'\n","    url = f'https://openapi.naver.com/v1/search/keyword.json?query={word}'\n","    headers = {\n","        'X-Naver-Client-Id': client_id,\n","        'X-Naver-Client-Secret': client_secret,\n","    }\n","    response = requests.get(url, headers=headers)\n","    return response.json()  # 응답 처리\n","\n","# 사용 예시\n","synonyms = get_synonyms_from_naver(\"간단한\")\n","print(synonyms)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3KnIS7RRP741","executionInfo":{"status":"ok","timestamp":1727084658478,"user_tz":-540,"elapsed":1201,"user":{"displayName":"김정은","userId":"15845624418781515931"}},"outputId":"4f762634-3578-4976-c52c-5a84038230d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'errorMessage': 'NID AUTH Result Invalid (1000) : Authentication failed. (인증에 실패했습니다.)', 'errorCode': '024'}\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('korean_wn')\n","\n","from nltk.corpus import wordnet as wn\n","\n","def get_korean_synonyms(word):\n","    synonyms = []\n","    for syn in wn.synsets(word, lang='kor'):\n","        for lemma in syn.lemmas(lang='kor'):\n","            synonyms.append(lemma.name())\n","    return list(set(synonyms))  # 중복 제거\n","\n","# 사용 예시\n","word = \"간단한\"\n","synonyms = get_korean_synonyms(word)\n","print(synonyms)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GBx43E-3RzCL","executionInfo":{"status":"error","timestamp":1727084851620,"user_tz":-540,"elapsed":301,"user":{"displayName":"김정은","userId":"15845624418781515931"}},"outputId":"136501a2-0ecf-4a20-c75c-7e5764590e52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Error loading korean_wn: Package 'korean_wn' not found in\n","[nltk_data]     index\n"]},{"output_type":"error","ename":"LookupError","evalue":"\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-a1cc89e1531e>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 사용 예시\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"간단한\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msynonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_korean_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynonyms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-a1cc89e1531e>\u001b[0m in \u001b[0;36mget_korean_synonyms\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_korean_synonyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msynonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msyn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0msynonyms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36msynsets\u001b[0;34m(self, lemma, pos, lang, check_exceptions)\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1765\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_lang_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1766\u001b[0m             \u001b[0msynset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_load_lang_data\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_omw_reader\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momw_langs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_omw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlangs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36madd_omw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_omw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_provs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_omw_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momw_langs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovenances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36madd_provs\u001b[0;34m(self, reader)\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_provs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;34m\"\"\"Add languages from Multilingual Wordnet to the provenance dictionary\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0mfileids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mprov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"]}]},{"cell_type":"code","source":["import random\n","import pickle\n","import re\n","\n","wordnet = {}\n","with open(\"wordnet.pickle\", \"rb\") as f:\n","\twordnet = pickle.load(f)\n","\n","\n","# 한글만 남기고 나머지는 삭제\n","def get_only_hangul(line):\n","\tparseText= re.compile('/ ^[ㄱ-ㅎㅏ-ㅣ가-힣]*$/').sub('',line)\n","\n","\treturn parseText\n","\n","\n","\n","########################################################################\n","# Synonym replacement\n","# Replace n words in the sentence with synonyms from wordnet\n","########################################################################\n","def synonym_replacement(words, n):\n","\tnew_words = words.copy()\n","\trandom_word_list = list(set([word for word in words]))\n","\trandom.shuffle(random_word_list)\n","\tnum_replaced = 0\n","\tfor random_word in random_word_list:\n","\t\tsynonyms = get_synonyms(random_word)\n","\t\tif len(synonyms) >= 1:\n","\t\t\tsynonym = random.choice(list(synonyms))\n","\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n","\t\t\tnum_replaced += 1\n","\t\tif num_replaced >= n:\n","\t\t\tbreak\n","\n","\tif len(new_words) != 0:\n","\t\tsentence = ' '.join(new_words)\n","\t\tnew_words = sentence.split(\" \")\n","\n","\telse:\n","\t\tnew_words = \"\"\n","\n","\treturn new_words\n","\n","\n","def get_synonyms(word):\n","\tsynomyms = []\n","\n","\ttry:\n","\t\tfor syn in wordnet[word]:\n","\t\t\tfor s in syn:\n","\t\t\t\tsynomyms.append(s)\n","\texcept:\n","\t\tpass\n","\n","\treturn synomyms\n","\n","########################################################################\n","# Random deletion\n","# Randomly delete words from the sentence with probability p\n","########################################################################\n","def random_deletion(words, p):\n","\tif len(words) == 1:\n","\t\treturn words\n","\n","\tnew_words = []\n","\tfor word in words:\n","\t\tr = random.uniform(0, 1)\n","\t\tif r > p:\n","\t\t\tnew_words.append(word)\n","\n","\tif len(new_words) == 0:\n","\t\trand_int = random.randint(0, len(words)-1)\n","\t\treturn [words[rand_int]]\n","\n","\treturn new_words\n","\n","########################################################################\n","# Random swap\n","# Randomly swap two words in the sentence n times\n","########################################################################\n","def random_swap(words, n):\n","\tnew_words = words.copy()\n","\tfor _ in range(n):\n","\t\tnew_words = swap_word(new_words)\n","\n","\treturn new_words\n","\n","def swap_word(new_words):\n","\trandom_idx_1 = random.randint(0, len(new_words)-1)\n","\trandom_idx_2 = random_idx_1\n","\tcounter = 0\n","\n","\twhile random_idx_2 == random_idx_1:\n","\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n","\t\tcounter += 1\n","\t\tif counter > 3:\n","\t\t\treturn new_words\n","\n","\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n","\treturn new_words\n","\n","########################################################################\n","# Random insertion\n","# Randomly insert n words into the sentence\n","########################################################################\n","def random_insertion(words, n):\n","\tnew_words = words.copy()\n","\tfor _ in range(n):\n","\t\tadd_word(new_words)\n","\n","\treturn new_words\n","\n","\n","def add_word(new_words):\n","\tsynonyms = []\n","\tcounter = 0\n","\twhile len(synonyms) < 1:\n","\t\tif len(new_words) >= 1:\n","\t\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n","\t\t\tsynonyms = get_synonyms(random_word)\n","\t\t\tcounter += 1\n","\t\telse:\n","\t\t\trandom_word = \"\"\n","\n","\t\tif counter >= 10:\n","\t\t\treturn\n","\n","\trandom_synonym = synonyms[0]\n","\trandom_idx = random.randint(0, len(new_words)-1)\n","\tnew_words.insert(random_idx, random_synonym)\n","\n","\n","\n","def augment_sentence(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n","\t# sentence = get_only_hangul(sentence)\n","\twords = sentence.split(' ')\n","\twords = [word for word in words if word is not \"\"]\n","\tnum_words = len(words)\n","\n","\taugmented_sentences = []\n","\tnum_new_per_technique = int(num_aug/4) + 1\n","\n","\tn_sr = max(1, int(alpha_sr*num_words))\n","\tn_ri = max(1, int(alpha_ri*num_words))\n","\tn_rs = max(1, int(alpha_rs*num_words))\n","\n","\t# sr\n","\tfor _ in range(num_new_per_technique):\n","\t\ta_words = synonym_replacement(words, n_sr)\n","\t\taugmented_sentences.append(' '.join(a_words))\n","\n","\t# ri\n","#\tfor _ in range(num_new_per_technique):\n","#\t\ta_words = random_insertion(words, n_ri)\n","#\t\taugmented_sentences.append(' '.join(a_words))\n","\n","\t# rs\n","#\tfor _ in range(num_new_per_technique):\n","#\t\ta_words = random_swap(words, n_rs)\n","#\t\taugmented_sentences.append(\" \".join(a_words))\n","\n","\t# rd\n","#\tfor _ in range(num_new_per_technique):\n","#\t\ta_words = random_deletion(words, p_rd)\n","#\t\taugmented_sentences.append(\" \".join(a_words))\n","\n","\taugmented_sentences = [get_only_hangul(sentence) for sentence in augmented_sentences]\n","\trandom.shuffle(augmented_sentences)\n","\n","\tif num_aug >= 1:\n","\t\taugmented_sentences = augmented_sentences[:num_aug]\n","\telse:\n","\t\tkeep_prob = num_aug / len(augmented_sentences)\n","\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n","\n","\taugmented_sentences.append(sentence)\n","\n","\treturn augmented_sentences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3xIL8qurUHfI","executionInfo":{"status":"ok","timestamp":1727086626062,"user_tz":-540,"elapsed":310,"user":{"displayName":"김정은","userId":"15845624418781515931"}},"outputId":"c613ab20-8b9f-459a-a765-aae087ed605f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:138: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n","<>:138: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n","<ipython-input-34-b8efcd2571b1>:138: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n","  words = [word for word in words if word is not \"\"]\n"]}]},{"cell_type":"code","source":["df = pd.read_csv('train.csv')\n","\n","augmented_samples = df.copy()\n","augmented_samples['sentence_1'] = augmented_samples['sentence_1'].apply(augment_sentence)\n","augmented_samples['sentence_2'] = augmented_samples['sentence_2'].apply(augment_sentence)\n","\n","augmented_samples.to_csv('SR_train_2.csv', index=False)"],"metadata":{"id":"9Y16fuikUWPP"},"execution_count":null,"outputs":[]}]}