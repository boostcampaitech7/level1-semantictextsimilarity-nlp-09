{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open train.csv as pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "valid = pd.read_csv('dev.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9324, 6), (550, 6), (1100, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[0] + valid.shape[0] + test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10974, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.concat([train, valid, test], ignore_index=True)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "source             0\n",
       "sentence_1         0\n",
       "sentence_2         0\n",
       "label           1100\n",
       "binary-label    1100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boostcamp-sts-v1-train-000</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~</td>\n",
       "      <td>반전도 있고,사랑도 있고재미도있네요.</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-train-001</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>앗 제가 접근권한이 없다고 뜹니다;;</td>\n",
       "      <td>오, 액세스 권한이 없다고 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boostcamp-sts-v1-train-002</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>주택청약조건 변경해주세요.</td>\n",
       "      <td>주택청약 무주택기준 변경해주세요.</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boostcamp-sts-v1-train-003</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>입사후 처음 대면으로 만나 반가웠습니다.</td>\n",
       "      <td>화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boostcamp-sts-v1-train-004</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>뿌듯뿌듯 하네요!!</td>\n",
       "      <td>꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id            source  \\\n",
       "0  boostcamp-sts-v1-train-000      nsmc-sampled   \n",
       "1  boostcamp-sts-v1-train-001         slack-rtt   \n",
       "2  boostcamp-sts-v1-train-002  petition-sampled   \n",
       "3  boostcamp-sts-v1-train-003     slack-sampled   \n",
       "4  boostcamp-sts-v1-train-004     slack-sampled   \n",
       "\n",
       "                               sentence_1                    sentence_2  \\\n",
       "0  스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~          반전도 있고,사랑도 있고재미도있네요.   \n",
       "1                    앗 제가 접근권한이 없다고 뜹니다;;           오, 액세스 권한이 없다고 합니다.   \n",
       "2                          주택청약조건 변경해주세요.            주택청약 무주택기준 변경해주세요.   \n",
       "3                  입사후 처음 대면으로 만나 반가웠습니다.  화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.   \n",
       "4                              뿌듯뿌듯 하네요!!         꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!   \n",
       "\n",
       "   label  binary-label  \n",
       "0    2.2           0.0  \n",
       "1    4.2           1.0  \n",
       "2    2.4           0.0  \n",
       "3    3.0           1.0  \n",
       "4    0.0           0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/miniconda3/envs/heejun-base/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/ephemeral/home/miniconda3/envs/heejun-base/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "tokenizing: 100%|██████████| 10974/10974 [00:06<00:00, 1744.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('klue/roberta-base', max_length=128)\n",
    "\n",
    "def tokenizing(self, dataframe):\n",
    "    data = []\n",
    "    tokens_data = []  # 토큰 데이터를 저장할 리스트\n",
    "    for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "        # 두 입력 문장을 [SEP] 토큰으로 이어붙여서 전처리합니다.\n",
    "        text = '[SEP]'.join([item[text_column] for text_column in ['sentence_1', 'sentence_2']])\n",
    "        outputs = tokenizer(text, add_special_tokens=True, padding='max_length', truncation=True)\n",
    "        data.append(outputs['input_ids'])\n",
    "        tokens = tokenizer.convert_ids_to_tokens(outputs['input_ids'])  # token id를 tokens로 변환\n",
    "        tokens_data.append(tokens)  # 토큰 리스트 추가\n",
    "    return data, tokens_data\n",
    "\n",
    "\n",
    "data, tokens = tokenizing(tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new column 'tokens' in the dataframe\n",
    "dataset['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10974, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boostcamp-sts-v1-train-000</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~</td>\n",
       "      <td>반전도 있고,사랑도 있고재미도있네요.</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], 스릴, ##도, ##있, ##고, 반전, ##도, 있, ##고, 여느...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-train-001</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>앗 제가 접근권한이 없다고 뜹니다;;</td>\n",
       "      <td>오, 액세스 권한이 없다고 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], 앗, 제, ##가, 접근, ##권, ##한, ##이, 없, ##다고,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boostcamp-sts-v1-train-002</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>주택청약조건 변경해주세요.</td>\n",
       "      <td>주택청약 무주택기준 변경해주세요.</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], 주택, ##청, ##약, ##조건, 변경, ##해, ##주, ##세요...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boostcamp-sts-v1-train-003</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>입사후 처음 대면으로 만나 반가웠습니다.</td>\n",
       "      <td>화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], 입사, ##후, 처음, 대면, ##으로, 만나, 반가웠, ##습, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boostcamp-sts-v1-train-004</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>뿌듯뿌듯 하네요!!</td>\n",
       "      <td>꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], 뿌듯, ##뿌, ##듯, 하, ##네, ##요, !, !, [SEP]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>boostcamp-sts-v1-train-005</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>오마이가뜨지져스크롸이스트휏</td>\n",
       "      <td>오 마이 갓 지저스 스크론 이스트 팬</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], [UNK], [SEP], 오, 마이, 갓, 지저, ##스, 스크, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>boostcamp-sts-v1-train-006</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>전 암만 찍어도 까만 하늘.. ㅠㅠ</td>\n",
       "      <td>암만 찍어도 하늘은 까맣다.. ㅠㅠ</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], 전, 암, ##만, 찍, ##어도, 까만, 하늘, ., ., ㅠㅠ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>boostcamp-sts-v1-train-007</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>이렇게 귀여운 쥐들은 처음이네요.ㅎㅎㅎ</td>\n",
       "      <td>이렇게 지겨운 공포영화는 처음..</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], 이렇게, 귀여운, 쥐, ##들, ##은, 처음, ##이, ##네, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boostcamp-sts-v1-train-008</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>미세먼지 해결이 가장 시급한 문제입니다!</td>\n",
       "      <td>가장 시급한 것이 신생아실 관리입니다!!!</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], 미세먼지, 해결, ##이, 가장, 시급, ##한, 문제, ##입니다,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>boostcamp-sts-v1-train-009</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>크림하우스 환불조치해주세요.</td>\n",
       "      <td>크림하우스 환불조치할 수 있도록해주세여</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], 크림, ##하우스, 환불, ##조치, ##해, ##주, ##세요, ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id            source  \\\n",
       "0  boostcamp-sts-v1-train-000      nsmc-sampled   \n",
       "1  boostcamp-sts-v1-train-001         slack-rtt   \n",
       "2  boostcamp-sts-v1-train-002  petition-sampled   \n",
       "3  boostcamp-sts-v1-train-003     slack-sampled   \n",
       "4  boostcamp-sts-v1-train-004     slack-sampled   \n",
       "5  boostcamp-sts-v1-train-005          nsmc-rtt   \n",
       "6  boostcamp-sts-v1-train-006         slack-rtt   \n",
       "7  boostcamp-sts-v1-train-007      nsmc-sampled   \n",
       "8  boostcamp-sts-v1-train-008  petition-sampled   \n",
       "9  boostcamp-sts-v1-train-009  petition-sampled   \n",
       "\n",
       "                               sentence_1                    sentence_2  \\\n",
       "0  스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~          반전도 있고,사랑도 있고재미도있네요.   \n",
       "1                    앗 제가 접근권한이 없다고 뜹니다;;           오, 액세스 권한이 없다고 합니다.   \n",
       "2                          주택청약조건 변경해주세요.            주택청약 무주택기준 변경해주세요.   \n",
       "3                  입사후 처음 대면으로 만나 반가웠습니다.  화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.   \n",
       "4                              뿌듯뿌듯 하네요!!         꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!   \n",
       "5                          오마이가뜨지져스크롸이스트휏          오 마이 갓 지저스 스크론 이스트 팬   \n",
       "6                     전 암만 찍어도 까만 하늘.. ㅠㅠ           암만 찍어도 하늘은 까맣다.. ㅠㅠ   \n",
       "7                   이렇게 귀여운 쥐들은 처음이네요.ㅎㅎㅎ            이렇게 지겨운 공포영화는 처음..   \n",
       "8                  미세먼지 해결이 가장 시급한 문제입니다!       가장 시급한 것이 신생아실 관리입니다!!!   \n",
       "9                         크림하우스 환불조치해주세요.         크림하우스 환불조치할 수 있도록해주세여   \n",
       "\n",
       "   label  binary-label                                             tokens  \n",
       "0    2.2           0.0  [[CLS], 스릴, ##도, ##있, ##고, 반전, ##도, 있, ##고, 여느...  \n",
       "1    4.2           1.0  [[CLS], 앗, 제, ##가, 접근, ##권, ##한, ##이, 없, ##다고,...  \n",
       "2    2.4           0.0  [[CLS], 주택, ##청, ##약, ##조건, 변경, ##해, ##주, ##세요...  \n",
       "3    3.0           1.0  [[CLS], 입사, ##후, 처음, 대면, ##으로, 만나, 반가웠, ##습, #...  \n",
       "4    0.0           0.0  [[CLS], 뿌듯, ##뿌, ##듯, 하, ##네, ##요, !, !, [SEP]...  \n",
       "5    2.6           1.0  [[CLS], [UNK], [SEP], 오, 마이, 갓, 지저, ##스, 스크, #...  \n",
       "6    3.6           1.0  [[CLS], 전, 암, ##만, 찍, ##어도, 까만, 하늘, ., ., ㅠㅠ, ...  \n",
       "7    0.6           0.0  [[CLS], 이렇게, 귀여운, 쥐, ##들, ##은, 처음, ##이, ##네, #...  \n",
       "8    0.4           0.0  [[CLS], 미세먼지, 해결, ##이, 가장, 시급, ##한, 문제, ##입니다,...  \n",
       "9    4.2           1.0  [[CLS], 크림, ##하우스, 환불, ##조치, ##해, ##주, ##세요, ....  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-train-001</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>앗 제가 접근권한이 없다고 뜹니다;;</td>\n",
       "      <td>오, 액세스 권한이 없다고 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], 앗, 제, ##가, 접근, ##권, ##한, ##이, 없, ##다고,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>boostcamp-sts-v1-train-005</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>오마이가뜨지져스크롸이스트휏</td>\n",
       "      <td>오 마이 갓 지저스 스크론 이스트 팬</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], [UNK], [SEP], 오, 마이, 갓, 지저, ##스, 스크, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>boostcamp-sts-v1-train-158</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>처음 뵌 분들과 빠르게 친해질 수 있을 것 같은 느낌!</td>\n",
       "      <td>처음 만나는 사람들과 금방 친해질 수 있는 것 같아요!</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], 처음, [UNK], 분, ##들, ##과, 빠르, ##게, 친해, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>boostcamp-sts-v1-train-187</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>와아아아안전 좋아요오오</td>\n",
       "      <td>꺄오오올!!!!! 환영합니다아아아</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], 와, ##아아, ##아, ##안전, 좋아, ##요, ##오, ##오,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>boostcamp-sts-v1-train-227</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>수지 목소리 너무좋아ㅜㅜㅜ</td>\n",
       "      <td>소소한재미ㅜㅋ여탯껏삶을다시돌아보게하는영화ㅜ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], 수지, 목소리, 너무, ##좋, ##아, ##ㅜㅜ, ##ㅜ, [SEP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10907</th>\n",
       "      <td>boostcamp-sts-v1-test-1033</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>삼가고인의 명복을 빕니다</td>\n",
       "      <td>삼가 고인의 명복을 빕니다</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[CLS], 삼가, ##고, ##인, ##의, 명복, ##을, [UNK], [SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10911</th>\n",
       "      <td>boostcamp-sts-v1-test-1037</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>담에는 오프에서 얼굴 뵐 수 있기를</td>\n",
       "      <td>제 머리가 어깨에 닿기 전에는 한번 뵐 수 있길</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[CLS], 담, ##에, ##는, 오프, ##에서, 얼굴, [UNK], 수, 있...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10934</th>\n",
       "      <td>boostcamp-sts-v1-test-1060</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>정말 재밌게봤습니다 ^^</td>\n",
       "      <td>정말 재밌게봣습니다^_^</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[CLS], 정말, 재밌, ##게, ##봤, ##습, ##니다, ^, ^, [SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>boostcamp-sts-v1-test-1063</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>항상 요맘때쯤 비가와서 아쉬웠는데 이번 봄은 벚꽃 개나리 진달래가 모두 한자리에 모...</td>\n",
       "      <td>이맘때쯤 비가 와서 아쉬웠는데, 이번 봄, 벚꽃, 개나리, 만개한 벚꽃이 만발하는 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[CLS], 항상, 요, ##맘, ##때, ##쯤, 비, ##가와, ##서, 아쉬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10972</th>\n",
       "      <td>boostcamp-sts-v1-test-1098</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>요즘 재미가 훅 떨어짐...</td>\n",
       "      <td>요즘 재미가 사라졌다...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[CLS], 요즘, 재미, ##가, [UNK], 떨어, ##짐, ., ., ., ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id         source  \\\n",
       "1      boostcamp-sts-v1-train-001      slack-rtt   \n",
       "5      boostcamp-sts-v1-train-005       nsmc-rtt   \n",
       "158    boostcamp-sts-v1-train-158      slack-rtt   \n",
       "187    boostcamp-sts-v1-train-187  slack-sampled   \n",
       "227    boostcamp-sts-v1-train-227   nsmc-sampled   \n",
       "...                           ...            ...   \n",
       "10907  boostcamp-sts-v1-test-1033   nsmc-sampled   \n",
       "10911  boostcamp-sts-v1-test-1037  slack-sampled   \n",
       "10934  boostcamp-sts-v1-test-1060   nsmc-sampled   \n",
       "10937  boostcamp-sts-v1-test-1063      slack-rtt   \n",
       "10972  boostcamp-sts-v1-test-1098       nsmc-rtt   \n",
       "\n",
       "                                              sentence_1  \\\n",
       "1                                   앗 제가 접근권한이 없다고 뜹니다;;   \n",
       "5                                         오마이가뜨지져스크롸이스트휏   \n",
       "158                       처음 뵌 분들과 빠르게 친해질 수 있을 것 같은 느낌!   \n",
       "187                                         와아아아안전 좋아요오오   \n",
       "227                                       수지 목소리 너무좋아ㅜㅜㅜ   \n",
       "...                                                  ...   \n",
       "10907                                      삼가고인의 명복을 빕니다   \n",
       "10911                                담에는 오프에서 얼굴 뵐 수 있기를   \n",
       "10934                                      정말 재밌게봤습니다 ^^   \n",
       "10937  항상 요맘때쯤 비가와서 아쉬웠는데 이번 봄은 벚꽃 개나리 진달래가 모두 한자리에 모...   \n",
       "10972                                    요즘 재미가 훅 떨어짐...   \n",
       "\n",
       "                                              sentence_2  label  binary-label  \\\n",
       "1                                    오, 액세스 권한이 없다고 합니다.    4.2           1.0   \n",
       "5                                   오 마이 갓 지저스 스크론 이스트 팬    2.6           1.0   \n",
       "158                       처음 만나는 사람들과 금방 친해질 수 있는 것 같아요!    3.4           1.0   \n",
       "187                                   꺄오오올!!!!! 환영합니다아아아    0.0           0.0   \n",
       "227                              소소한재미ㅜㅋ여탯껏삶을다시돌아보게하는영화ㅜ    0.0           0.0   \n",
       "...                                                  ...    ...           ...   \n",
       "10907                                     삼가 고인의 명복을 빕니다    NaN           NaN   \n",
       "10911                         제 머리가 어깨에 닿기 전에는 한번 뵐 수 있길    NaN           NaN   \n",
       "10934                                      정말 재밌게봣습니다^_^    NaN           NaN   \n",
       "10937  이맘때쯤 비가 와서 아쉬웠는데, 이번 봄, 벚꽃, 개나리, 만개한 벚꽃이 만발하는 ...    NaN           NaN   \n",
       "10972                                     요즘 재미가 사라졌다...    NaN           NaN   \n",
       "\n",
       "                                                  tokens  \n",
       "1      [[CLS], 앗, 제, ##가, 접근, ##권, ##한, ##이, 없, ##다고,...  \n",
       "5      [[CLS], [UNK], [SEP], 오, 마이, 갓, 지저, ##스, 스크, #...  \n",
       "158    [[CLS], 처음, [UNK], 분, ##들, ##과, 빠르, ##게, 친해, #...  \n",
       "187    [[CLS], 와, ##아아, ##아, ##안전, 좋아, ##요, ##오, ##오,...  \n",
       "227    [[CLS], 수지, 목소리, 너무, ##좋, ##아, ##ㅜㅜ, ##ㅜ, [SEP...  \n",
       "...                                                  ...  \n",
       "10907  [[CLS], 삼가, ##고, ##인, ##의, 명복, ##을, [UNK], [SE...  \n",
       "10911  [[CLS], 담, ##에, ##는, 오프, ##에서, 얼굴, [UNK], 수, 있...  \n",
       "10934  [[CLS], 정말, 재밌, ##게, ##봤, ##습, ##니다, ^, ^, [SE...  \n",
       "10937  [[CLS], 항상, 요, ##맘, ##때, ##쯤, 비, ##가와, ##서, 아쉬...  \n",
       "10972  [[CLS], 요즘, 재미, ##가, [UNK], 떨어, ##짐, ., ., ., ...  \n",
       "\n",
       "[470 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find '[UNK]' token in dataset['tokens'] and make a new df\n",
    "unk_list = []\n",
    "for i in range(len(dataset['tokens'])):\n",
    "    if '[UNK]' in dataset['tokens'][i]:\n",
    "        unk_list.append(i)\n",
    "        \n",
    "unk_df = dataset.loc[unk_list]\n",
    "unk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a txt file with unk_df 'tokens' data\n",
    "with open('unk.txt', 'w') as f:\n",
    "    for i in range(len(unk_df)):\n",
    "        f.write(str(unk_df['sentence_1'].values[i]) + ' @ ' + str(unk_df['sentence_2'].values[i]) + '\\n' + str(unk_df['tokens'].values[i]) + '\\n' + '****************************************************************************************************' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#오운완 #어쩌구 #저쩌구 &lt;&lt; 이자리에 해시태그가 있었음</td>\n",
       "      <td>()()() () &lt;&lt; 이자리에 빈괄호 있었음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>띄어쓰기를안해버리기</td>\n",
       "      <td>띄어쓰기를      두번이상 해도    한번만          띄어쓰기가 된다고?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지</td>\n",
       "      <td>맟춥뻡 틀려버리기~</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sentence_1  \\\n",
       "0  #오운완 #어쩌구 #저쩌구 << 이자리에 해시태그가 있었음   \n",
       "1                        띄어쓰기를안해버리기   \n",
       "2          맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지   \n",
       "\n",
       "                                       sentence_2  \n",
       "0                       ()()() () << 이자리에 빈괄호 있었음  \n",
       "1  띄어쓰기를      두번이상 해도    한번만          띄어쓰기가 된다고?   \n",
       "2                                      맟춥뻡 틀려버리기~  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame({\n",
    "    'sentence_1': ['#오운완 #어쩌구 #저쩌구 << 이자리에 해시태그가 있었음', '띄어쓰기를안해버리기', '맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지'],\n",
    "    'sentence_2': ['()()() () << 이자리에 빈괄호 있었음', '띄어쓰기를      두번이상 해도    한번만          띄어쓰기가 된다고? ', '맟춥뻡 틀려버리기~']\n",
    "})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def delete_etcs(dataframe):\n",
    "#     for i in range(len(dataframe['sentence_1'])):\n",
    "#         dataframe['sentence_1'][i] = re.sub(r'#', '', dataframe['sentence_1'][i])\n",
    "#         dataframe['sentence_2'][i] = re.sub(r'#', '', dataframe['sentence_2'][i])\n",
    "#         dataframe['sentence_1'][i] = re.sub(r'\\(\\)', '', dataframe['sentence_1'][i])\n",
    "#         dataframe['sentence_2'][i] = re.sub(r'\\(\\)', '', dataframe['sentence_2'][i])\n",
    "#     return dataframe\n",
    "\n",
    "# delete_etcs(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply_spacing: 100%|██████████| 3/3 [00:00<00:00,  6.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#오운 완 #어쩌구 #저쩌구 &lt;&lt; 이 자리에 해시 태그가 있었음</td>\n",
       "      <td>()()() () &lt;&lt; 이 자리에 빈괄호 있었음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>띄어쓰기를 안 해버리기</td>\n",
       "      <td>띄어쓰기를 두 번 이상 해도 한 번만 띄어쓰기가 된다고?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>맞춤법 틀리면 외 않되? 쓰고 싶은 대로 쓰면 돼지</td>\n",
       "      <td>맟춥뻡 틀려버리기~</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            sentence_1                       sentence_2\n",
       "0  #오운 완 #어쩌구 #저쩌구 << 이 자리에 해시 태그가 있었음       ()()() () << 이 자리에 빈괄호 있었음\n",
       "1                         띄어쓰기를 안 해버리기  띄어쓰기를 두 번 이상 해도 한 번만 띄어쓰기가 된다고?\n",
       "2         맞춤법 틀리면 외 않되? 쓰고 싶은 대로 쓰면 돼지                       맟춥뻡 틀려버리기~"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each values in train['sentence_1'] or train['sentence_2], apply spacing\n",
    "from pykospacing import Spacing\n",
    "\n",
    "def apply_spacing(dataframe):\n",
    "    spacing = Spacing()\n",
    "    for i in tqdm(range(len(dataframe['sentence_1'])), desc='apply_spacing'):\n",
    "        dataframe.loc[i, 'sentence_1'] = spacing(dataframe.loc[i, 'sentence_1'])\n",
    "        dataframe.loc[i, 'sentence_2'] = spacing(dataframe.loc[i, 'sentence_2'])\n",
    "    return dataframe\n",
    "\n",
    "apply_spacing(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 3/3 [00:05<00:00,  1.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#오운 완 #어쩌고 #저쩌고 &lt;&lt; 이 자리에 해시 태그가 있었음</td>\n",
       "      <td>()()() () &lt;&lt; 이 자리에 빈 괄호 있었음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>띄어쓰기를 안 해버리기</td>\n",
       "      <td>띄어쓰기를 두 번 이상 해도 한 번만 띄어쓰기가 된다고?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지</td>\n",
       "      <td>맟춥뻡 틀려버리기~</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            sentence_1                       sentence_2\n",
       "0  #오운 완 #어쩌고 #저쩌고 << 이 자리에 해시 태그가 있었음      ()()() () << 이 자리에 빈 괄호 있었음\n",
       "1                         띄어쓰기를 안 해버리기  띄어쓰기를 두 번 이상 해도 한 번만 띄어쓰기가 된다고?\n",
       "2         맞춤법 틀리면 왜 안돼? 쓰고 싶은 대로 쓰면 되지                       맟춥뻡 틀려버리기~"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "네이버 맞춤법 api 사용\n",
    "- 이모티콘 사라짐: '👌👌👌' -> ''\n",
    "- 신조어 등은 안바뀜: '갬성' -> '갬성', '퐈이야' -> '퐈이야' 등\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "\n",
    "def check_spell(dataframe):\n",
    "    def get_passport_key():\n",
    "        url = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EB%A7%9E%EC%B6%A4%EB%B2%95%EA%B2%80%EC%82%AC%EA%B8%B0\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            match = re.search(r'passportKey=([a-zA-Z0-9-_]+)', html)\n",
    "            if match:\n",
    "                passport_key = match.group(1)\n",
    "                print(f\"passportKey found: {passport_key}\")\n",
    "                return passport_key\n",
    "            else:\n",
    "                raise ValueError(\"passportKey not found in the HTML response.\")\n",
    "        else:\n",
    "            raise ConnectionError(f\"Failed to fetch the page, status code: {response.status_code}\")\n",
    "\n",
    "    # 맞춤법 검사를 처리하는 내부 함수\n",
    "    def _spell_check_request(text, passport_key):\n",
    "        payload = {\n",
    "            'passportKey': passport_key,\n",
    "            '_callback': passport_key,\n",
    "            'q': text,\n",
    "            'color_blindness': '0'\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'referer': 'https://search.naver.com/',\n",
    "        }\n",
    "\n",
    "        start_time = time.time()\n",
    "        r = requests.get(\"https://m.search.naver.com/p/csearch/ocontent/util/SpellerProxy\", params=payload, headers=headers)\n",
    "        passed_time = time.time() - start_time\n",
    "\n",
    "        json_match = re.search(r'\\{.*\\}', r.text)\n",
    "        if json_match:\n",
    "            json_data = json_match.group(0)\n",
    "            data = json.loads(json_data)\n",
    "            html = data['message']['result']['html']\n",
    "            return _remove_tags(html)\n",
    "        else:\n",
    "            raise ValueError(\"No JSON data found in the response.\")\n",
    "\n",
    "    def _remove_tags(text):\n",
    "        text = '<content>{}</content>'.format(text).replace('<br>','')\n",
    "        result = ''.join(re.sub(r'<[^>]+>', '', text))\n",
    "        return result\n",
    "\n",
    "    def check(text, passport_key):\n",
    "        try:\n",
    "            return _spell_check_request(text, passport_key)\n",
    "        except ValueError as e:\n",
    "            if 'No JSON data found in the response' in str(e):\n",
    "                print(\"passport_key expired, fetching a new one.\")\n",
    "                passport_key = get_passport_key()  # 새로운 passport_key 가져오기\n",
    "                return _spell_check_request(text, passport_key)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    passport_key = get_passport_key()\n",
    "\n",
    "    for i in tqdm(range(len(dataframe['sentence_1'])), desc='check_spell'):\n",
    "        dataframe.loc[i, 'sentence_1'] = html.unescape(check(dataframe.loc[i, 'sentence_1'], passport_key))\n",
    "        dataframe.loc[i, 'sentence_2'] = html.unescape(check(dataframe.loc[i, 'sentence_2'], passport_key))        \n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 5/5 [00:00<00:00, 18.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>맞앜ㅋㅋㅋㅋ뭔데욬ㅋㅋㅋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>갬성 있네</td>\n",
       "      <td>진ㅉ ㅏ? 정말로?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>넵 알겠습니다 ㅋ</td>\n",
       "      <td>문재인 정부</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>퐈이야</td>\n",
       "      <td>봬요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>저번에 뵌 거 같은데</td>\n",
       "      <td>쭈뼛쭈뼛</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence_1    sentence_2\n",
       "0               맞앜ㅋㅋㅋㅋ뭔데욬ㅋㅋㅋ\n",
       "1        갬성 있네    진ㅉ ㅏ? 정말로?\n",
       "2    넵 알겠습니다 ㅋ        문재인 정부\n",
       "3          퐈이야            봬요\n",
       "4  저번에 뵌 거 같은데          쭈뼛쭈뼛"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_test = pd.DataFrame({\n",
    "    'sentence_1': ['👌', '갬성있네', '넵 알겠습니다 ㅋ', '퐈이야', '저번에 뵌거같은데'],\n",
    "    'sentence_2': ['맞앜ㅋㅋㅋㅋ뭔데욬ㅋㅋㅋ', '진ㅉ ㅏ? 정말로?', '문재인정부', '봬용', '쭈뼛쭈뼛']\n",
    "})\n",
    "\n",
    "check_spell(local_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "repeat_normalize: 100%|██████████| 2/2 [00:00<00:00, 2535.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>맞앜ㅋㅋ</td>\n",
       "      <td>맞앜ㅋㅋ!!!!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>와하하</td>\n",
       "      <td>캬캬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_1        sentence_2\n",
       "0       맞앜ㅋㅋ  맞앜ㅋㅋ!!!!!!!!!!!!\n",
       "1        와하하                캬캬"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use soynlp to correct spelling\n",
    "from soynlp.normalizer import repeat_normalize as r_n\n",
    "\n",
    "def repeat_normalize(dataframe):\n",
    "    for i in tqdm(range(len(dataframe['sentence_1'])), desc='repeat_normalize'):\n",
    "        dataframe.loc[i, 'sentence_1'] = r_n(dataframe.loc[i, 'sentence_1'], num_repeats=2)\n",
    "        dataframe.loc[i, 'sentence_2'] = r_n(dataframe.loc[i, 'sentence_2'], num_repeats=2)\n",
    "    return dataframe\n",
    "\n",
    "local_test = pd.DataFrame({\n",
    "    'sentence_1': ['맞앜ㅋㅋㅋㅋ', '와하하하하하하'],\n",
    "    'sentence_2': ['맞앜ㅋㅋㅋㅋ!!!!!!!!!!!!', '캬캬캬캬캬캬캬']\n",
    "})\n",
    "    \n",
    "repeat_normalize(local_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10974, 7)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # copy dataset\n",
    "# dataset_filter_1 = dataset.copy()\n",
    "# dataset_filter_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply_spacing: 100%|██████████| 10974/10974 [09:26<00:00, 19.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 10974/10974 [12:05<00:00, 15.13it/s]\n",
      "repeat_normalize: 100%|██████████| 10974/10974 [00:04<00:00, 2196.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# # apply all the functions\n",
    "# # dataset_filter_1 = delete_etcs(dataset_filter_1)\n",
    "# dataset_filter_1 = apply_spacing(dataset_filter_1)\n",
    "# dataset_filter_1 = check_spell(dataset_filter_1)\n",
    "# dataset_filter_1 = repeat_normalize(dataset_filter_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100%|██████████| 10974/10974 [00:06<00:00, 1809.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# data, tokens = tokenizing(tokenizer, dataset_filter_1)\n",
    "# dataset_filter_1['tokens'] = tokens\n",
    "\n",
    "# # find '[UNK]' token in dataset_filter_1['tokens'] and make a new df\n",
    "# unk_list = []\n",
    "# for i in range(len(dataset_filter_1['tokens'])):\n",
    "#     if '[UNK]' in dataset_filter_1['tokens'][i]:\n",
    "#         unk_list.append(i)\n",
    "        \n",
    "# unk_df = dataset_filter_1.loc[unk_list]\n",
    "\n",
    "# # make a txt file with unk_df 'tokens' data\n",
    "# with open('unk_filter_1.txt', 'w') as f:\n",
    "#     for i in range(len(unk_df)):\n",
    "#         f.write(str(unk_df['sentence_1'].values[i]) + ' @ ' + str(unk_df['sentence_2'].values[i]) + '\\n' + str(unk_df['tokens'].values[i]) + '\\n' + '****************************************************************************************************' + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL: preprocess train.csv to train_preprocessed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply_spacing: 100%|██████████| 9324/9324 [07:59<00:00, 19.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 9324/9324 [10:26<00:00, 14.87it/s]\n",
      "repeat_normalize: 100%|██████████| 9324/9324 [00:03<00:00, 2450.75it/s]\n",
      "apply_spacing: 100%|██████████| 9324/9324 [08:03<00:00, 19.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 9324/9324 [09:30<00:00, 16.33it/s]\n",
      "repeat_normalize: 100%|██████████| 9324/9324 [00:03<00:00, 2458.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# # load train.csv\n",
    "# train = pd.read_csv('train.csv')\n",
    "\n",
    "# # spacing\n",
    "# train_spacing = apply_spacing(train)\n",
    "# train_spacing.to_csv('train_spacing.csv', index=False)\n",
    "\n",
    "# # spell check\n",
    "# train_spell = check_spell(train)\n",
    "# train_spell.to_csv('train_spell.csv', index=False)\n",
    "\n",
    "# # repeat normalize\n",
    "# train_repeat = repeat_normalize(train)\n",
    "# train_repeat.to_csv('train_repeat.csv', index=False)\n",
    "\n",
    "# train_spacing_spell = apply_spacing(train)\n",
    "# train_spacing_spell = check_spell(train_spacing_spell)\n",
    "# train_spacing_spell.to_csv('train_spacing_spell.csv', index=False)\n",
    "\n",
    "# train_spacing_repeat = apply_spacing(train)\n",
    "# train_spacing_repeat = repeat_normalize(train_spacing_repeat)\n",
    "# train_spacing_repeat.to_csv('train_spacing_repeat.csv', index=False)\n",
    "\n",
    "# train_spell_repeat = check_spell(train)\n",
    "# train_spell_repeat = repeat_normalize(train_spell_repeat)\n",
    "# train_spell_repeat.to_csv('train_spell_repeat.csv', index=False)\n",
    "\n",
    "# train_preproc_all = apply_spacing(train)\n",
    "# train_preproc_all = check_spell(train_preproc_all)\n",
    "# train_preproc_all = repeat_normalize(train_preproc_all)\n",
    "# train_preproc_all.to_csv('train_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply_spacing: 100%|██████████| 550/550 [00:27<00:00, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 550/550 [00:37<00:00, 14.77it/s]\n",
      "repeat_normalize: 100%|██████████| 550/550 [00:00<00:00, 3496.96it/s]\n",
      "apply_spacing: 100%|██████████| 550/550 [00:28<00:00, 19.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 550/550 [00:35<00:00, 15.38it/s]\n",
      "apply_spacing: 100%|██████████| 550/550 [00:27<00:00, 19.79it/s]\n",
      "repeat_normalize: 100%|██████████| 550/550 [00:00<00:00, 3578.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 550/550 [00:30<00:00, 18.13it/s]\n",
      "repeat_normalize: 100%|██████████| 550/550 [00:00<00:00, 3504.34it/s]\n",
      "apply_spacing: 100%|██████████| 550/550 [00:27<00:00, 19.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 550/550 [00:30<00:00, 17.98it/s]\n",
      "repeat_normalize: 100%|██████████| 550/550 [00:00<00:00, 3439.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# dev = pd.read_csv('dev.csv')\n",
    "\n",
    "# dev_spacing = apply_spacing(dev)\n",
    "# dev_spacing.to_csv('dev_spacing.csv', index=False)\n",
    "\n",
    "# dev_spell = check_spell(dev)\n",
    "# dev_spell.to_csv('dev_spell.csv', index=False)\n",
    "\n",
    "# dev_repeat = repeat_normalize(dev)\n",
    "# dev_repeat.to_csv('dev_repeat.csv', index=False)\n",
    "\n",
    "# dev_spacing_spell = apply_spacing(dev)\n",
    "# dev_spacing_spell = check_spell(dev_spacing_spell)\n",
    "# dev_spacing_spell.to_csv('dev_spacing_spell.csv', index=False)\n",
    "\n",
    "# dev_spacing_repeat = apply_spacing(dev)\n",
    "# dev_spacing_repeat = repeat_normalize(dev_spacing_repeat)\n",
    "# dev_spacing_repeat.to_csv('dev_spacing_repeat.csv', index=False)\n",
    "\n",
    "# dev_spell_repeat = check_spell(dev)\n",
    "# dev_spell_repeat = repeat_normalize(dev_spell_repeat)\n",
    "# dev_spell_repeat.to_csv('dev_spell_repeat.csv', index=False)\n",
    "\n",
    "# dev_preproc_all = apply_spacing(dev)\n",
    "# dev_preproc_all = check_spell(dev_preproc_all)\n",
    "# dev_preproc_all = repeat_normalize(dev_preproc_all)\n",
    "# dev_preproc_all.to_csv('dev_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply_spacing: 100%|██████████| 1100/1100 [00:55<00:00, 19.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 1100/1100 [01:09<00:00, 15.93it/s]\n",
      "repeat_normalize: 100%|██████████| 1100/1100 [00:00<00:00, 4957.67it/s]\n",
      "apply_spacing: 100%|██████████| 1100/1100 [00:55<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 1100/1100 [01:04<00:00, 17.03it/s]\n",
      "apply_spacing: 100%|██████████| 1100/1100 [00:56<00:00, 19.55it/s]\n",
      "repeat_normalize: 100%|██████████| 1100/1100 [00:00<00:00, 4930.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: b7584e2dbf34edd27f75b6430787c04eb65feb52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 1100/1100 [01:06<00:00, 16.55it/s]\n",
      "repeat_normalize: 100%|██████████| 1100/1100 [00:00<00:00, 4853.54it/s]\n",
      "apply_spacing: 100%|██████████| 1100/1100 [01:00<00:00, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: b7584e2dbf34edd27f75b6430787c04eb65feb52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|██████████| 1100/1100 [01:05<00:00, 16.67it/s]\n",
      "repeat_normalize: 100%|██████████| 1100/1100 [00:00<00:00, 4954.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# test = pd.read_csv('test.csv')\n",
    "\n",
    "# test_spacing = apply_spacing(test)\n",
    "# test_spacing.to_csv('test_spacing.csv', index=False)\n",
    "\n",
    "# test_spell = check_spell(test)\n",
    "# test_spell.to_csv('test_spell.csv', index=False)\n",
    "\n",
    "# test_repeat = repeat_normalize(test)\n",
    "# test_repeat.to_csv('test_repeat.csv', index=False)\n",
    "\n",
    "# test_spacing_spell = apply_spacing(test)\n",
    "# test_spacing_spell = check_spell(test_spacing_spell)\n",
    "# test_spacing_spell.to_csv('test_spacing_spell.csv', index=False)\n",
    "\n",
    "# test_spacing_repeat = apply_spacing(test)\n",
    "# test_spacing_repeat = repeat_normalize(test_spacing_repeat)\n",
    "# test_spacing_repeat.to_csv('test_spacing_repeat.csv', index=False)\n",
    "\n",
    "# test_spell_repeat = check_spell(test)\n",
    "# test_spell_repeat = repeat_normalize(test_spell_repeat)\n",
    "# test_spell_repeat.to_csv('test_spell_repeat.csv', index=False)\n",
    "\n",
    "# test_preproc_all = apply_spacing(test)\n",
    "# test_preproc_all = check_spell(test_preproc_all)\n",
    "# test_preproc_all = repeat_normalize(test_preproc_all)\n",
    "# test_preproc_all.to_csv('test_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizer: add_token() -> add <PERSON>, <ADDRESS> tokens\n",
    "# tokenizer.add_tokens(['<PERSON>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_slang(text):\n",
    "#     # 줄임말 -> 정식 표현으로 변환\n",
    "#     slang_dict = {\n",
    "#         '넵': '네',\n",
    "#         ' 훅 ': ' 갑자기 '\n",
    "#     }\n",
    "\n",
    "#     # 줄임말 사전 기반으로 정규화\n",
    "#     for slang, formal in slang_dict.items():\n",
    "#         text = text.replace(slang, formal)\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# # 예시 문장\n",
    "# text = \"넘 졸귀탱 ㅋㅋ 진짜 짱이야 ㅠㅠ\"\n",
    "\n",
    "# # 줄임말 정규화 적용\n",
    "# normalized_text = normalize_slang(text)\n",
    "# print(normalized_text)  # \"너무 졸라 귀엽다 웃음 진짜 정말이야 슬픔\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
