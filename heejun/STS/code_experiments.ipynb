{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/miniconda3/envs/heejun-base/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('klue/roberta-base', max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing(self, dataframe):\n",
    "    data = []\n",
    "    for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "    # 두 입력 문장을 [SEP] 토큰으로 이어붙여서 전처리합니다.\n",
    "        text = '[SEP]'.join([item[text_column] for text_column in ['sentence_1', 'sentence_2']])\n",
    "        # padding=True와 truncation=True 옵션 추가\n",
    "        outputs = tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',  # max_length로 패딩을 고정\n",
    "            truncation=True,       # 텍스트를 최대 길이로 자름\n",
    "            max_length=128         # max_length 설정\n",
    "        )\n",
    "        data.append(outputs['input_ids'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS token ID: 0\n",
      "Padding token ID: 1\n",
      "Separation token ID: 2\n",
      "Unknown token ID: 3\n"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.cls_token_id\n",
    "print(f\"CLS token ID: {cls_token_id}\")\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "print(f\"Padding token ID: {pad_token_id}\")\n",
    "\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "print(f\"Separation token ID: {sep_token_id}\")\n",
    "\n",
    "unk_token_id = tokenizer.unk_token_id\n",
    "print(f\"Unknown token ID: {unk_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 10 samples for testing\n",
    "train = train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100%|██████████| 10/10 [00:00<00:00, 2493.79it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = tokenizing(tokenizer, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  1478,\n",
       "  2465,\n",
       "  2119,\n",
       "  1891,\n",
       "  1141,\n",
       "  3685,\n",
       "  2178,\n",
       "  2067,\n",
       "  2321,\n",
       "  193,\n",
       "  17,\n",
       "  2,\n",
       "  32,\n",
       "  21639,\n",
       "  2238,\n",
       "  18946,\n",
       "  34,\n",
       "  805,\n",
       "  2119,\n",
       "  1891,\n",
       "  1141,\n",
       "  4146,\n",
       "  2097,\n",
       "  2178,\n",
       "  2067,\n",
       "  2321,\n",
       "  5,\n",
       "  5311,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  1437,\n",
       "  2343,\n",
       "  2138,\n",
       "  4530,\n",
       "  5333,\n",
       "  6396,\n",
       "  1160,\n",
       "  2318,\n",
       "  858,\n",
       "  13964,\n",
       "  16,\n",
       "  4665,\n",
       "  2119,\n",
       "  23009,\n",
       "  2052,\n",
       "  8260,\n",
       "  2112,\n",
       "  1043,\n",
       "  2170,\n",
       "  1378,\n",
       "  2031,\n",
       "  2359,\n",
       "  3683,\n",
       "  4785,\n",
       "  4483,\n",
       "  2075,\n",
       "  637,\n",
       "  2125,\n",
       "  4787,\n",
       "  2259,\n",
       "  1521,\n",
       "  7263,\n",
       "  2112,\n",
       "  7478,\n",
       "  2359,\n",
       "  2180,\n",
       "  2778,\n",
       "  2182,\n",
       "  18,\n",
       "  2,\n",
       "  5333,\n",
       "  6396,\n",
       "  3771,\n",
       "  2138,\n",
       "  1170,\n",
       "  13964,\n",
       "  6670,\n",
       "  2052,\n",
       "  3760,\n",
       "  1378,\n",
       "  2496,\n",
       "  2051,\n",
       "  2112,\n",
       "  4665,\n",
       "  2138,\n",
       "  6992,\n",
       "  2118,\n",
       "  1380,\n",
       "  2886,\n",
       "  3683,\n",
       "  4483,\n",
       "  2522,\n",
       "  4665,\n",
       "  2125,\n",
       "  4787,\n",
       "  2138,\n",
       "  1521,\n",
       "  3987,\n",
       "  6396,\n",
       "  6001,\n",
       "  2359,\n",
       "  2062,\n",
       "  18,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  3839,\n",
       "  2470,\n",
       "  3758,\n",
       "  2138,\n",
       "  31149,\n",
       "  3806,\n",
       "  4835,\n",
       "  2088,\n",
       "  16,\n",
       "  3848,\n",
       "  5487,\n",
       "  2085,\n",
       "  1295,\n",
       "  1513,\n",
       "  2051,\n",
       "  2112,\n",
       "  3940,\n",
       "  19391,\n",
       "  2414,\n",
       "  2728,\n",
       "  555,\n",
       "  2227,\n",
       "  2182,\n",
       "  97,\n",
       "  97,\n",
       "  5,\n",
       "  5311,\n",
       "  2,\n",
       "  3667,\n",
       "  3758,\n",
       "  2031,\n",
       "  2069,\n",
       "  8065,\n",
       "  3806,\n",
       "  5194,\n",
       "  19521,\n",
       "  5487,\n",
       "  2085,\n",
       "  1295,\n",
       "  1513,\n",
       "  2051,\n",
       "  2112,\n",
       "  831,\n",
       "  19391,\n",
       "  2414,\n",
       "  575,\n",
       "  555,\n",
       "  2227,\n",
       "  2182,\n",
       "  97,\n",
       "  97,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  21345,\n",
       "  2540,\n",
       "  2104,\n",
       "  3635,\n",
       "  4052,\n",
       "  1897,\n",
       "  2223,\n",
       "  5971,\n",
       "  2,\n",
       "  21345,\n",
       "  2540,\n",
       "  2104,\n",
       "  12589,\n",
       "  15648,\n",
       "  2332,\n",
       "  4052,\n",
       "  2144,\n",
       "  2438,\n",
       "  2343,\n",
       "  7316,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  5729,\n",
       "  6177,\n",
       "  2179,\n",
       "  18119,\n",
       "  30804,\n",
       "  6516,\n",
       "  2,\n",
       "  1453,\n",
       "  3760,\n",
       "  2356,\n",
       "  2173,\n",
       "  2667,\n",
       "  2182,\n",
       "  6516,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  3840,\n",
       "  2073,\n",
       "  4602,\n",
       "  2138,\n",
       "  4089,\n",
       "  16599,\n",
       "  18,\n",
       "  2,\n",
       "  3840,\n",
       "  12989,\n",
       "  5875,\n",
       "  2069,\n",
       "  22457,\n",
       "  2223,\n",
       "  5971,\n",
       "  18,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  63,\n",
       "  1300,\n",
       "  2073,\n",
       "  742,\n",
       "  2052,\n",
       "  1642,\n",
       "  2015,\n",
       "  64,\n",
       "  29293,\n",
       "  9043,\n",
       "  6372,\n",
       "  18,\n",
       "  32,\n",
       "  21639,\n",
       "  2238,\n",
       "  18946,\n",
       "  34,\n",
       "  805,\n",
       "  2073,\n",
       "  7401,\n",
       "  1642,\n",
       "  2279,\n",
       "  2477,\n",
       "  2471,\n",
       "  18,\n",
       "  2,\n",
       "  1656,\n",
       "  2517,\n",
       "  3135,\n",
       "  4035,\n",
       "  27135,\n",
       "  32,\n",
       "  21639,\n",
       "  2238,\n",
       "  18946,\n",
       "  34,\n",
       "  805,\n",
       "  1642,\n",
       "  2015,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  4380,\n",
       "  1041,\n",
       "  15351,\n",
       "  5283,\n",
       "  3628,\n",
       "  2092,\n",
       "  3707,\n",
       "  35,\n",
       "  2,\n",
       "  4380,\n",
       "  1041,\n",
       "  2052,\n",
       "  16116,\n",
       "  35,\n",
       "  35,\n",
       "  35,\n",
       "  35,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  4178,\n",
       "  8984,\n",
       "  2073,\n",
       "  4229,\n",
       "  1583,\n",
       "  2323,\n",
       "  2,\n",
       "  4178,\n",
       "  8984,\n",
       "  2073,\n",
       "  4177,\n",
       "  18,\n",
       "  18,\n",
       "  3901,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  897,\n",
       "  2893,\n",
       "  2384,\n",
       "  2893,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  4035,\n",
       "  2069,\n",
       "  11115,\n",
       "  8980,\n",
       "  2370,\n",
       "  7630,\n",
       "  17984,\n",
       "  5229,\n",
       "  15302,\n",
       "  5,\n",
       "  5,\n",
       "  2,\n",
       "  1316,\n",
       "  2067,\n",
       "  2027,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  918,\n",
       "  18,\n",
       "  18,\n",
       "  18,\n",
       "  543,\n",
       "  2088,\n",
       "  2585,\n",
       "  2203,\n",
       "  2182,\n",
       "  4692,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6119</th>\n",
       "      <td>boostcamp-sts-v1-train-6119</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>원곡도 한 번 들어보시죠 ㅎ -</td>\n",
       "      <td>&lt;PERSON&gt; 님도 한 번 고려해보시죠! ㅎㅎ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>boostcamp-sts-v1-train-6050</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>영드를 보고 궁금해서 보게 됐는데, 드라마도 각색이 심해서 맘에 안들었지만 그래도 ...</td>\n",
       "      <td>궁금해서 영화를 봤는데 적응이 너무 안되어서 드라마를 좋아하지 않았지만 연기와 드라...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9159</th>\n",
       "      <td>boostcamp-sts-v1-train-9159</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>이러한 이야기를 스스럼 없이 나누고, 같이 공감할 수 있어서 더욱 즐거웠던것 같아요...</td>\n",
       "      <td>이런 이야기들을 주저 없이 공유하고 공감할 수 있어서 더 즐거웠던 것 같아요~~!</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>boostcamp-sts-v1-train-3291</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>대진침대 문제 해결 해주세요</td>\n",
       "      <td>대진침대 수거 누락건 해결부탁드립니다</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415</th>\n",
       "      <td>boostcamp-sts-v1-train-2415</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>조기교육인가요 귀요미 ㅠㅠ</td>\n",
       "      <td>와 너무귀여워요 ㅠㅠ</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9034</th>\n",
       "      <td>boostcamp-sts-v1-train-9034</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>삼성은 노조를 인정하라.</td>\n",
       "      <td>삼성 이재용 부회장을 풀어주세요.</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4963</th>\n",
       "      <td>boostcamp-sts-v1-train-4963</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>[숨은 냥이 찾기] 삼청동 지붕 풍경. &lt;PERSON&gt; 님은 금방 찾으실듯.</td>\n",
       "      <td>첫번째 사진에서 &lt;PERSON&gt;님 찾기!</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>boostcamp-sts-v1-train-394</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>이게 말이나 된다고 생각합니까?</td>\n",
       "      <td>이게 말이 됩니까????</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6138</th>\n",
       "      <td>boostcamp-sts-v1-train-6138</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>마지막 반전은 진짜 지림</td>\n",
       "      <td>마지막 반전은 무슨.. ㅋㅋㅋ</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5647</th>\n",
       "      <td>boostcamp-sts-v1-train-5647</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>따쉬따쉬... 사진을 어딘가 잃어버려 돌아온 프로필 자랑타임!!</td>\n",
       "      <td>스시집...또...가고싶네요ㅋㅋ!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id            source  \\\n",
       "6119  boostcamp-sts-v1-train-6119     slack-sampled   \n",
       "6050  boostcamp-sts-v1-train-6050          nsmc-rtt   \n",
       "9159  boostcamp-sts-v1-train-9159         slack-rtt   \n",
       "3291  boostcamp-sts-v1-train-3291  petition-sampled   \n",
       "2415  boostcamp-sts-v1-train-2415     slack-sampled   \n",
       "9034  boostcamp-sts-v1-train-9034  petition-sampled   \n",
       "4963  boostcamp-sts-v1-train-4963     slack-sampled   \n",
       "394    boostcamp-sts-v1-train-394  petition-sampled   \n",
       "6138  boostcamp-sts-v1-train-6138      nsmc-sampled   \n",
       "5647  boostcamp-sts-v1-train-5647     slack-sampled   \n",
       "\n",
       "                                             sentence_1  \\\n",
       "6119                                  원곡도 한 번 들어보시죠 ㅎ -   \n",
       "6050  영드를 보고 궁금해서 보게 됐는데, 드라마도 각색이 심해서 맘에 안들었지만 그래도 ...   \n",
       "9159  이러한 이야기를 스스럼 없이 나누고, 같이 공감할 수 있어서 더욱 즐거웠던것 같아요...   \n",
       "3291                                    대진침대 문제 해결 해주세요   \n",
       "2415                                     조기교육인가요 귀요미 ㅠㅠ   \n",
       "9034                                      삼성은 노조를 인정하라.   \n",
       "4963         [숨은 냥이 찾기] 삼청동 지붕 풍경. <PERSON> 님은 금방 찾으실듯.   \n",
       "394                                   이게 말이나 된다고 생각합니까?   \n",
       "6138                                      마지막 반전은 진짜 지림   \n",
       "5647                따쉬따쉬... 사진을 어딘가 잃어버려 돌아온 프로필 자랑타임!!   \n",
       "\n",
       "                                             sentence_2  label  binary-label  \n",
       "6119                         <PERSON> 님도 한 번 고려해보시죠! ㅎㅎ    0.0           0.0  \n",
       "6050  궁금해서 영화를 봤는데 적응이 너무 안되어서 드라마를 좋아하지 않았지만 연기와 드라...    2.4           0.0  \n",
       "9159      이런 이야기들을 주저 없이 공유하고 공감할 수 있어서 더 즐거웠던 것 같아요~~!    4.4           1.0  \n",
       "3291                               대진침대 수거 누락건 해결부탁드립니다    2.6           1.0  \n",
       "2415                                        와 너무귀여워요 ㅠㅠ    1.2           0.0  \n",
       "9034                                 삼성 이재용 부회장을 풀어주세요.    0.6           0.0  \n",
       "4963                             첫번째 사진에서 <PERSON>님 찾기!    0.8           0.0  \n",
       "394                                       이게 말이 됩니까????    4.0           1.0  \n",
       "6138                                   마지막 반전은 무슨.. ㅋㅋㅋ    1.2           0.0  \n",
       "5647                                 스시집...또...가고싶네요ㅋㅋ!    0.0           0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/miniconda3/envs/heejun-base/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n",
      "32000\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "tokenizer_small = transformers.AutoTokenizer.from_pretrained('klue/roberta-small', max_length=128)\n",
    "tokenizer_large = transformers.AutoTokenizer.from_pretrained('klue/roberta-large', max_length=128)\n",
    "print(tokenizer_small.vocab_size)\n",
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer_large.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 04:39:38.210252: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-25 04:39:38.226044: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 04:39:38.248826: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 04:39:38.255866: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 04:39:38.273223: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 04:39:39.270491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['맞춤법', '틀리', '##면', '외', '않', '##되', '?']\n",
      "Token IDs: [30810, 13535, 2460, 1462, 1380, 2496, 35]\n",
      "Tokens: ['맞춤법', '틀리', '##면', '외', '않', '##되', '?']\n",
      "Decoded: 맞춤법 틀리면 외 않되?\n",
      "Token ID 7591 corresponds to token: 걸렸\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"맞춤법 틀리면 외 않되?\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "tokens_1 = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "token_decode = tokenizer.decode(token_ids)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "print(f\"Tokens: {tokens_1}\")\n",
    "print(f\"Decoded: {token_decode}\")\n",
    "\n",
    "token_id = 7591\n",
    "token = tokenizer.decode([token_id])\n",
    "\n",
    "print(f\"Token ID {token_id} corresponds to token: {token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing2(self, dataframe):\n",
    "    data = []\n",
    "    for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "    # 두 입력 문장을 [SEP] 토큰으로 이어붙여서 전처리합니다.\n",
    "        text = '[SEP]'.join([item[text_column] for text_column in ['sentence_1', 'sentence_2']])\n",
    "        # padding=True와 truncation=True 옵션 추가\n",
    "        outputs = tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',  # max_length로 패딩을 고정\n",
    "            truncation=True,       # 텍스트를 최대 길이로 자름\n",
    "            max_length=128         # max_length 설정\n",
    "        )\n",
    "        data.append(outputs['input_ids'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100%|██████████| 10/10 [00:00<00:00, 2559.84it/s]\n"
     ]
    }
   ],
   "source": [
    "train_tokenized2 = tokenizing2(tokenizer, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '원', '##곡', '##도', '한', '번', '들어', '##보', '##시', '##죠', 'ㅎ', '-', '[SEP]', '<', 'PER', '##S', '##ON', '>', '님', '##도', '한', '번', '고려', '##해', '##보', '##시', '##죠', '!', 'ㅎㅎ', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "['[CLS]', '원', '##곡', '##도', '한', '번', '들어', '##보', '##시', '##죠', 'ㅎ', '-', '[SEP]', '<', 'PER', '##S', '##ON', '>', '님', '##도', '한', '번', '고려', '##해', '##보', '##시', '##죠', '!', 'ㅎㅎ', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokens1 = tokenizer.convert_ids_to_tokens(train_tokenized[0])\n",
    "tokens2 = tokenizer.convert_ids_to_tokens(train_tokenized2[0])\n",
    "print(tokens1)\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizing_at(self, dataframe):\n",
    "    data = []\n",
    "    attention_masks = []\n",
    "    for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "        # 두 입력 문장을 [SEP] 토큰으로 이어붙여서 전처리합니다.\n",
    "        text = '[SEP]'.join([item[text_column] for text_column in ['sentence_1', 'sentence_2']])\n",
    "        # padding=True와 truncation=True 옵션 추가\n",
    "        outputs = tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',  # max_length로 패딩을 고정\n",
    "            truncation=True,       # 텍스트를 최대 길이로 자름\n",
    "            max_length=50         # max_length 설정\n",
    "        )\n",
    "        data.append(outputs['input_ids'])\n",
    "        attention_masks.append(outputs['attention_mask'])  # Attention mask 추가\n",
    "    return data, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100%|██████████| 10/10 [00:00<00:00, 2995.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1478, 2465, 2119, 1891, 1141, 3685, 2178, 2067, 2321, 193, 17, 2, 32, 21639, 2238, 18946, 34, 805, 2119, 1891, 1141, 4146, 2097, 2178, 2067, 2321, 5, 5311, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenized, train_attention_masks = tokenizing_at(tokenizer, train)\n",
    "print(train_tokenized[0])\n",
    "print(train_attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100%|██████████| 10/10 [00:00<00:00, 3073.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizing_at(tokenizer, train)\n",
    "print(inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3], [4, 5, 6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iii = [1,2,3], [4,5,6]\n",
    "iii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6119</th>\n",
       "      <td>boostcamp-sts-v1-train-6119</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>원곡도 한 번 들어보시죠 ㅎ -</td>\n",
       "      <td>&lt;PERSON&gt; 님도 한 번 고려해보시죠! ㅎㅎ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6050</th>\n",
       "      <td>boostcamp-sts-v1-train-6050</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>영드를 보고 궁금해서 보게 됐는데, 드라마도 각색이 심해서 맘에 안들었지만 그래도 ...</td>\n",
       "      <td>궁금해서 영화를 봤는데 적응이 너무 안되어서 드라마를 좋아하지 않았지만 연기와 드라...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9159</th>\n",
       "      <td>boostcamp-sts-v1-train-9159</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>이러한 이야기를 스스럼 없이 나누고, 같이 공감할 수 있어서 더욱 즐거웠던것 같아요...</td>\n",
       "      <td>이런 이야기들을 주저 없이 공유하고 공감할 수 있어서 더 즐거웠던 것 같아요~~!</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>boostcamp-sts-v1-train-3291</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>대진침대 문제 해결 해주세요</td>\n",
       "      <td>대진침대 수거 누락건 해결부탁드립니다</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2415</th>\n",
       "      <td>boostcamp-sts-v1-train-2415</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>조기교육인가요 귀요미 ㅠㅠ</td>\n",
       "      <td>와 너무귀여워요 ㅠㅠ</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9034</th>\n",
       "      <td>boostcamp-sts-v1-train-9034</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>삼성은 노조를 인정하라.</td>\n",
       "      <td>삼성 이재용 부회장을 풀어주세요.</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4963</th>\n",
       "      <td>boostcamp-sts-v1-train-4963</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>[숨은 냥이 찾기] 삼청동 지붕 풍경. &lt;PERSON&gt; 님은 금방 찾으실듯.</td>\n",
       "      <td>첫번째 사진에서 &lt;PERSON&gt;님 찾기!</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>boostcamp-sts-v1-train-394</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>이게 말이나 된다고 생각합니까?</td>\n",
       "      <td>이게 말이 됩니까????</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6138</th>\n",
       "      <td>boostcamp-sts-v1-train-6138</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>마지막 반전은 진짜 지림</td>\n",
       "      <td>마지막 반전은 무슨.. ㅋㅋㅋ</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5647</th>\n",
       "      <td>boostcamp-sts-v1-train-5647</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>따쉬따쉬... 사진을 어딘가 잃어버려 돌아온 프로필 자랑타임!!</td>\n",
       "      <td>스시집...또...가고싶네요ㅋㅋ!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id            source  \\\n",
       "6119  boostcamp-sts-v1-train-6119     slack-sampled   \n",
       "6050  boostcamp-sts-v1-train-6050          nsmc-rtt   \n",
       "9159  boostcamp-sts-v1-train-9159         slack-rtt   \n",
       "3291  boostcamp-sts-v1-train-3291  petition-sampled   \n",
       "2415  boostcamp-sts-v1-train-2415     slack-sampled   \n",
       "9034  boostcamp-sts-v1-train-9034  petition-sampled   \n",
       "4963  boostcamp-sts-v1-train-4963     slack-sampled   \n",
       "394    boostcamp-sts-v1-train-394  petition-sampled   \n",
       "6138  boostcamp-sts-v1-train-6138      nsmc-sampled   \n",
       "5647  boostcamp-sts-v1-train-5647     slack-sampled   \n",
       "\n",
       "                                             sentence_1  \\\n",
       "6119                                  원곡도 한 번 들어보시죠 ㅎ -   \n",
       "6050  영드를 보고 궁금해서 보게 됐는데, 드라마도 각색이 심해서 맘에 안들었지만 그래도 ...   \n",
       "9159  이러한 이야기를 스스럼 없이 나누고, 같이 공감할 수 있어서 더욱 즐거웠던것 같아요...   \n",
       "3291                                    대진침대 문제 해결 해주세요   \n",
       "2415                                     조기교육인가요 귀요미 ㅠㅠ   \n",
       "9034                                      삼성은 노조를 인정하라.   \n",
       "4963         [숨은 냥이 찾기] 삼청동 지붕 풍경. <PERSON> 님은 금방 찾으실듯.   \n",
       "394                                   이게 말이나 된다고 생각합니까?   \n",
       "6138                                      마지막 반전은 진짜 지림   \n",
       "5647                따쉬따쉬... 사진을 어딘가 잃어버려 돌아온 프로필 자랑타임!!   \n",
       "\n",
       "                                             sentence_2  label  binary-label  \n",
       "6119                         <PERSON> 님도 한 번 고려해보시죠! ㅎㅎ    0.0           0.0  \n",
       "6050  궁금해서 영화를 봤는데 적응이 너무 안되어서 드라마를 좋아하지 않았지만 연기와 드라...    2.4           0.0  \n",
       "9159      이런 이야기들을 주저 없이 공유하고 공감할 수 있어서 더 즐거웠던 것 같아요~~!    4.4           1.0  \n",
       "3291                               대진침대 수거 누락건 해결부탁드립니다    2.6           1.0  \n",
       "2415                                        와 너무귀여워요 ㅠㅠ    1.2           0.0  \n",
       "9034                                 삼성 이재용 부회장을 풀어주세요.    0.6           0.0  \n",
       "4963                             첫번째 사진에서 <PERSON>님 찾기!    0.8           0.0  \n",
       "394                                       이게 말이 됩니까????    4.0           1.0  \n",
       "6138                                   마지막 반전은 무슨.. ㅋㅋㅋ    1.2           0.0  \n",
       "5647                                 스시집...또...가고싶네요ㅋㅋ!    0.0           0.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 768\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# klue/roberta-large 모델의 config 불러오기\n",
    "config = AutoConfig.from_pretrained(\"klue/roberta-base\")\n",
    "\n",
    "# hidden size 확인\n",
    "print(\"Hidden size:\", config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 0.1788, -0.0276]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "plm = transformers.AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-base\")\n",
    "\n",
    "# simple example\n",
    "input_ids = torch.tensor([[1, 2, 3, 4, 5]])\n",
    "attention_mask = torch.tensor([[1, 1, 1, 1, 1]])\n",
    "outputs = plm(input_ids=input_ids, attention_mask=attention_mask)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.0.attention.self.query.weight\n",
      "0\n",
      "roberta.encoder.layer.0.attention.self.query.bias\n",
      "0\n",
      "roberta.encoder.layer.0.attention.self.key.weight\n",
      "0\n",
      "roberta.encoder.layer.0.attention.self.key.bias\n",
      "0\n",
      "roberta.encoder.layer.0.attention.self.value.weight\n",
      "0\n",
      "roberta.encoder.layer.0.attention.self.value.bias\n",
      "0\n",
      "roberta.encoder.layer.0.attention.output.dense.weight\n",
      "0\n",
      "roberta.encoder.layer.0.attention.output.dense.bias\n",
      "0\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "0\n",
      "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "0\n",
      "roberta.encoder.layer.0.intermediate.dense.weight\n",
      "0\n",
      "roberta.encoder.layer.0.intermediate.dense.bias\n",
      "0\n",
      "roberta.encoder.layer.0.output.dense.weight\n",
      "0\n",
      "roberta.encoder.layer.0.output.dense.bias\n",
      "0\n",
      "roberta.encoder.layer.0.output.LayerNorm.weight\n",
      "0\n",
      "roberta.encoder.layer.0.output.LayerNorm.bias\n",
      "0\n",
      "roberta.encoder.layer.1.attention.self.query.weight\n",
      "1\n",
      "roberta.encoder.layer.1.attention.self.query.bias\n",
      "1\n",
      "roberta.encoder.layer.1.attention.self.key.weight\n",
      "1\n",
      "roberta.encoder.layer.1.attention.self.key.bias\n",
      "1\n",
      "roberta.encoder.layer.1.attention.self.value.weight\n",
      "1\n",
      "roberta.encoder.layer.1.attention.self.value.bias\n",
      "1\n",
      "roberta.encoder.layer.1.attention.output.dense.weight\n",
      "1\n",
      "roberta.encoder.layer.1.attention.output.dense.bias\n",
      "1\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "1\n",
      "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "1\n",
      "roberta.encoder.layer.1.intermediate.dense.weight\n",
      "1\n",
      "roberta.encoder.layer.1.intermediate.dense.bias\n",
      "1\n",
      "roberta.encoder.layer.1.output.dense.weight\n",
      "1\n",
      "roberta.encoder.layer.1.output.dense.bias\n",
      "1\n",
      "roberta.encoder.layer.1.output.LayerNorm.weight\n",
      "1\n",
      "roberta.encoder.layer.1.output.LayerNorm.bias\n",
      "1\n",
      "roberta.encoder.layer.2.attention.self.query.weight\n",
      "2\n",
      "roberta.encoder.layer.2.attention.self.query.bias\n",
      "2\n",
      "roberta.encoder.layer.2.attention.self.key.weight\n",
      "2\n",
      "roberta.encoder.layer.2.attention.self.key.bias\n",
      "2\n",
      "roberta.encoder.layer.2.attention.self.value.weight\n",
      "2\n",
      "roberta.encoder.layer.2.attention.self.value.bias\n",
      "2\n",
      "roberta.encoder.layer.2.attention.output.dense.weight\n",
      "2\n",
      "roberta.encoder.layer.2.attention.output.dense.bias\n",
      "2\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2\n",
      "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2\n",
      "roberta.encoder.layer.2.intermediate.dense.weight\n",
      "2\n",
      "roberta.encoder.layer.2.intermediate.dense.bias\n",
      "2\n",
      "roberta.encoder.layer.2.output.dense.weight\n",
      "2\n",
      "roberta.encoder.layer.2.output.dense.bias\n",
      "2\n",
      "roberta.encoder.layer.2.output.LayerNorm.weight\n",
      "2\n",
      "roberta.encoder.layer.2.output.LayerNorm.bias\n",
      "2\n",
      "roberta.encoder.layer.3.attention.self.query.weight\n",
      "3\n",
      "roberta.encoder.layer.3.attention.self.query.bias\n",
      "3\n",
      "roberta.encoder.layer.3.attention.self.key.weight\n",
      "3\n",
      "roberta.encoder.layer.3.attention.self.key.bias\n",
      "3\n",
      "roberta.encoder.layer.3.attention.self.value.weight\n",
      "3\n",
      "roberta.encoder.layer.3.attention.self.value.bias\n",
      "3\n",
      "roberta.encoder.layer.3.attention.output.dense.weight\n",
      "3\n",
      "roberta.encoder.layer.3.attention.output.dense.bias\n",
      "3\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "3\n",
      "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "3\n",
      "roberta.encoder.layer.3.intermediate.dense.weight\n",
      "3\n",
      "roberta.encoder.layer.3.intermediate.dense.bias\n",
      "3\n",
      "roberta.encoder.layer.3.output.dense.weight\n",
      "3\n",
      "roberta.encoder.layer.3.output.dense.bias\n",
      "3\n",
      "roberta.encoder.layer.3.output.LayerNorm.weight\n",
      "3\n",
      "roberta.encoder.layer.3.output.LayerNorm.bias\n",
      "3\n",
      "roberta.encoder.layer.4.attention.self.query.weight\n",
      "4\n",
      "roberta.encoder.layer.4.attention.self.query.bias\n",
      "4\n",
      "roberta.encoder.layer.4.attention.self.key.weight\n",
      "4\n",
      "roberta.encoder.layer.4.attention.self.key.bias\n",
      "4\n",
      "roberta.encoder.layer.4.attention.self.value.weight\n",
      "4\n",
      "roberta.encoder.layer.4.attention.self.value.bias\n",
      "4\n",
      "roberta.encoder.layer.4.attention.output.dense.weight\n",
      "4\n",
      "roberta.encoder.layer.4.attention.output.dense.bias\n",
      "4\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "4\n",
      "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "4\n",
      "roberta.encoder.layer.4.intermediate.dense.weight\n",
      "4\n",
      "roberta.encoder.layer.4.intermediate.dense.bias\n",
      "4\n",
      "roberta.encoder.layer.4.output.dense.weight\n",
      "4\n",
      "roberta.encoder.layer.4.output.dense.bias\n",
      "4\n",
      "roberta.encoder.layer.4.output.LayerNorm.weight\n",
      "4\n",
      "roberta.encoder.layer.4.output.LayerNorm.bias\n",
      "4\n",
      "roberta.encoder.layer.5.attention.self.query.weight\n",
      "5\n",
      "roberta.encoder.layer.5.attention.self.query.bias\n",
      "5\n",
      "roberta.encoder.layer.5.attention.self.key.weight\n",
      "5\n",
      "roberta.encoder.layer.5.attention.self.key.bias\n",
      "5\n",
      "roberta.encoder.layer.5.attention.self.value.weight\n",
      "5\n",
      "roberta.encoder.layer.5.attention.self.value.bias\n",
      "5\n",
      "roberta.encoder.layer.5.attention.output.dense.weight\n",
      "5\n",
      "roberta.encoder.layer.5.attention.output.dense.bias\n",
      "5\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "5\n",
      "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "5\n",
      "roberta.encoder.layer.5.intermediate.dense.weight\n",
      "5\n",
      "roberta.encoder.layer.5.intermediate.dense.bias\n",
      "5\n",
      "roberta.encoder.layer.5.output.dense.weight\n",
      "5\n",
      "roberta.encoder.layer.5.output.dense.bias\n",
      "5\n",
      "roberta.encoder.layer.5.output.LayerNorm.weight\n",
      "5\n",
      "roberta.encoder.layer.5.output.LayerNorm.bias\n",
      "5\n",
      "roberta.encoder.layer.6.attention.self.query.weight\n",
      "6\n",
      "roberta.encoder.layer.6.attention.self.query.bias\n",
      "6\n",
      "roberta.encoder.layer.6.attention.self.key.weight\n",
      "6\n",
      "roberta.encoder.layer.6.attention.self.key.bias\n",
      "6\n",
      "roberta.encoder.layer.6.attention.self.value.weight\n",
      "6\n",
      "roberta.encoder.layer.6.attention.self.value.bias\n",
      "6\n",
      "roberta.encoder.layer.6.attention.output.dense.weight\n",
      "6\n",
      "roberta.encoder.layer.6.attention.output.dense.bias\n",
      "6\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "6\n",
      "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "6\n",
      "roberta.encoder.layer.6.intermediate.dense.weight\n",
      "6\n",
      "roberta.encoder.layer.6.intermediate.dense.bias\n",
      "6\n",
      "roberta.encoder.layer.6.output.dense.weight\n",
      "6\n",
      "roberta.encoder.layer.6.output.dense.bias\n",
      "6\n",
      "roberta.encoder.layer.6.output.LayerNorm.weight\n",
      "6\n",
      "roberta.encoder.layer.6.output.LayerNorm.bias\n",
      "6\n",
      "roberta.encoder.layer.7.attention.self.query.weight\n",
      "7\n",
      "roberta.encoder.layer.7.attention.self.query.bias\n",
      "7\n",
      "roberta.encoder.layer.7.attention.self.key.weight\n",
      "7\n",
      "roberta.encoder.layer.7.attention.self.key.bias\n",
      "7\n",
      "roberta.encoder.layer.7.attention.self.value.weight\n",
      "7\n",
      "roberta.encoder.layer.7.attention.self.value.bias\n",
      "7\n",
      "roberta.encoder.layer.7.attention.output.dense.weight\n",
      "7\n",
      "roberta.encoder.layer.7.attention.output.dense.bias\n",
      "7\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "7\n",
      "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "7\n",
      "roberta.encoder.layer.7.intermediate.dense.weight\n",
      "7\n",
      "roberta.encoder.layer.7.intermediate.dense.bias\n",
      "7\n",
      "roberta.encoder.layer.7.output.dense.weight\n",
      "7\n",
      "roberta.encoder.layer.7.output.dense.bias\n",
      "7\n",
      "roberta.encoder.layer.7.output.LayerNorm.weight\n",
      "7\n",
      "roberta.encoder.layer.7.output.LayerNorm.bias\n",
      "7\n",
      "roberta.encoder.layer.8.attention.self.query.weight\n",
      "8\n",
      "roberta.encoder.layer.8.attention.self.query.bias\n",
      "8\n",
      "roberta.encoder.layer.8.attention.self.key.weight\n",
      "8\n",
      "roberta.encoder.layer.8.attention.self.key.bias\n",
      "8\n",
      "roberta.encoder.layer.8.attention.self.value.weight\n",
      "8\n",
      "roberta.encoder.layer.8.attention.self.value.bias\n",
      "8\n",
      "roberta.encoder.layer.8.attention.output.dense.weight\n",
      "8\n",
      "roberta.encoder.layer.8.attention.output.dense.bias\n",
      "8\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "8\n",
      "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "8\n",
      "roberta.encoder.layer.8.intermediate.dense.weight\n",
      "8\n",
      "roberta.encoder.layer.8.intermediate.dense.bias\n",
      "8\n",
      "roberta.encoder.layer.8.output.dense.weight\n",
      "8\n",
      "roberta.encoder.layer.8.output.dense.bias\n",
      "8\n",
      "roberta.encoder.layer.8.output.LayerNorm.weight\n",
      "8\n",
      "roberta.encoder.layer.8.output.LayerNorm.bias\n",
      "8\n",
      "roberta.encoder.layer.9.attention.self.query.weight\n",
      "9\n",
      "roberta.encoder.layer.9.attention.self.query.bias\n",
      "9\n",
      "roberta.encoder.layer.9.attention.self.key.weight\n",
      "9\n",
      "roberta.encoder.layer.9.attention.self.key.bias\n",
      "9\n",
      "roberta.encoder.layer.9.attention.self.value.weight\n",
      "9\n",
      "roberta.encoder.layer.9.attention.self.value.bias\n",
      "9\n",
      "roberta.encoder.layer.9.attention.output.dense.weight\n",
      "9\n",
      "roberta.encoder.layer.9.attention.output.dense.bias\n",
      "9\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "9\n",
      "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "9\n",
      "roberta.encoder.layer.9.intermediate.dense.weight\n",
      "9\n",
      "roberta.encoder.layer.9.intermediate.dense.bias\n",
      "9\n",
      "roberta.encoder.layer.9.output.dense.weight\n",
      "9\n",
      "roberta.encoder.layer.9.output.dense.bias\n",
      "9\n",
      "roberta.encoder.layer.9.output.LayerNorm.weight\n",
      "9\n",
      "roberta.encoder.layer.9.output.LayerNorm.bias\n",
      "9\n",
      "roberta.encoder.layer.10.attention.self.query.weight\n",
      "10\n",
      "roberta.encoder.layer.10.attention.self.query.bias\n",
      "10\n",
      "roberta.encoder.layer.10.attention.self.key.weight\n",
      "10\n",
      "roberta.encoder.layer.10.attention.self.key.bias\n",
      "10\n",
      "roberta.encoder.layer.10.attention.self.value.weight\n",
      "10\n",
      "roberta.encoder.layer.10.attention.self.value.bias\n",
      "10\n",
      "roberta.encoder.layer.10.attention.output.dense.weight\n",
      "10\n",
      "roberta.encoder.layer.10.attention.output.dense.bias\n",
      "10\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "10\n",
      "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "10\n",
      "roberta.encoder.layer.10.intermediate.dense.weight\n",
      "10\n",
      "roberta.encoder.layer.10.intermediate.dense.bias\n",
      "10\n",
      "roberta.encoder.layer.10.output.dense.weight\n",
      "10\n",
      "roberta.encoder.layer.10.output.dense.bias\n",
      "10\n",
      "roberta.encoder.layer.10.output.LayerNorm.weight\n",
      "10\n",
      "roberta.encoder.layer.10.output.LayerNorm.bias\n",
      "10\n",
      "roberta.encoder.layer.11.attention.self.query.weight\n",
      "11\n",
      "roberta.encoder.layer.11.attention.self.query.bias\n",
      "11\n",
      "roberta.encoder.layer.11.attention.self.key.weight\n",
      "11\n",
      "roberta.encoder.layer.11.attention.self.key.bias\n",
      "11\n",
      "roberta.encoder.layer.11.attention.self.value.weight\n",
      "11\n",
      "roberta.encoder.layer.11.attention.self.value.bias\n",
      "11\n",
      "roberta.encoder.layer.11.attention.output.dense.weight\n",
      "11\n",
      "roberta.encoder.layer.11.attention.output.dense.bias\n",
      "11\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "11\n",
      "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "11\n",
      "roberta.encoder.layer.11.intermediate.dense.weight\n",
      "11\n",
      "roberta.encoder.layer.11.intermediate.dense.bias\n",
      "11\n",
      "roberta.encoder.layer.11.output.dense.weight\n",
      "11\n",
      "roberta.encoder.layer.11.output.dense.bias\n",
      "11\n",
      "roberta.encoder.layer.11.output.LayerNorm.weight\n",
      "11\n",
      "roberta.encoder.layer.11.output.LayerNorm.bias\n",
      "11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, param in plm.named_parameters():\n",
    "    if 'layer' in name:\n",
    "        print(name)\n",
    "        layer_num = int(name.split('.')[3])  # layer.{n}에서 n 추출\n",
    "        print(layer_num)\n",
    "        \n",
    "# num layers\n",
    "num_layers = plm.config.num_hidden_layers\n",
    "num_layers\n",
    "freeze_range = (num_layers // 4) * 3\n",
    "freeze_range  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 24\n",
      "Number of layers: 12\n"
     ]
    }
   ],
   "source": [
    "# check the number of layers\n",
    "config = AutoConfig.from_pretrained(\"klue/roberta-large\")\n",
    "print(\"Number of layers:\", config.num_hidden_layers)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"klue/roberta-base\")\n",
    "print(\"Number of layers:\", config.num_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1257126/1438697405.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('/data/ephemeral/home/heejun/STS/lightning_logs/0.9314_klue-roberta-large/checkpoints/epoch=4-step=3750.ckpt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('/data/ephemeral/home/heejun/STS/lightning_logs/0.9314_klue-roberta-large/checkpoints/epoch=4-step=3750.ckpt')\n",
    "\n",
    "# 만약 전체 모델을 저장했다면\n",
    "# model = checkpoint['model']\n",
    "print(checkpoint.keys())\n",
    "\n",
    "# .pt 파일로 저장\n",
    "torch.save(checkpoint, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/ephemeral/home/heejun/STS/lightning_logs/0.9314_klue-roberta-large/output-0.9314_klue-roberta-large.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boostcamp-sts-v1-test-000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-test-001</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boostcamp-sts-v1-test-002</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boostcamp-sts-v1-test-003</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boostcamp-sts-v1-test-004</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>boostcamp-sts-v1-test-1095</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>boostcamp-sts-v1-test-1096</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>boostcamp-sts-v1-test-1097</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>boostcamp-sts-v1-test-1098</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>boostcamp-sts-v1-test-1099</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  target\n",
       "0      boostcamp-sts-v1-test-000     4.0\n",
       "1      boostcamp-sts-v1-test-001     4.4\n",
       "2      boostcamp-sts-v1-test-002     2.2\n",
       "3      boostcamp-sts-v1-test-003     0.5\n",
       "4      boostcamp-sts-v1-test-004     3.7\n",
       "...                          ...     ...\n",
       "1095  boostcamp-sts-v1-test-1095     1.8\n",
       "1096  boostcamp-sts-v1-test-1096     4.3\n",
       "1097  boostcamp-sts-v1-test-1097     3.9\n",
       "1098  boostcamp-sts-v1-test-1098     4.2\n",
       "1099  boostcamp-sts-v1-test-1099     5.1\n",
       "\n",
       "[1100 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "path = '/data/ephemeral/home/heejun/STS/lightning_logs/0.9314_klue-roberta-large/output-0.9314_klue-roberta-large.csv'\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값 1.51 는 가장 가까운 라벨 1.5 로 변환됩니다.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 주어진 데이터\n",
    "labels = np.array([1.4, 1.5, 1.6])  # 가능한 라벨 값들\n",
    "counts = np.array([17, 4, 18])  # 각 라벨에 대한 데이터 개수\n",
    "\n",
    "# 예측값\n",
    "predicted_value = 1.51\n",
    "\n",
    "# 라벨 분포에 따른 확률 계산\n",
    "def find_closest_label(predicted_value, labels, counts):\n",
    "    # 라벨 간의 거리를 계산하고, 각 라벨에 대한 가중치(빈도)를 적용하여 확률 계산\n",
    "    distances = np.abs(labels - predicted_value)\n",
    "    weighted_distances = distances / counts\n",
    "    \n",
    "    # 가장 가까운 라벨 선택\n",
    "    closest_label = labels[np.argmin(weighted_distances)]\n",
    "    return closest_label\n",
    "\n",
    "closest_label = find_closest_label(predicted_value, labels, counts)\n",
    "print(\"예측값\", predicted_value, \"는 가장 가까운 라벨\", closest_label, \"로 변환됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0. , 0.2, 0.4, 0.5, 0.6, 0.8, 1. , 1.2, 1.4, 1.5, 1.6, 1.8, 2. ,\n",
       "        2.2, 2.4, 2.6, 2.8, 3. , 3.2, 3.4, 3.5, 3.6, 3.8, 4. , 4.2, 4.4,\n",
       "        4.5, 4.6, 4.8, 5. ]),\n",
       " array([21,  7, 16,  2, 20, 22, 22, 27, 17,  4, 18, 22, 22, 23, 21, 22, 22,\n",
       "        22, 22, 22,  3, 19, 22, 22, 25, 19,  7, 15, 22, 22]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "valid = pd.read_csv('../data/dev.csv')\n",
    "valid['label'].value_counts().sort_index()\n",
    "\n",
    "labels = np.array(valid['label'].value_counts().sort_index().index)\n",
    "labels\n",
    "counts = np.array(valid['label'].value_counts().sort_index().values)\n",
    "labels, counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_label(predicted_value):\n",
    "    labels = np.array([0. , 0.2, 0.4, 0.5, 0.6, 0.8, 1. , 1.2, 1.4, 1.5, 1.6, 1.8, 2. ,\n",
    "        2.2, 2.4, 2.6, 2.8, 3. , 3.2, 3.4, 3.5, 3.6, 3.8, 4. , 4.2, 4.4,\n",
    "        4.5, 4.6, 4.8, 5. ])\n",
    "    counts = np.array([21,  7, 16,  2, 20, 22, 22, 27, 17,  4, 18, 22, 22, 23, 21, 22, 22,\n",
    "        22, 22, 22,  3, 19, 22, 22, 25, 19,  7, 15, 22, 22])\n",
    "    \n",
    "    distances = np.abs(labels - predicted_value)\n",
    "    weighted_distances = distances / counts\n",
    "    \n",
    "    # 가장 가까운 라벨 선택\n",
    "    closest_label = labels[np.argmin(weighted_distances)]\n",
    "    return closest_label\n",
    "\n",
    "closest_label = find_closest_label(1.51)\n",
    "print(\"예측값\", predicted_value, \"는 가장 가까운 라벨\", closest_label, \"로 변환됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값 1.51 는 가장 가까운 라벨 1.4 로 변환됩니다.\n"
     ]
    }
   ],
   "source": [
    "closest_label = find_closest_label(1.51)\n",
    "print(\"예측값\", predicted_value, \"는 가장 가까운 라벨\", closest_label, \"로 변환됩니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output1 = pd.read_csv('models/output-0.933421_klue-roberta-large.pt.csv')\n",
    "output2 = pd.read_csv('models/output2-0.933421_klue-roberta-large.pt.csv')\n",
    "\n",
    "output1['real_label'] = output2['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>real_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boostcamp-sts-v1-test-000</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-test-001</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boostcamp-sts-v1-test-002</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boostcamp-sts-v1-test-003</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boostcamp-sts-v1-test-004</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>boostcamp-sts-v1-test-1095</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>boostcamp-sts-v1-test-1096</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>boostcamp-sts-v1-test-1097</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>boostcamp-sts-v1-test-1098</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>boostcamp-sts-v1-test-1099</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  target  real_label\n",
       "0      boostcamp-sts-v1-test-000     3.8         3.9\n",
       "1      boostcamp-sts-v1-test-001     4.4         4.4\n",
       "2      boostcamp-sts-v1-test-002     1.5         1.4\n",
       "3      boostcamp-sts-v1-test-003     0.2         0.2\n",
       "4      boostcamp-sts-v1-test-004     3.8         3.8\n",
       "...                          ...     ...         ...\n",
       "1095  boostcamp-sts-v1-test-1095     2.0         2.0\n",
       "1096  boostcamp-sts-v1-test-1096     4.2         4.2\n",
       "1097  boostcamp-sts-v1-test-1097     4.2         4.2\n",
       "1098  boostcamp-sts-v1-test-1098     4.2         4.2\n",
       "1099  boostcamp-sts-v1-test-1099     5.1         5.1\n",
       "\n",
       "[1100 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array([0. , 0.2, 0.4, 0.5, 0.6, 0.8, 1. , 1.2, 1.4, 1.5, 1.6, 1.8, 2. ,\n",
    "    2.2, 2.4, 2.6, 2.8, 3. , 3.2, 3.4, 3.5, 3.6, 3.8, 4. , 4.2, 4.4,\n",
    "    4.5, 4.6, 4.8, 5. ])\n",
    "counts = np.array([21,  7, 16,  2, 20, 22, 22, 27, 17,  4, 18, 22, 22, 23, 21, 22, 22,\n",
    "    22, 22, 22,  3, 19, 22, 22, 25, 19,  7, 15, 22, 22])\n",
    "\n",
    "counts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(-0.0 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([0. , 0.2, 0.4, 0.5, 0.6, 0.8, 1. , 1.2, 1.4, 1.5, 1.6, 1.8, 2. ,\n",
    "    2.2, 2.4, 2.6, 2.8, 3. , 3.2, 3.4, 3.5, 3.6, 3.8, 4. , 4.2, 4.4,\n",
    "    4.5, 4.6, 4.8, 5. ])\n",
    "counts = np.array([21,  7, 16,  2, 20, 22, 22, 27, 17,  4, 18, 22, 22, 23, 21, 22, 22,\n",
    "    22, 22, 22,  3, 19, 22, 22, 25, 19,  7, 15, 22, 22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.8, 4.4, 1.4, 0.2, 3.8, 3.4, 4.2, 1.8, 0.5, 4.0, 1.2, 3.0, 3.5, 2.2, 1.2, 4.6, 5.0, 2.8, 5.0, 4.5, 3.0, 2.8, 2.2, 0.8, 1.8, 2.2, 1.2, 1.5, 3.8, 2.6, 4.8, 0.8, 2.6, 3.8, 2.8, 0.6, 3.6, 0.4, 1.2, 3.2, 0.6, 4.2, 0.0, 1.4, 2.8, 5.0, 2.4, 1.2, 0.4, 3.5, 0.0, 1.5, 3.0, 3.0, 4.2, 2.4, 3.8, 1.2, 2.2, 4.2, 4.2, 3.8, 1.2, 1.8, 2.4, 5.0, 3.6, 0.6, 2.8, 3.2, 0.8, 3.2, 5.0, 4.8, 0.0, 2.2, 3.2, 2.8, 2.8, 4.2, 1.2, 3.4, 1.6, 2.0, 1.2, 5.0, 4.2, 1.2, 4.0, 4.2, 4.6, 2.8, 0.8, 2.2, 5.0, 2.2, 3.2, 3.5, 0.5, 3.8, 1.6, 5.0, 4.8, 3.8, 4.2, 0.8, 3.6, 3.8, 2.8, 3.8, 3.5, 0.4, 0.0, 3.0, 2.2, 2.6, 0.4, 5.0, 2.8, 3.8, 4.2, 2.2, 2.6, 3.8, 3.4, 2.0, 4.2, 5.0, 4.2, 5.0, 2.8, 4.0, 5.0, 2.8, 5.0, 3.8, 1.4, 3.4, 2.6, 3.4, 4.4, 1.2, 2.8, 5.0, 1.0, 0.8, 1.0, 1.0, 0.8, 4.2, 1.2, 5.0, 2.4, 2.8, 0.8, 1.6, 5.0, 4.8, 5.0, 2.2, 4.5, 0.4, 3.5, 1.8, 3.8, 5.0, 0.8, 4.2, 0.6, 4.4, 3.5, 2.6, 4.8, 1.8, 4.2, 1.8, 0.0, 5.0, 1.8, 4.2, 2.8, 2.8, 2.2, 0.4, 3.0, 4.4, 3.2, 4.2, 0.0, 3.8, 3.0, 1.8, 1.2, 2.6, 2.0, 1.8, 1.6, 4.8, 3.8, 0.8, 3.5, 3.2, 4.8, 0.4, 2.4, 2.2, 1.5, 1.8, 0.8, 3.2, 2.6, 2.8, 1.2, 1.5, 4.2, 4.0, 3.8, 1.8, 1.5, 0.2, 3.8, 3.4, 3.5, 3.0, 5.0, 4.2, 4.2, 3.8, 3.2, 0.2, 1.2, 1.2, 4.0, 1.6, 0.0, 1.5, 2.2, 2.2, 3.0, 2.8, 1.5, 1.0, 0.2, 5.0, 2.2, 4.4, 3.2, 4.2, 4.5, 4.2, 0.8, 1.0, 0.0, 3.5, 3.2, 5.0, 2.8, 1.2, 4.5, 3.0, 4.8, 4.8, 4.5, 5.0, 4.4, 3.8, 4.6, 2.8, 1.2, 0.0, 4.8, 1.6, 2.6, 5.0, 4.0, 4.8, 2.2, 1.8, 2.8, 4.8, 1.0, 4.0, 4.5, 0.5, 3.8, 1.8, 1.4, 3.8, 0.0, 0.0, 0.6, 0.4, 4.0, 3.4, 2.0, 3.0, 3.8, 0.0, 5.0, 0.8, 5.0, 2.8, 1.2, 3.8, 3.8, 2.2, 3.2, 5.0, 0.8, 1.2, 4.8, 2.2, 0.5, 5.0, 3.0, 2.8, 4.2, 0.8, 5.0, 1.2, 1.0, 1.2, 4.6, 0.8, 0.4, 4.8, 4.2, 1.6, 3.2, 0.8, 1.8, 4.2, 4.0, 5.0, 2.0, 4.2, 4.0, 1.8, 5.0, 2.2, 2.0, 5.0, 3.0, 2.6, 5.0, 3.8, 2.2, 2.6, 3.0, 1.2, 5.0, 3.2, 2.2, 0.6, 0.0, 2.4, 4.0, 1.0, 2.0, 4.2, 4.2, 1.8, 3.0, 0.0, 2.8, 5.0, 2.8, 1.4, 1.2, 0.8, 1.5, 4.4, 3.8, 0.2, 1.2, 4.6, 2.2, 3.4, 5.0, 4.8, 2.2, 0.6, 2.8, 2.6, 1.6, 1.8, 3.8, 1.8, 0.8, 2.2, 4.2, 2.8, 1.2, 5.0, 4.4, 4.2, 0.8, 4.2, 2.2, 0.8, 2.8, 2.0, 1.8, 5.0, 5.0, 1.8, 4.2, 1.2, 3.8, 5.0, 3.2, 1.8, 0.8, 4.2, 3.4, 0.4, 3.4, 3.0, 5.0, 1.8, 4.2, 3.8, 3.8, 4.2, 2.8, 2.0, 2.0, 5.0, 4.2, 3.5, 1.0, 1.4, 2.2, 1.5, 0.2, 2.8, 3.0, 2.0, 2.8, 3.8, 0.4, 2.2, 1.2, 3.4, 4.2, 0.6, 3.8, 1.8, 4.6, 4.0, 3.4, 1.2, 4.2, 2.2, 2.4, 1.8, 1.0, 2.2, 0.4, 2.8, 0.0, 4.2, 5.0, 3.2, 1.2, 0.0, 2.2, 1.2, 2.8, 1.8, 2.8, 2.2, 4.2, 2.0, 2.8, 0.0, 5.0, 1.8, 2.2, 1.8, 4.5, 1.5, 5.0, 5.0, 3.0, 3.2, 1.6, 3.8, 0.4, 1.2, 4.2, 2.4, 2.6, 0.8, 4.0, 0.0, 5.0, 5.0, 3.4, 2.8, 0.0, 5.0, 4.2, 3.5, 5.0, 1.2, 1.8, 0.6, 0.0, 4.2, 1.8, 1.2, 0.2, 2.6, 1.4, 0.8, 2.0, 2.2, 5.0, 1.2, 0.8, 2.8, 2.2, 2.8, 1.8, 3.6, 1.8, 1.2, 1.8, 4.6, 5.0, 1.4, 0.0, 1.2, 1.5, 4.8, 4.2, 0.4, 1.6, 0.8, 1.2, 4.5, 2.8, 1.2, 1.8, 2.2, 5.0, 5.0, 3.4, 2.2, 1.2, 0.6, 3.0, 3.0, 5.0, 3.8, 1.2, 1.8, 2.2, 1.5, 4.5, 4.8, 1.2, 2.2, 1.6, 3.8, 3.6, 4.2, 4.0, 5.0, 1.8, 2.6, 3.8, 3.0, 2.6, 4.5, 4.5, 0.4, 0.4, 2.8, 3.8, 0.0, 0.4, 4.8, 0.6, 4.5, 3.2, 0.8, 4.8, 0.5, 2.8, 3.5, 2.6, 4.2, 3.8, 1.2, 4.2, 4.0, 2.8, 2.8, 1.8, 4.5, 4.8, 3.0, 4.6, 0.0, 2.6, 2.6, 0.2, 1.8, 4.2, 4.5, 3.8, 1.2, 0.8, 3.8, 3.0, 3.0, 3.8, 2.2, 2.0, 2.4, 2.0, 3.8, 4.4, 0.4, 4.2, 5.0, 2.6, 0.2, 1.2, 1.8, 1.8, 3.8, 0.0, 2.0, 2.6, 1.2, 4.2, 4.5, 2.6, 5.0, 1.4, 3.2, 2.6, 0.4, 4.6, 3.5, 0.8, 2.6, 4.2, 1.8, 4.2, 0.6, 2.4, 2.6, 5.0, 1.0, 2.0, 4.8, 4.4, 0.0, 2.8, 3.4, 1.5, 2.0, 1.6, 3.2, 2.2, 4.8, 2.2, 4.2, 2.8, 5.0, 2.2, 4.2, 2.6, 2.8, 5.0, 3.8, 0.8, 1.4, 1.5, 3.8, 3.8, 3.6, 5.0, 4.2, 5.0, 1.8, 4.2, 3.8, 0.2, 5.0, 2.6, 3.8, 2.8, 3.8, 4.4, 0.8, 5.0, 0.4, 2.8, 1.8, 2.4, 3.4, 2.6, 4.2, 3.5, 5.0, 1.2, 2.2, 5.0, 3.2, 3.2, 0.0, 3.0, 4.8, 4.2, 5.0, 4.4, 5.0, 0.4, 0.0, 0.4, 1.2, 1.4, 4.4, 4.0, 5.0, 2.2, 1.8, 2.4, 4.2, 2.6, 4.2, 2.2, 4.2, 2.8, 2.6, 2.6, 1.8, 2.6, 4.5, 3.4, 3.8, 4.2, 3.0, 3.8, 1.5, 3.8, 4.5, 0.0, 2.8, 1.2, 2.2, 4.2, 1.2, 3.8, 1.2, 3.8, 1.5, 4.2, 5.0, 4.0, 1.0, 2.2, 4.2, 5.0, 2.8, 4.6, 1.8, 0.8, 2.4, 0.2, 3.6, 4.2, 3.0, 2.6, 0.8, 1.2, 3.8, 0.8, 2.8, 4.8, 2.6, 2.8, 4.2, 2.0, 0.2, 2.6, 3.6, 2.2, 1.2, 2.8, 5.0, 2.6, 4.2, 0.6, 3.0, 0.2, 3.0, 3.0, 2.2, 2.2, 0.2, 1.2, 3.0, 3.8, 4.2, 4.2, 4.2, 2.4, 3.8, 2.2, 1.8, 2.2, 3.2, 4.2, 4.2, 1.8, 1.2, 1.8, 4.2, 0.4, 0.8, 4.2, 4.8, 3.2, 3.0, 1.8, 1.2, 3.4, 1.2, 2.6, 3.0, 4.8, 4.2, 3.8, 0.8, 1.2, 4.5, 4.4, 4.2, 1.0, 3.8, 5.0, 4.2, 4.2, 1.4, 2.4, 1.4, 4.0, 1.4, 4.5, 4.4, 2.8, 1.2, 1.6, 1.2, 5.0, 1.4, 4.0, 4.2, 5.0, 3.6, 1.6, 0.0, 3.4, 0.8, 4.6, 3.0, 2.2, 3.8, 2.8, 1.2, 5.0, 1.5, 3.5, 1.2, 4.2, 4.5, 3.0, 4.4, 3.8, 4.2, 2.8, 4.2, 2.2, 5.0, 4.2, 2.2, 1.8, 4.2, 0.8, 4.2, 2.8, 1.8, 0.0, 3.5, 0.8, 1.4, 1.2, 3.5, 1.8, 3.0, 4.2, 1.4, 0.8, 4.2, 4.4, 0.4, 2.8, 4.0, 1.6, 2.8, 3.2, 4.5, 4.4, 4.4, 3.0, 4.2, 3.8, 5.0, 5.0, 0.5, 1.5, 5.0, 5.0, 4.8, 5.0, 4.2, 1.2, 1.6, 4.2, 0.0, 3.0, 4.2, 1.8, 4.2, 4.8, 4.2, 3.8, 2.4, 3.5, 0.0, 1.6, 2.6, 5.0, 3.4, 3.8, 4.2, 0.8, 4.4, 3.8, 4.5, 1.8, 2.6, 3.2, 2.2, 4.0, 3.8, 4.0, 5.0, 4.2, 3.0, 1.5, 4.0, 3.6, 0.4, 2.2, 1.6, 2.2, 3.4, 3.0, 2.4, 2.2, 3.5, 3.6, 3.8, 1.4, 4.2, 4.2, 1.6, 4.4, 1.8, 1.6, 3.4, 5.0, 3.2, 1.8, 5.0, 4.5, 1.2, 1.8, 1.2, 2.8, 3.8, 3.2, 3.5, 4.4, 4.4, 1.4, 3.8, 5.0, 3.0, 4.2, 0.0, 2.8, 0.8, 0.4, 5.0, 1.5, 2.8, 3.8, 3.2, 3.8, 4.4, 1.4, 3.6, 3.8, 0.8, 0.5, 3.8, 4.2, 3.8, 1.6, 4.2, 2.0, 4.6, 2.2, 5.0, 2.6, 5.0, 4.2, 2.0, 2.2, 0.6, 3.2, 5.0, 4.0, 1.2, 3.8, 3.8, 3.5, 4.2, 4.6, 3.8, 1.0, 2.2, 4.8, 4.5, 1.2, 0.2, 1.2, 4.2, 4.6, 0.8, 1.2, 4.4, 5.0, 2.0, 5.0, 3.8, 1.5, 1.2, 2.2, 4.0, 4.2, 1.8, 4.5, 0.4, 5.0, 2.6, 3.6, 1.8, 3.6, 0.0, 4.2, 3.0, 4.2, 3.8, 3.8, 1.8, 1.0, 2.2, 1.6, 3.8, 2.8, 1.2, 0.0, 4.6, 2.4, 4.2, 4.6, 2.0, 4.2, 4.2, 4.2, 5.0]\n",
      "0.0 5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_closest_label(predicted_value):\n",
    "    if predicted_value < 0:\n",
    "        value = 0.0\n",
    "    \n",
    "    elif predicted_value == -0.0:\n",
    "        value = 0.0\n",
    "        \n",
    "    elif predicted_value > 5:\n",
    "        value = 5.0\n",
    "        \n",
    "    elif (predicted_value*10) % 2 != 0:\n",
    "        labels = np.array([0. , 0.2, 0.4, 0.5, 0.6, 0.8, 1. , 1.2, 1.4, 1.5, 1.6, 1.8, 2. ,\n",
    "            2.2, 2.4, 2.6, 2.8, 3. , 3.2, 3.4, 3.5, 3.6, 3.8, 4. , 4.2, 4.4,\n",
    "            4.5, 4.6, 4.8, 5. ])\n",
    "        counts = np.array([21,  7, 16,  2, 20, 22, 22, 27, 17,  4, 18, 22, 22, 23, 21, 22, 22,\n",
    "            22, 22, 22,  3, 19, 22, 22, 25, 19,  7, 15, 22, 22])\n",
    "        \n",
    "        distances = np.abs(labels - predicted_value)\n",
    "        weighted_distances = distances / counts\n",
    "        \n",
    "        # 가장 가까운 라벨 선택\n",
    "        value = labels[np.argmin(weighted_distances)]\n",
    "    \n",
    "    else:\n",
    "        value = predicted_value\n",
    "        \n",
    "    return float(value)\n",
    "\n",
    "def process_list(input_list):\n",
    "    return [find_closest_label(x) for x in input_list]\n",
    "\n",
    "\n",
    "predictions = [3.9, 4.4, 1.4, 0.2, 3.8, 3.4, 4.2, 1.8, 0.5, 4.0, 1.2, 3.0, 3.5, 2.3, 1.1, 4.6, 5.1, 2.7, 5.2, 4.5, 3.1, 2.7, 2.3, 0.8, 1.9, 2.3, 1.3, 1.5, 3.9, 2.5, 4.8, 0.7, 2.6, 3.7, 2.9, 0.6, 3.6, 0.4, 1.2, 3.2, 0.6, 4.1, 0.0, 1.4, 2.8, 5.2, 2.4, 1.1, 0.4, 3.5, -0.0, 1.5, 3.0, 3.1, 4.3, 2.4, 3.8, 1.2, 2.3, 4.1, 4.3, 3.9, 1.2, 1.7, 2.4, 5.2, 3.6, 0.6, 2.7, 3.2, 0.7, 3.3, 5.1, 4.7, -0.1, 2.2, 3.3, 2.9, 2.8, 4.1, 1.1, 3.4, 1.6, 2.0, 1.2, 4.9, 4.2, 1.1, 4.0, 4.3, 4.6, 2.7, 0.7, 2.3, 5.2, 2.2, 3.2, 3.5, 0.5, 3.7, 1.6, 5.1, 4.8, 3.8, 4.1, 0.8, 3.6, 3.8, 2.9, 3.9, 3.5, 0.4, 0.0, 3.0, 2.1, 2.5, 0.4, 5.2, 2.9, 3.9, 4.3, 2.3, 2.5, 3.8, 3.4, 2.0, 4.1, 5.2, 4.2, 5.2, 2.7, 4.0, 4.9, 2.9, 5.2, 3.9, 1.4, 3.4, 2.5, 3.4, 4.4, 1.2, 2.7, 5.0, 1.0, 0.9, 1.0, 1.0, 0.9, 4.2, 1.2, 5.2, 2.4, 2.8, 0.9, 1.6, 4.9, 4.8, 5.0, 2.2, 4.5, 0.4, 3.5, 1.8, 3.7, 5.1, 0.7, 4.2, 0.6, 4.4, 3.5, 2.6, 4.8, 1.7, 4.3, 1.7, 0.0, 5.1, 1.8, 4.2, 2.7, 2.8, 2.1, 0.4, 3.1, 4.4, 3.3, 4.1, -0.1, 3.7, 3.0, 1.7, 1.3, 2.5, 2.0, 1.8, 1.6, 4.7, 3.9, 0.8, 3.5, 3.2, 4.7, 0.3, 2.4, 2.2, 1.5, 1.8, 0.7, 3.2, 2.6, 2.9, 1.1, 1.5, 4.2, 4.0, 3.9, 1.7, 1.5, 0.2, 3.8, 3.4, 3.5, 3.1, 4.9, 4.1, 4.2, 3.8, 3.2, 0.2, 1.1, 1.2, 4.0, 1.6, -0.1, 1.5, 2.2, 2.3, 3.0, 2.8, 1.5, 1.0, 0.2, 5.1, 2.2, 4.4, 3.2, 4.3, 4.5, 4.3, 0.9, 1.0, 0.1, 3.5, 3.2, 5.2, 2.7, 1.3, 4.5, 3.0, 4.7, 4.8, 4.5, 5.2, 4.4, 3.7, 4.6, 2.7, 1.1, 0.1, 4.8, 1.6, 2.5, 5.2, 4.0, 4.7, 2.3, 1.9, 2.8, 4.8, 1.0, 4.0, 4.5, 0.5, 3.9, 1.8, 1.4, 3.9, -0.1, -0.1, 0.6, 0.3, 4.0, 3.4, 2.0, 3.0, 3.8, -0.0, 5.2, 0.9, 5.2, 2.8, 1.1, 3.7, 3.9, 2.2, 3.3, 5.2, 0.9, 1.2, 4.7, 2.1, 0.5, 5.2, 3.0, 2.7, 4.3, 0.9, 5.2, 1.2, 1.0, 1.1, 4.6, 0.8, 0.3, 4.7, 4.3, 1.6, 3.3, 0.9, 1.7, 4.2, 4.0, 5.2, 2.0, 4.2, 4.0, 1.8, 5.2, 2.3, 2.0, 5.2, 3.0, 2.5, 5.2, 3.7, 2.3, 2.5, 3.0, 1.3, 5.0, 3.3, 2.3, 0.6, 0.1, 2.4, 4.0, 1.0, 2.0, 4.1, 4.1, 1.9, 3.0, -0.1, 2.9, 5.2, 2.8, 1.4, 1.1, 0.9, 1.5, 4.4, 3.7, 0.2, 1.1, 4.6, 2.3, 3.4, 5.2, 4.7, 2.1, 0.6, 2.8, 2.6, 1.6, 1.8, 3.9, 1.7, 0.8, 2.1, 4.1, 2.8, 1.2, 5.2, 4.4, 4.1, 0.8, 4.3, 2.2, 0.8, 2.7, 2.0, 1.9, 5.2, 5.2, 1.8, 4.2, 1.3, 3.9, 5.1, 3.2, 1.7, 0.9, 4.3, 3.4, 0.3, 3.4, 3.1, 5.2, 1.9, 4.1, 3.9, 3.9, 4.3, 2.7, 2.0, 2.0, 5.2, 4.2, 3.5, 1.0, 1.4, 2.2, 1.5, 0.2, 2.7, 3.1, 2.0, 2.7, 3.8, 0.4, 2.3, 1.3, 3.4, 4.2, 0.6, 3.9, 1.8, 4.6, 4.0, 3.4, 1.2, 4.1, 2.2, 2.4, 1.8, 1.0, 2.2, 0.4, 2.8, 0.1, 4.2, 5.2, 3.3, 1.1, 0.1, 2.3, 1.3, 2.7, 1.7, 2.7, 2.3, 4.3, 2.0, 2.8, -0.0, 5.0, 1.8, 2.1, 1.7, 4.5, 1.5, 5.2, 5.0, 3.0, 3.2, 1.6, 3.7, 0.4, 1.3, 4.2, 2.4, 2.6, 0.8, 4.0, -0.0, 5.2, 5.2, 3.4, 2.9, -0.0, 5.2, 4.1, 3.5, 5.2, 1.3, 1.9, 0.6, 0.0, 4.2, 1.9, 1.2, 0.2, 2.5, 1.4, 0.9, 2.0, 2.2, 5.1, 1.1, 0.8, 2.8, 2.2, 2.9, 1.8, 3.6, 1.8, 1.3, 1.9, 4.6, 5.2, 1.4, 0.1, 1.3, 1.5, 4.8, 4.2, 0.3, 1.6, 0.9, 1.3, 4.5, 2.9, 1.3, 1.9, 2.1, 5.2, 5.2, 3.4, 2.2, 1.1, 0.6, 3.1, 3.1, 5.0, 3.9, 1.2, 1.8, 2.2, 1.5, 4.5, 4.7, 1.2, 2.2, 1.6, 3.9, 3.6, 4.1, 4.0, 5.2, 1.7, 2.5, 3.9, 3.1, 2.5, 4.5, 4.5, 0.3, 0.4, 2.8, 3.9, 0.1, 0.4, 4.7, 0.6, 4.5, 3.2, 0.7, 4.7, 0.5, 2.9, 3.5, 2.5, 4.2, 3.9, 1.2, 4.3, 4.0, 2.8, 2.8, 1.8, 4.5, 4.8, 3.1, 4.6, -0.1, 2.6, 2.6, 0.2, 1.8, 4.2, 4.5, 3.9, 1.2, 0.7, 3.8, 3.1, 3.0, 3.9, 2.1, 2.0, 2.4, 2.0, 3.9, 4.4, 0.3, 4.1, 5.0, 2.5, 0.2, 1.2, 1.9, 1.8, 3.8, 0.1, 2.0, 2.6, 1.2, 4.2, 4.5, 2.5, 4.9, 1.4, 3.3, 2.6, 0.4, 4.6, 3.5, 0.7, 2.5, 4.3, 1.9, 4.2, 0.6, 2.4, 2.5, 5.2, 1.0, 2.0, 4.7, 4.4, 0.0, 2.9, 3.4, 1.5, 2.0, 1.6, 3.2, 2.3, 4.7, 2.3, 4.2, 2.8, 4.9, 2.1, 4.2, 2.6, 2.9, 5.2, 3.9, 0.7, 1.4, 1.5, 3.8, 3.8, 3.6, 5.0, 4.3, 5.1, 1.9, 4.3, 3.9, 0.2, 5.2, 2.5, 3.8, 2.9, 3.8, 4.4, 0.9, 5.2, 0.3, 2.9, 1.7, 2.4, 3.4, 2.5, 4.2, 3.5, 5.2, 1.1, 2.1, 5.2, 3.2, 3.3, -0.0, 3.1, 4.8, 4.1, 5.2, 4.4, 5.1, 0.4, -0.1, 0.4, 1.1, 1.4, 4.4, 4.0, 5.2, 2.1, 1.9, 2.4, 4.1, 2.6, 4.3, 2.1, 4.3, 2.8, 2.5, 2.6, 1.7, 2.6, 4.5, 3.4, 3.9, 4.1, 3.0, 3.9, 1.5, 3.9, 4.5, -0.1, 2.8, 1.2, 2.3, 4.1, 1.2, 3.7, 1.2, 3.9, 1.5, 4.1, 5.2, 4.0, 1.0, 2.2, 4.1, 5.1, 2.7, 4.6, 1.8, 0.7, 2.4, 0.2, 3.6, 4.3, 3.1, 2.5, 0.9, 1.3, 3.9, 0.9, 2.8, 4.7, 2.5, 2.9, 4.2, 2.0, 0.2, 2.6, 3.6, 2.3, 1.3, 2.7, 5.2, 2.6, 4.2, 0.6, 3.0, 0.2, 3.0, 3.1, 2.1, 2.2, 0.2, 1.3, 3.0, 3.9, 4.2, 4.1, 4.1, 2.4, 3.7, 2.1, 1.7, 2.3, 3.2, 4.3, 4.2, 1.9, 1.3, 1.9, 4.1, 0.3, 0.9, 4.1, 4.8, 3.2, 3.0, 1.9, 1.1, 3.4, 1.2, 2.5, 3.0, 4.7, 4.1, 3.9, 0.7, 1.1, 4.5, 4.4, 4.2, 1.0, 3.7, 5.2, 4.3, 4.3, 1.4, 2.4, 1.4, 4.0, 1.4, 4.5, 4.4, 2.9, 1.1, 1.6, 1.2, 5.2, 1.4, 4.0, 4.3, 5.2, 3.6, 1.6, 0.0, 3.4, 0.9, 4.6, 3.0, 2.1, 3.7, 2.7, 1.3, 5.2, 1.5, 3.5, 1.3, 4.1, 4.5, 3.0, 4.4, 3.9, 4.3, 2.9, 4.1, 2.3, 4.9, 4.1, 2.1, 1.9, 4.2, 0.7, 4.2, 2.7, 1.8, -0.1, 3.5, 0.7, 1.4, 1.3, 3.5, 1.9, 3.0, 4.1, 1.4, 0.9, 4.2, 4.4, 0.4, 2.8, 4.0, 1.6, 2.7, 3.2, 4.5, 4.4, 4.4, 3.1, 4.1, 3.7, 5.2, 5.2, 0.5, 1.5, 5.1, 5.1, 4.7, 5.2, 4.1, 1.3, 1.6, 4.2, -0.1, 3.0, 4.2, 1.7, 4.2, 4.7, 4.3, 3.9, 2.4, 3.5, -0.1, 1.6, 2.5, 5.2, 3.4, 3.7, 4.1, 0.9, 4.4, 3.9, 4.5, 1.7, 2.6, 3.3, 2.3, 4.0, 3.7, 4.0, 5.2, 4.3, 3.1, 1.5, 4.0, 3.6, 0.3, 2.3, 1.6, 2.1, 3.4, 3.0, 2.4, 2.2, 3.5, 3.6, 3.8, 1.4, 4.1, 4.3, 1.6, 4.4, 1.7, 1.6, 3.4, 5.2, 3.2, 1.9, 5.0, 4.5, 1.3, 1.8, 1.2, 2.9, 3.8, 3.2, 3.5, 4.4, 4.4, 1.4, 3.8, 5.2, 3.1, 4.3, -0.1, 2.8, 0.9, 0.4, 5.1, 1.5, 2.8, 3.9, 3.3, 3.7, 4.4, 1.4, 3.6, 3.9, 0.8, 0.5, 3.7, 4.1, 3.9, 1.6, 4.2, 2.0, 4.6, 2.1, 5.2, 2.5, 5.2, 4.2, 2.0, 2.1, 0.6, 3.2, 5.1, 4.0, 1.1, 3.7, 3.9, 3.5, 4.3, 4.6, 3.8, 1.0, 2.1, 4.8, 4.5, 1.1, 0.2, 1.2, 4.2, 4.6, 0.9, 1.2, 4.4, 4.9, 2.0, 5.2, 3.7, 1.5, 1.2, 2.3, 4.0, 4.1, 1.7, 4.5, 0.4, 5.2, 2.5, 3.6, 1.7, 3.6, -0.0, 4.2, 3.1, 4.2, 3.9, 3.7, 1.7, 1.0, 2.2, 1.6, 3.8, 2.8, 1.2, 0.0, 4.6, 2.4, 4.3, 4.6, 2.0, 4.2, 4.2, 4.2, 5.1]\n",
    "p = process_list(predictions)\n",
    "print(p)\n",
    "print(min(p), max(p))\n",
    "# pred = 2.7\n",
    "# for i in range(61):\n",
    "#     i = i/10\n",
    "#     closest_label = find_closest_label(i)\n",
    "#     # print(\"예측값\", i, \"는 가장 가까운 라벨\", closest_label, \"로 변환됩니다.\")\n",
    "#     print(type(closest_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.89999999999962"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare sum of target - real_label\n",
    "diff = sum(abs(output1['target'] - output1['real_label']))\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output2['target'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1.to_csv('models/compare.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = pd.read_csv('../data/sample_submission.csv')\n",
    "sample['target'].hist(bins=51)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
