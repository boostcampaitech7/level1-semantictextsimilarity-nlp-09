{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ÎÑ§Ïù¥Î≤Ñ ÎßûÏ∂§Î≤ï api ÏÇ¨Ïö©\n",
    "- Ïù¥Î™®Ìã∞ÏΩò ÏÇ¨ÎùºÏßê: 'üëåüëåüëå' -> ''\n",
    "- Ïã†Ï°∞Ïñ¥ Îì±ÏùÄ ÏïàÎ∞îÎÄú: 'Í∞¨ÏÑ±' -> 'Í∞¨ÏÑ±', 'ÌêàÏù¥Ïïº' -> 'ÌêàÏù¥Ïïº' Îì±\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import html\n",
    "from pykospacing import Spacing\n",
    "from tqdm import tqdm\n",
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "\n",
    "def prep_spacing(text):\n",
    "    spacing = Spacing()\n",
    "    return spacing(text)\n",
    "\n",
    "\n",
    "def prep_naver(text):\n",
    "    def get_passport_key():\n",
    "        url = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EB%A7%9E%EC%B6%A4%EB%B2%95%EA%B2%80%EC%82%AC%EA%B8%B0\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            match = re.search(r'passportKey=([a-zA-Z0-9-_]+)', html)\n",
    "            if match:\n",
    "                passport_key = match.group(1)\n",
    "                return passport_key\n",
    "            else:\n",
    "                raise ValueError(\"passportKey not found in the HTML response.\")\n",
    "        else:\n",
    "            raise ConnectionError(f\"Failed to fetch the page, status code: {response.status_code}\")\n",
    "\n",
    "    # ÎßûÏ∂§Î≤ï Í≤ÄÏÇ¨Î•º Ï≤òÎ¶¨ÌïòÎäî ÎÇ¥Î∂Ä Ìï®Ïàò\n",
    "    def _spell_check_request(text, passport_key):\n",
    "        payload = {\n",
    "            'passportKey': passport_key,\n",
    "            '_callback': passport_key,\n",
    "            'q': text,\n",
    "            'color_blindness': '0'\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'referer': 'https://search.naver.com/',\n",
    "        }\n",
    "\n",
    "        start_time = time.time()\n",
    "        r = requests.get(\"https://m.search.naver.com/p/csearch/ocontent/util/SpellerProxy\", params=payload, headers=headers)\n",
    "        passed_time = time.time() - start_time\n",
    "\n",
    "        json_match = re.search(r'\\{.*\\}', r.text)\n",
    "        if json_match:\n",
    "            json_data = json_match.group(0)\n",
    "            data = json.loads(json_data)\n",
    "            html = data['message']['result']['html']\n",
    "            return _remove_tags(html)\n",
    "        else:\n",
    "            raise ValueError(\"No JSON data found in the response.\")\n",
    "\n",
    "    def _remove_tags(text):\n",
    "        text = '<content>{}</content>'.format(text).replace('<br>','')\n",
    "        result = ''.join(re.sub(r'<[^>]+>', '', text))\n",
    "        return result\n",
    "\n",
    "    def check(text, passport_key):\n",
    "        try:\n",
    "            return _spell_check_request(text, passport_key)\n",
    "        except ValueError as e:\n",
    "            if 'No JSON data found in the response' in str(e):\n",
    "                print(\"passport_key expired, fetching a new one.\")\n",
    "                passport_key = get_passport_key()  # ÏÉàÎ°úÏö¥ passport_key Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "                return _spell_check_request(text, passport_key)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    passport_key = get_passport_key()\n",
    "    text = html.unescape(check(text, passport_key))\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def prep_repeats(text):\n",
    "    \n",
    "    # Î∞òÎ≥µÎêòÎäî Í∞êÌÉÑÏÇ¨ÎÇò ÎπÑÏä∑Ìïú ÌëúÌòÑÏùÑ Ï§ÑÏù¥Îäî Ìï®Ïàò\n",
    "    def normalize_text(text):\n",
    "        # Î∞òÎ≥µÎêòÎäî Í∏ÄÏûê(Í∞êÌÉÑÏÇ¨, ÏõÉÏùåÏÜåÎ¶¨ Îì±)Î•º ÏµúÎåÄ 2Í∞úÎ°ú Ï§ÑÏûÑ\n",
    "        normalized_text = repeat_normalize(text, num_repeats=2)\n",
    "        return normalized_text\n",
    "    \n",
    "    text = normalize_text(text)\n",
    "\n",
    "    # ÎäêÎÇåÌëú, Î¨ºÏùåÌëú, Ï†ê(.) Îì±Ïùò Î¨∏Ïû• Î∂ÄÌò∏ Î∞òÎ≥µÏùÑ ÏµúÎåÄ 2Í∞úÎ°ú Ï§ÑÏûÑ\n",
    "    text = re.sub(r'([!?.])\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    # ÌïúÍ∏Ä ÏûêÏùå, Î™®Ïùå Î∞òÎ≥µ (Ïòà: „Öã„Öã„Öã, „Ö†„Ö†„Ö†„Ö† Îì±)\n",
    "    text = re.sub(r'([„Ñ±-„Öé„Öè-„Ö£])\\1{2,}', r'\\1\\1', text)\n",
    "            \n",
    "    # Ïù¥Î™®ÏßÄÎÇò ÌäπÏàò Î¨∏ÏûêÏùò Î∞òÎ≥µ Ï§ÑÏù¥Í∏∞\n",
    "    text = re.sub(r'([\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F])\\1{2,}', r'\\1\\1', text)  # Ïù¥Î™®ÏßÄ\n",
    "    text = re.sub(r'([#$%&*])\\1{2,}', r'\\1\\1', text)  # ÌäπÏàòÎ¨∏Ïûê\n",
    "\n",
    "    # ÌëúÏ§Ä Í≥µÎ∞± Ïô∏ Îã§Î•∏ Í≥µÎ∞± Î¨∏ÏûêÎì§ÎèÑ Ï≤òÎ¶¨\n",
    "    text = re.sub(r'[\\u00A0\\u1680\\u2000-\\u200A\\u202F\\u205F\\u3000]', ' ', text)\n",
    "        \n",
    "    # Ïó¨Îü¨ Í∞úÏùò Í≥µÎ∞±ÏùÑ ÌïòÎÇòÏùò Í≥µÎ∞±ÏúºÎ°ú Ï§ÑÏûÑ\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess(df):\n",
    "    df['sentence_1'] = df['sentence_1'].apply(prep_repeats)\n",
    "    df['sentence_2'] = df['sentence_2'].apply(prep_repeats)\n",
    "    # df['sentence_1'] = df['sentence_1'].apply(prep_naver)\n",
    "    # df['sentence_2'] = df['sentence_2'].apply(prep_naver)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ÎÑ§Ïù¥Î≤Ñ ÎßûÏ∂§Î≤ï api ÏÇ¨Ïö©\n",
    "- Ïù¥Î™®Ìã∞ÏΩò ÏÇ¨ÎùºÏßê: 'üëåüëåüëå' -> ''\n",
    "- Ïã†Ï°∞Ïñ¥ Îì±ÏùÄ ÏïàÎ∞îÎÄú: 'Í∞¨ÏÑ±' -> 'Í∞¨ÏÑ±', 'ÌêàÏù¥Ïïº' -> 'ÌêàÏù¥Ïïº' Îì±\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "\n",
    "def check_spell(dataframe):\n",
    "    def get_passport_key():\n",
    "        url = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EB%A7%9E%EC%B6%A4%EB%B2%95%EA%B2%80%EC%82%AC%EA%B8%B0\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            match = re.search(r'passportKey=([a-zA-Z0-9-_]+)', html)\n",
    "            if match:\n",
    "                passport_key = match.group(1)\n",
    "                print(f\"passportKey found: {passport_key}\")\n",
    "                return passport_key\n",
    "            else:\n",
    "                raise ValueError(\"passportKey not found in the HTML response.\")\n",
    "        else:\n",
    "            raise ConnectionError(f\"Failed to fetch the page, status code: {response.status_code}\")\n",
    "\n",
    "    # ÎßûÏ∂§Î≤ï Í≤ÄÏÇ¨Î•º Ï≤òÎ¶¨ÌïòÎäî ÎÇ¥Î∂Ä Ìï®Ïàò\n",
    "    def _spell_check_request(text, passport_key):\n",
    "        payload = {\n",
    "            'passportKey': passport_key,\n",
    "            '_callback': passport_key,\n",
    "            'q': text,\n",
    "            'color_blindness': '0'\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'referer': 'https://search.naver.com/',\n",
    "        }\n",
    "\n",
    "        start_time = time.time()\n",
    "        r = requests.get(\"https://m.search.naver.com/p/csearch/ocontent/util/SpellerProxy\", params=payload, headers=headers)\n",
    "        passed_time = time.time() - start_time\n",
    "\n",
    "        json_match = re.search(r'\\{.*\\}', r.text)\n",
    "        if json_match:\n",
    "            json_data = json_match.group(0)\n",
    "            data = json.loads(json_data)\n",
    "            html = data['message']['result']['html']\n",
    "            return _remove_tags(html)\n",
    "        else:\n",
    "            raise ValueError(\"No JSON data found in the response.\")\n",
    "\n",
    "    def _remove_tags(text):\n",
    "        text = '<content>{}</content>'.format(text).replace('<br>','')\n",
    "        result = ''.join(re.sub(r'<[^>]+>', '', text))\n",
    "        return result\n",
    "\n",
    "    def check(text, passport_key):\n",
    "        try:\n",
    "            return _spell_check_request(text, passport_key)\n",
    "        except ValueError as e:\n",
    "            if 'No JSON data found in the response' in str(e):\n",
    "                print(\"passport_key expired, fetching a new one.\")\n",
    "                passport_key = get_passport_key()  # ÏÉàÎ°úÏö¥ passport_key Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "                return _spell_check_request(text, passport_key)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    passport_key = get_passport_key()\n",
    "\n",
    "    for i in tqdm(range(len(dataframe['sentence_1'])), desc='check_spell'):\n",
    "        dataframe.loc[i, 'sentence_1'] = html.unescape(check(dataframe.loc[i, 'sentence_1'], passport_key))\n",
    "        dataframe.loc[i, 'sentence_2'] = html.unescape(check(dataframe.loc[i, 'sentence_2'], passport_key))        \n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: 2caa5e5496fed85692709081d304c63cf6eaa3bc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9324/9324 [10:52<00:00, 14.28it/s]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train_preprop_v2 = preprocess(train)\n",
    "train_preprop_v2 = check_spell(train_preprop_v2)\n",
    "train_preprop_v2.to_csv('train_preprop_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: 2caa5e5496fed85692709081d304c63cf6eaa3bc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 550/550 [00:36<00:00, 15.09it/s]\n"
     ]
    }
   ],
   "source": [
    "dev = pd.read_csv('dev.csv')\n",
    "dev_preprop_v2 = preprocess(dev)\n",
    "dev_preprop_v2 = check_spell(dev_preprop_v2)\n",
    "dev_preprop_v2.to_csv('dev_preprop_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_preprop_v2 = pd.read_csv('dev_preprop_v2.csv')\n",
    "dev_preprop_v2_no_label = dev_preprop_v2.drop(columns=['label', 'binary-label'])\n",
    "dev_preprop_v2_no_label.to_csv('dev_preprop_v2_no_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: 2caa5e5496fed85692709081d304c63cf6eaa3bc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1100/1100 [01:22<00:00, 13.35it/s]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test_preprop_v2 = preprocess(test)\n",
    "test_preprop_v2 = check_spell(test_preprop_v2)\n",
    "test_preprop_v2.to_csv('test_preprop_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1340446/1849768944.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentence_1'] = df['sentence_1'].apply(prep_repeats)\n",
      "/tmp/ipykernel_1340446/1849768944.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentence_2'] = df['sentence_2'].apply(prep_repeats)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>boostcamp-sts-v1-train-020</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>ÏïûÎ®∏Î¶¨ ÏÉàÎ°ú ÌïòÏÖ®ÏäµÎãàÎã§. ^^</td>\n",
       "      <td>Í∞ÄÎ∞©Ïóê ÎÑ£Ïñ¥ Îã§ÎãàÎ©¥ÏÑú Ï°∞Í∏àÏî© Î®πÏäµÎãàÎã§. ^^</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>boostcamp-sts-v1-train-021</td>\n",
       "      <td>petition-rtt</td>\n",
       "      <td>ÍπÄÍ∏∞Îçï Ï°∞Ïû¨ÌòÑ ÏÑ±Ìè≠Ìñâ Ï≤†Ï†ÄÌûà ÏàòÏÇ¨Ìï¥ Ï£ºÏÑ∏Ïöî!</td>\n",
       "      <td>ÍπÄÍ∏∞Îçï¬∑Ï°∞Ïû¨ÌòÑ ÏÑ±Ìè≠Ìñâ ÏùòÌòπ Ï≤†Ï†ÄÌûà ÏàòÏÇ¨ÌïòÎùº!</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>boostcamp-sts-v1-train-022</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>ÎãµÎãµÌï† Îïå Î≥¥Î©¥ ÏÜçÏù¥ Îª• Îö´Î¶¥ Í≤É Í∞ôÏïÑÏöî</td>\n",
       "      <td>ÏñëÎ≥¥Îã® ÌïúÏûÖ Î®πÎäî ÏàúÍ∞Ñ Í≥†ÏÇê ÌíÄÎ¶¥ Í≤É Í∞ôÏïÑÏöî „Öã„Öã</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>boostcamp-sts-v1-train-023</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>ÎÖ∏ÎûòÏôÄ Ïûò Ïñ¥Ïö∞Îü¨ÏßÄÎäî ÏòÅÏÉÅ ÎçïÎ∂ÑÏù∏ÏßÄ ÏßßÏßÄÎßå Í∞ïÌïú Ïù∏ÏÉÅÏù¥ ÎÇ®ÎÑ§Ïöî..</td>\n",
       "      <td>Ï°∞Í∏à Ïú†ÏπòÌïòÏßÄÎßå Í∞ÄÎ≥çÍ≤å Î≥º ÏàòÎäî ÏûàÎäî ÏòÅÌôîÎÑ§Ïöî!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>boostcamp-sts-v1-train-024</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>Íµ∞ÎåÄ Í∞ÄÍ∏∞ Ï†ÑÏóê Î¥§ÏóàÎäîÎç∞ ÏßÑÏßú ÏúàÌÑ∞Ïä§ Í∞ôÏùÄ ÏÇ¨ÎûåÏù¥ ÏÉÅÍ¥ÄÏù¥Î©¥ Î™©Ïà® Í±∏Í≥† Ïã∏ÏõåÎèÑ ÌõÑÌöåÎäî...</td>\n",
       "      <td>ÏûÖÎåÄÌïòÍ∏∞ Ï†ÑÏóê Î¥§ÎäîÎç∞ ÏúàÌÑ∞Ïä§ Í∞ôÏùÄ ÏÇ¨ÎûåÏù¥ ÏßÑÏã¨ÏúºÎ°ú ÏïÑÍª¥Ï§ÄÎã§Î©¥ Î™©Ïà®ÏùÑ Í±∏Í≥† Ïã∏ÏõåÎèÑ ÌõÑ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>boostcamp-sts-v1-train-025</td>\n",
       "      <td>petition-rtt</td>\n",
       "      <td>Íµ≠ÎØºÏ≤≠ÏõêÏóê Ïò¨Î¶∞ Í∏Ä ÏÇ≠Ï†úÌïòÎäî Ï≤≠ÏôÄÎåÄ Îâ¥ÎØ∏ÎîîÏñ¥Ï†ïÏ±ÖÏã§ÏùÄ ÏñµÏö∏Ìïú ÌîºÌï¥ÏûêÎ•º Ï£ΩÏù¥Í≥† Í≤ΩÏ∞∞ÏóêÍ≤å...</td>\n",
       "      <td>Íµ≠ÎØºÏ≤≠Ïõê Í∏ÄÏùÑ ÏÇ≠Ï†úÌïòÎäî Ï≤≠ÏôÄÎåÄ Îâ¥ÎØ∏ÎîîÏñ¥Ï†ïÏ±ÖÏã§ÏùÄ Î∂ÄÎãπÌïú ÌîºÌï¥ÏûêÎ•º ÏÇ¥Ìï¥ÌïòÍ≥†, Í≤ΩÏ∞∞Ïóê Ï¶ù...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>boostcamp-sts-v1-train-026</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>Ï†ÑÎëêÌôòÏùÑ Ï≤òÎ≤åÌï¥ Ï£ºÏÑ∏Ïöî</td>\n",
       "      <td>Ïù¥Ïû¨Ïö©ÏùÑ Íµ¨ÏÜçÌï¥ Ï£ºÏÑ∏Ïöî</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>boostcamp-sts-v1-train-027</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>ÎßàÏßÄÎßâÏúºÎ°ú Î¶¨Î™®Ìä∏ Í∑ºÎ¨¥Ïùò Ïû•Ï†êÏóê ÎåÄÌï¥ Ïù¥ÏïºÍ∏∞ÌñàÎäîÎç∞, ÏãúÍ∞ÑÏùÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©Ìï† Ïàò Ïûà...</td>\n",
       "      <td>ÎßàÏßÄÎßâÏúºÎ°ú Ïû¨ÌÉùÍ∑ºÎ¨¥Ïùò Ïû•Ï†êÏóê ÎåÄÌï¥ Ïù¥ÏïºÍ∏∞Î•º ÎÇòÎàÑÏóàÍ≥†, ÏãúÍ∞ÑÏùÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>boostcamp-sts-v1-train-028</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>Í≤®Ïö∏ÏÇ∞Ïù¥ ÏòàÏÅòÏßÄÎßå ÏÇ∞ÏùÑ Ïûò Î™ª ÌÉÄÏÑú ÎåÄÎ¶¨ÎßåÏ°± Ï§ëÏûÖÎãàÎã§</td>\n",
       "      <td>Í≤®Ïö∏ÏÇ∞ÏùÄ ÏòàÏÅúÎç∞ Ï†úÍ∞Ä Îì±ÏÇ∞ÏùÑ Ïûò Î™ªÌï¥ÏÑú ÎåÄÎßåÏ°±ÏûÖÎãàÎã§.</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>boostcamp-sts-v1-train-029</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>Ìïú ÏÇ¨ÎûåÏùò ÌååÎ©∏ÏùÑ Ï†ÅÎÇòÎùºÌïòÍ≤å ÎìúÎü¨ÎÇ¥ Ï§Ä ÏòÅÌôî</td>\n",
       "      <td>Ìïú ÏÇ¨ÎûåÏùò ÌååÎ©∏ÏùÑ ÎìúÎü¨ÎÇ¥Îäî ÏòÅÌôî</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>boostcamp-sts-v1-train-030</td>\n",
       "      <td>petition-rtt</td>\n",
       "      <td>Í∏àÏúµÏúÑÏõêÌöåÏùò Í≥µÎ™®Ï£º Í∞úÏù∏ Î∞∞Ï†ï Ï∂ïÏÜå(ÌèêÏßÄ)Î•º ÎßâÏïÑÏ£ºÏÑ∏Ïöî</td>\n",
       "      <td>Í∏àÏúµÍ∞êÎèÖÏõêÏùò Í≥µÎ™®Ï£º Í∞úÏù∏ Î∞∞Î∂Ñ Ï∂ïÏÜå(ÌèêÏßÄ)Î•º ÎßâÏïÑÏ£ºÏÑ∏Ïöî.</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>boostcamp-sts-v1-train-031</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>ÏßÄÍ∏à ÏïÑÏ£º ÎßåÏ°±Ìï¥Ïöî.</td>\n",
       "      <td>ÏßÄÍ∏àÏùÄ Îß§Ïö∞ ÎßåÏ°±Ìï©ÎãàÎã§.</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>boostcamp-sts-v1-train-032</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>Ï†ÄÎèÑ ÎÑàÎ¨¥ Ï¢ãÏïÑÌïòÎäî ÎÖ∏Îûò„Öé„Öé</td>\n",
       "      <td>Ïò§! Ï†ÄÎèÑ Ï¢ãÏïÑÌïòÎäî ÎÖ∏Îûò „Öé„Öé</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>boostcamp-sts-v1-train-033</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>Ïûò ÎßåÎì† ÏòÅÌôîÍµ∞Ïöî.</td>\n",
       "      <td>Ïûò ÎßåÎì† ÏòÅÌôîÎùºÍ≥† ÏÉùÍ∞ÅÌïúÎã§.</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>boostcamp-sts-v1-train-034</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>Ï†ÑÏÑ§ Îî∞Îùº ÏÇºÏ≤úÎ¶¨(ÏÇºÎßå Î¶¨Ïù∏Í∞Ä?)Ïóê ÏùòÌïòÎ©¥ Ïù¥ ÏòÅÌôîÍ∞Ä ÍΩ§ Í¥úÏ∞ÆÎã§Í≥† Ìï† Îøê ÏïÑÎãàÎùº ÎÇ¥...</td>\n",
       "      <td>Ï†ÑÏÑ§Ïóê Îî∞Î•¥Î©¥ ÏÇºÏ≤úÎ¶¨(ÏÇºÏ≤úÎ¶¨?)Ïóê Îî∞Î•¥Î©¥ Ïù¥ ÏòÅÌôîÎäî ÍΩ§ Í¥úÏ∞ÆÏùÑ ÎøêÎßå ÏïÑÎãàÎùº ÎÇ¥Í∞Ä Ï†ú...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>boostcamp-sts-v1-train-035</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>Îòê Î®πÏñ¥ Î≥¥Í≥† Ïã∂ÎÑ§Ïöî.</td>\n",
       "      <td>ÏñºÎßàÎÇò ÎìúÏãúÍ≥† Ïã∂ÏóàÏúºÎ©¥‚Ä¶</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>boostcamp-sts-v1-train-036</td>\n",
       "      <td>petition-rtt</td>\n",
       "      <td>Î≥¥ÌóòÏù¥ Ïû¨ÏÇ∞ÏïïÎ•òÎ•º Ìï¥ÏÑúÎäî Ïïà Îê† ÏùºÏûÖÎãàÎã§.</td>\n",
       "      <td>Î≥¥ÌóòÏùÄ Ïû¨ÏÇ∞ÏùÑ Î™∞ÏàòÌï¥ÏÑúÎäî Ïïà Îê©ÎãàÎã§.</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>boostcamp-sts-v1-train-037</td>\n",
       "      <td>petition-rtt</td>\n",
       "      <td>ÏóëÏÜå Ïóò Ï†ÑÍ¥ëÌåê ÏÇ¨Í±¥ Í≥µÎ°†Ìôî</td>\n",
       "      <td>ÏóëÏÜå Ïóò ÎπåÎ≥¥Îìú ÏÇ¨Í±¥ Í≥µÍ∞ú</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>boostcamp-sts-v1-train-038</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>ÏïÑÎ¨¥Î¶¨ Í∑∏ÎûòÎèÑ Í±∏Ïñ¥Í∞ÑÎã§Îäî ÏÑ§Ï†ïÏùÄ Ï¢Ä-_-;</td>\n",
       "      <td>ÌïòÎÇò ÏïåÎ¶¨Ïä® Î°úÎ®ºÏù¥ Î≤óÏùÄ Í±¥ Ï∂©Í≤© -_-;;</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>boostcamp-sts-v1-train-039</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>Ìú¥ÍµêÎ†πÏùÑ ÎÇ¥Î†§Ï£ºÏÑ∏Ïöî</td>\n",
       "      <td>ÎåÄÍµ¨Í¥ëÏó≠Ïãú Ìú¥ÍµêÎ†π ÎÇ¥Î†§Ï£ºÏã≠ÏãúÏò§</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>boostcamp-sts-v1-train-040</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>Ïù¥Îü∞ Î™ÖÏûëÏùÑ Ïôú Ïù¥Ï†ú Î¥§ÏùÑÍπå..</td>\n",
       "      <td>Ïôú Ïù¥Îü∞ Î™ÖÏûëÏùÑ Ïù¥Ï†úÏïº Î≥∏ Í±∏Íπå</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>boostcamp-sts-v1-train-041</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>ÎÇòÏù¥Í∞Ä Î¨∏Ï†úÏù∏Í∞ÄÏöî?</td>\n",
       "      <td>Ï†ÄÎäî Î¨¥ÏóáÏù∏Í∞ÄÏöî??</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>boostcamp-sts-v1-train-042</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>ÏÜåÎÖÑÎ≤ïÏùÑ ÌèêÏßÄÌï¥ Ï£ºÏãúÍ∏∞ Î∞îÎûçÎãàÎã§</td>\n",
       "      <td>Ï≤≠ÏÜåÎÖÑ Î≥¥Ìò∏Î≤ï ÌèêÏßÄÌï¥ Ï£ºÏãúÍ∏∞ Î∞îÎûçÎãàÎã§.</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>boostcamp-sts-v1-train-043</td>\n",
       "      <td>petition-rtt</td>\n",
       "      <td>ÏùòÎ£åÍ≥Ñ ÌôòÍ≤Ω Í∞úÏÑ† Î∞è Í≤ΩÏ∞∞ ÏÜåÎ∞©Í≥µÎ¨¥Ïõê ÏßÄÏõêÌï¥ Ï£ºÏÑ∏Ïöî</td>\n",
       "      <td>ÏùòÎ£åÌôòÍ≤Ω Í∞úÏÑ†Í≥º Í≤ΩÏ∞∞¬∑ÏÜåÎ∞©Í¥Ä ÏßÄÏõê Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§.</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>boostcamp-sts-v1-train-044</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>Í≥µÎ¨¥ÏõêÏù¥ ÏàòÎãπÏùÑ Î∂ÄÏ†ïÏàòÎ†πÌïòÏßÄ ÏïäÎèÑÎ°ù Ìï¥Ï£ºÏÑ∏Ïöî</td>\n",
       "      <td>ÏµúÏ†ÄÏûÑÍ∏à Ïù∏ÏÉÅÏóê Îî∞Î•∏ Ï†ïÎ∂Ä ÏßÄÏõêÍ∏à Î∂ÄÏ†ïÏàòÍ∏âÏùÑ Ï†ÅÎ∞úÌïòÏó¨ Ï≤òÎ≤åÌï¥ Ï£ºÏã≠ÏãúÏò§</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>boostcamp-sts-v1-train-045</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>ÎÇÆ ÏÇ∞Ï±Ö Î¨¥Ï°∞Í±¥ Ï∂îÏ≤ú~!</td>\n",
       "      <td>ÎÇÆ ÏÇ∞Ï±ÖÏùÄ Íº≠ Ï∂îÏ≤úÌï©ÎãàÎã§!</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>boostcamp-sts-v1-train-046</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>Ï†ÄÎäî Ïôú ÏÜ•Ïóê Îì§Ïñ¥Í∞ÄÍ≥ÑÏãúÎÇò ÌñàÏñ¥Ïöî „Öã„Öã</td>\n",
       "      <td>Ï†ÄÎäî ÏÇ¨Í≥ºÎÇò Ïö∞Ïú†Ïóê ÌîÑÎ°úÌã¥ ÌÉÄÏÑú Î®πÏñ¥Ïöî „Öé„Öé</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>boostcamp-sts-v1-train-047</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>ÎèôÏÉùÏù¥Îûë Í∞ôÏù¥ Î¥§ÎäîÎç∞ ÏÇ¥Ïù∏Îç∞ Í∑∏Îã§ÏßÄ Î¨¥ÏÑúÏõåÌïòÏßÄ ÏïäÍ≥† Ïûò Î≥∏ Í≤É Í∞ôÏùå..</td>\n",
       "      <td>ÎÇ¥Ïö©ÎèÑ Í∑∏Î†áÍ≥† Î≥ÑÎ°ú ÎßòÏóê Îì§ÏßÄ ÏïäÎäîÎã§..</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>boostcamp-sts-v1-train-048</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>ÏïåÏΩîÏò¨Ï§ëÎèÖÏûêÍ∞Ä Í∏∞Ï¥àÏÉùÌôúÏàòÍ∏âÏûê?</td>\n",
       "      <td>Í∏∞Ï¥àÏÉùÌôúÏàòÍ∏âÏûê Ïû•Ïï†Ïù∏ÏùÑ ÏúÑÌïòÏó¨.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>boostcamp-sts-v1-train-049</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>Íµ¨ÏùòÏõê, ÏãúÏùòÏõê ÏÑ†Í±∞. Ïù¥Ï†ú Í∑∏ÎßåÎë°ÏãúÎã§</td>\n",
       "      <td>Íµ≠ÌöåÏùòÏõê, ÎèÑÏùòÏõê, ÏãúÏùòÏõê Îì§ 3ÏÑ† Ïù¥ÏÉÅ Ï∂úÎßà Í∏àÏßÄ</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id            source  \\\n",
       "20  boostcamp-sts-v1-train-020     slack-sampled   \n",
       "21  boostcamp-sts-v1-train-021      petition-rtt   \n",
       "22  boostcamp-sts-v1-train-022     slack-sampled   \n",
       "23  boostcamp-sts-v1-train-023      nsmc-sampled   \n",
       "24  boostcamp-sts-v1-train-024          nsmc-rtt   \n",
       "25  boostcamp-sts-v1-train-025      petition-rtt   \n",
       "26  boostcamp-sts-v1-train-026  petition-sampled   \n",
       "27  boostcamp-sts-v1-train-027         slack-rtt   \n",
       "28  boostcamp-sts-v1-train-028         slack-rtt   \n",
       "29  boostcamp-sts-v1-train-029          nsmc-rtt   \n",
       "30  boostcamp-sts-v1-train-030      petition-rtt   \n",
       "31  boostcamp-sts-v1-train-031         slack-rtt   \n",
       "32  boostcamp-sts-v1-train-032     slack-sampled   \n",
       "33  boostcamp-sts-v1-train-033      nsmc-sampled   \n",
       "34  boostcamp-sts-v1-train-034          nsmc-rtt   \n",
       "35  boostcamp-sts-v1-train-035     slack-sampled   \n",
       "36  boostcamp-sts-v1-train-036      petition-rtt   \n",
       "37  boostcamp-sts-v1-train-037      petition-rtt   \n",
       "38  boostcamp-sts-v1-train-038      nsmc-sampled   \n",
       "39  boostcamp-sts-v1-train-039  petition-sampled   \n",
       "40  boostcamp-sts-v1-train-040      nsmc-sampled   \n",
       "41  boostcamp-sts-v1-train-041  petition-sampled   \n",
       "42  boostcamp-sts-v1-train-042  petition-sampled   \n",
       "43  boostcamp-sts-v1-train-043      petition-rtt   \n",
       "44  boostcamp-sts-v1-train-044  petition-sampled   \n",
       "45  boostcamp-sts-v1-train-045         slack-rtt   \n",
       "46  boostcamp-sts-v1-train-046     slack-sampled   \n",
       "47  boostcamp-sts-v1-train-047      nsmc-sampled   \n",
       "48  boostcamp-sts-v1-train-048  petition-sampled   \n",
       "49  boostcamp-sts-v1-train-049  petition-sampled   \n",
       "\n",
       "                                           sentence_1  \\\n",
       "20                                   ÏïûÎ®∏Î¶¨ ÏÉàÎ°ú ÌïòÏÖ®ÏäµÎãàÎã§. ^^   \n",
       "21                           ÍπÄÍ∏∞Îçï Ï°∞Ïû¨ÌòÑ ÏÑ±Ìè≠Ìñâ Ï≤†Ï†ÄÌûà ÏàòÏÇ¨Ìï¥ Ï£ºÏÑ∏Ïöî!   \n",
       "22                             ÎãµÎãµÌï† Îïå Î≥¥Î©¥ ÏÜçÏù¥ Îª• Îö´Î¶¥ Í≤É Í∞ôÏïÑÏöî   \n",
       "23               ÎÖ∏ÎûòÏôÄ Ïûò Ïñ¥Ïö∞Îü¨ÏßÄÎäî ÏòÅÏÉÅ ÎçïÎ∂ÑÏù∏ÏßÄ ÏßßÏßÄÎßå Í∞ïÌïú Ïù∏ÏÉÅÏù¥ ÎÇ®ÎÑ§Ïöî..   \n",
       "24  Íµ∞ÎåÄ Í∞ÄÍ∏∞ Ï†ÑÏóê Î¥§ÏóàÎäîÎç∞ ÏßÑÏßú ÏúàÌÑ∞Ïä§ Í∞ôÏùÄ ÏÇ¨ÎûåÏù¥ ÏÉÅÍ¥ÄÏù¥Î©¥ Î™©Ïà® Í±∏Í≥† Ïã∏ÏõåÎèÑ ÌõÑÌöåÎäî...   \n",
       "25  Íµ≠ÎØºÏ≤≠ÏõêÏóê Ïò¨Î¶∞ Í∏Ä ÏÇ≠Ï†úÌïòÎäî Ï≤≠ÏôÄÎåÄ Îâ¥ÎØ∏ÎîîÏñ¥Ï†ïÏ±ÖÏã§ÏùÄ ÏñµÏö∏Ìïú ÌîºÌï¥ÏûêÎ•º Ï£ΩÏù¥Í≥† Í≤ΩÏ∞∞ÏóêÍ≤å...   \n",
       "26                                       Ï†ÑÎëêÌôòÏùÑ Ï≤òÎ≤åÌï¥ Ï£ºÏÑ∏Ïöî   \n",
       "27  ÎßàÏßÄÎßâÏúºÎ°ú Î¶¨Î™®Ìä∏ Í∑ºÎ¨¥Ïùò Ïû•Ï†êÏóê ÎåÄÌï¥ Ïù¥ÏïºÍ∏∞ÌñàÎäîÎç∞, ÏãúÍ∞ÑÏùÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©Ìï† Ïàò Ïûà...   \n",
       "28                      Í≤®Ïö∏ÏÇ∞Ïù¥ ÏòàÏÅòÏßÄÎßå ÏÇ∞ÏùÑ Ïûò Î™ª ÌÉÄÏÑú ÎåÄÎ¶¨ÎßåÏ°± Ï§ëÏûÖÎãàÎã§   \n",
       "29                           Ìïú ÏÇ¨ÎûåÏùò ÌååÎ©∏ÏùÑ Ï†ÅÎÇòÎùºÌïòÍ≤å ÎìúÎü¨ÎÇ¥ Ï§Ä ÏòÅÌôî   \n",
       "30                     Í∏àÏúµÏúÑÏõêÌöåÏùò Í≥µÎ™®Ï£º Í∞úÏù∏ Î∞∞Ï†ï Ï∂ïÏÜå(ÌèêÏßÄ)Î•º ÎßâÏïÑÏ£ºÏÑ∏Ïöî   \n",
       "31                                        ÏßÄÍ∏à ÏïÑÏ£º ÎßåÏ°±Ìï¥Ïöî.   \n",
       "32                                    Ï†ÄÎèÑ ÎÑàÎ¨¥ Ï¢ãÏïÑÌïòÎäî ÎÖ∏Îûò„Öé„Öé   \n",
       "33                                         Ïûò ÎßåÎì† ÏòÅÌôîÍµ∞Ïöî.   \n",
       "34  Ï†ÑÏÑ§ Îî∞Îùº ÏÇºÏ≤úÎ¶¨(ÏÇºÎßå Î¶¨Ïù∏Í∞Ä?)Ïóê ÏùòÌïòÎ©¥ Ïù¥ ÏòÅÌôîÍ∞Ä ÍΩ§ Í¥úÏ∞ÆÎã§Í≥† Ìï† Îøê ÏïÑÎãàÎùº ÎÇ¥...   \n",
       "35                                       Îòê Î®πÏñ¥ Î≥¥Í≥† Ïã∂ÎÑ§Ïöî.   \n",
       "36                            Î≥¥ÌóòÏù¥ Ïû¨ÏÇ∞ÏïïÎ•òÎ•º Ìï¥ÏÑúÎäî Ïïà Îê† ÏùºÏûÖÎãàÎã§.   \n",
       "37                                    ÏóëÏÜå Ïóò Ï†ÑÍ¥ëÌåê ÏÇ¨Í±¥ Í≥µÎ°†Ìôî   \n",
       "38                            ÏïÑÎ¨¥Î¶¨ Í∑∏ÎûòÎèÑ Í±∏Ïñ¥Í∞ÑÎã§Îäî ÏÑ§Ï†ïÏùÄ Ï¢Ä-_-;   \n",
       "39                                         Ìú¥ÍµêÎ†πÏùÑ ÎÇ¥Î†§Ï£ºÏÑ∏Ïöî   \n",
       "40                                  Ïù¥Îü∞ Î™ÖÏûëÏùÑ Ïôú Ïù¥Ï†ú Î¥§ÏùÑÍπå..   \n",
       "41                                         ÎÇòÏù¥Í∞Ä Î¨∏Ï†úÏù∏Í∞ÄÏöî?   \n",
       "42                                  ÏÜåÎÖÑÎ≤ïÏùÑ ÌèêÏßÄÌï¥ Ï£ºÏãúÍ∏∞ Î∞îÎûçÎãàÎã§   \n",
       "43                       ÏùòÎ£åÍ≥Ñ ÌôòÍ≤Ω Í∞úÏÑ† Î∞è Í≤ΩÏ∞∞ ÏÜåÎ∞©Í≥µÎ¨¥Ïõê ÏßÄÏõêÌï¥ Ï£ºÏÑ∏Ïöî   \n",
       "44                           Í≥µÎ¨¥ÏõêÏù¥ ÏàòÎãπÏùÑ Î∂ÄÏ†ïÏàòÎ†πÌïòÏßÄ ÏïäÎèÑÎ°ù Ìï¥Ï£ºÏÑ∏Ïöî   \n",
       "45                                      ÎÇÆ ÏÇ∞Ï±Ö Î¨¥Ï°∞Í±¥ Ï∂îÏ≤ú~!   \n",
       "46                              Ï†ÄÎäî Ïôú ÏÜ•Ïóê Îì§Ïñ¥Í∞ÄÍ≥ÑÏãúÎÇò ÌñàÏñ¥Ïöî „Öã„Öã   \n",
       "47            ÎèôÏÉùÏù¥Îûë Í∞ôÏù¥ Î¥§ÎäîÎç∞ ÏÇ¥Ïù∏Îç∞ Í∑∏Îã§ÏßÄ Î¨¥ÏÑúÏõåÌïòÏßÄ ÏïäÍ≥† Ïûò Î≥∏ Í≤É Í∞ôÏùå..   \n",
       "48                                   ÏïåÏΩîÏò¨Ï§ëÎèÖÏûêÍ∞Ä Í∏∞Ï¥àÏÉùÌôúÏàòÍ∏âÏûê?   \n",
       "49                              Íµ¨ÏùòÏõê, ÏãúÏùòÏõê ÏÑ†Í±∞. Ïù¥Ï†ú Í∑∏ÎßåÎë°ÏãúÎã§   \n",
       "\n",
       "                                           sentence_2  label  binary-label  \n",
       "20                           Í∞ÄÎ∞©Ïóê ÎÑ£Ïñ¥ Îã§ÎãàÎ©¥ÏÑú Ï°∞Í∏àÏî© Î®πÏäµÎãàÎã§. ^^    0.0           0.0  \n",
       "21                           ÍπÄÍ∏∞Îçï¬∑Ï°∞Ïû¨ÌòÑ ÏÑ±Ìè≠Ìñâ ÏùòÌòπ Ï≤†Ï†ÄÌûà ÏàòÏÇ¨ÌïòÎùº!    4.2           1.0  \n",
       "22                        ÏñëÎ≥¥Îã® ÌïúÏûÖ Î®πÎäî ÏàúÍ∞Ñ Í≥†ÏÇê ÌíÄÎ¶¥ Í≤É Í∞ôÏïÑÏöî „Öã„Öã    0.0           0.0  \n",
       "23                         Ï°∞Í∏à Ïú†ÏπòÌïòÏßÄÎßå Í∞ÄÎ≥çÍ≤å Î≥º ÏàòÎäî ÏûàÎäî ÏòÅÌôîÎÑ§Ïöî!    0.0           0.0  \n",
       "24  ÏûÖÎåÄÌïòÍ∏∞ Ï†ÑÏóê Î¥§ÎäîÎç∞ ÏúàÌÑ∞Ïä§ Í∞ôÏùÄ ÏÇ¨ÎûåÏù¥ ÏßÑÏã¨ÏúºÎ°ú ÏïÑÍª¥Ï§ÄÎã§Î©¥ Î™©Ïà®ÏùÑ Í±∏Í≥† Ïã∏ÏõåÎèÑ ÌõÑ...    4.2           1.0  \n",
       "25  Íµ≠ÎØºÏ≤≠Ïõê Í∏ÄÏùÑ ÏÇ≠Ï†úÌïòÎäî Ï≤≠ÏôÄÎåÄ Îâ¥ÎØ∏ÎîîÏñ¥Ï†ïÏ±ÖÏã§ÏùÄ Î∂ÄÎãπÌïú ÌîºÌï¥ÏûêÎ•º ÏÇ¥Ìï¥ÌïòÍ≥†, Í≤ΩÏ∞∞Ïóê Ï¶ù...    4.0           1.0  \n",
       "26                                       Ïù¥Ïû¨Ïö©ÏùÑ Íµ¨ÏÜçÌï¥ Ï£ºÏÑ∏Ïöî    0.2           0.0  \n",
       "27  ÎßàÏßÄÎßâÏúºÎ°ú Ïû¨ÌÉùÍ∑ºÎ¨¥Ïùò Ïû•Ï†êÏóê ÎåÄÌï¥ Ïù¥ÏïºÍ∏∞Î•º ÎÇòÎàÑÏóàÍ≥†, ÏãúÍ∞ÑÏùÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú ÏÇ¨Ïö©Ìï† Ïàò ...    4.2           1.0  \n",
       "28                      Í≤®Ïö∏ÏÇ∞ÏùÄ ÏòàÏÅúÎç∞ Ï†úÍ∞Ä Îì±ÏÇ∞ÏùÑ Ïûò Î™ªÌï¥ÏÑú ÎåÄÎßåÏ°±ÏûÖÎãàÎã§.    3.2           1.0  \n",
       "29                                  Ìïú ÏÇ¨ÎûåÏùò ÌååÎ©∏ÏùÑ ÎìúÎü¨ÎÇ¥Îäî ÏòÅÌôî    3.6           1.0  \n",
       "30                    Í∏àÏúµÍ∞êÎèÖÏõêÏùò Í≥µÎ™®Ï£º Í∞úÏù∏ Î∞∞Î∂Ñ Ï∂ïÏÜå(ÌèêÏßÄ)Î•º ÎßâÏïÑÏ£ºÏÑ∏Ïöî.    4.4           1.0  \n",
       "31                                      ÏßÄÍ∏àÏùÄ Îß§Ïö∞ ÎßåÏ°±Ìï©ÎãàÎã§.    4.4           1.0  \n",
       "32                                   Ïò§! Ï†ÄÎèÑ Ï¢ãÏïÑÌïòÎäî ÎÖ∏Îûò „Öé„Öé    4.0           1.0  \n",
       "33                                    Ïûò ÎßåÎì† ÏòÅÌôîÎùºÍ≥† ÏÉùÍ∞ÅÌïúÎã§.    3.6           1.0  \n",
       "34  Ï†ÑÏÑ§Ïóê Îî∞Î•¥Î©¥ ÏÇºÏ≤úÎ¶¨(ÏÇºÏ≤úÎ¶¨?)Ïóê Îî∞Î•¥Î©¥ Ïù¥ ÏòÅÌôîÎäî ÍΩ§ Í¥úÏ∞ÆÏùÑ ÎøêÎßå ÏïÑÎãàÎùº ÎÇ¥Í∞Ä Ï†ú...    4.0           1.0  \n",
       "35                                      ÏñºÎßàÎÇò ÎìúÏãúÍ≥† Ïã∂ÏóàÏúºÎ©¥‚Ä¶    1.2           0.0  \n",
       "36                               Î≥¥ÌóòÏùÄ Ïû¨ÏÇ∞ÏùÑ Î™∞ÏàòÌï¥ÏÑúÎäî Ïïà Îê©ÎãàÎã§.    3.4           1.0  \n",
       "37                                     ÏóëÏÜå Ïóò ÎπåÎ≥¥Îìú ÏÇ¨Í±¥ Í≥µÍ∞ú    0.8           0.0  \n",
       "38                           ÌïòÎÇò ÏïåÎ¶¨Ïä® Î°úÎ®ºÏù¥ Î≤óÏùÄ Í±¥ Ï∂©Í≤© -_-;;    0.0           0.0  \n",
       "39                                   ÎåÄÍµ¨Í¥ëÏó≠Ïãú Ìú¥ÍµêÎ†π ÎÇ¥Î†§Ï£ºÏã≠ÏãúÏò§    2.8           1.0  \n",
       "40                                  Ïôú Ïù¥Îü∞ Î™ÖÏûëÏùÑ Ïù¥Ï†úÏïº Î≥∏ Í±∏Íπå    4.2           1.0  \n",
       "41                                         Ï†ÄÎäî Î¨¥ÏóáÏù∏Í∞ÄÏöî??    0.0           0.0  \n",
       "42                              Ï≤≠ÏÜåÎÖÑ Î≥¥Ìò∏Î≤ï ÌèêÏßÄÌï¥ Ï£ºÏãúÍ∏∞ Î∞îÎûçÎãàÎã§.    3.8           1.0  \n",
       "43                         ÏùòÎ£åÌôòÍ≤Ω Í∞úÏÑ†Í≥º Í≤ΩÏ∞∞¬∑ÏÜåÎ∞©Í¥Ä ÏßÄÏõê Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§.    4.4           1.0  \n",
       "44             ÏµúÏ†ÄÏûÑÍ∏à Ïù∏ÏÉÅÏóê Îî∞Î•∏ Ï†ïÎ∂Ä ÏßÄÏõêÍ∏à Î∂ÄÏ†ïÏàòÍ∏âÏùÑ Ï†ÅÎ∞úÌïòÏó¨ Ï≤òÎ≤åÌï¥ Ï£ºÏã≠ÏãúÏò§    0.8           0.0  \n",
       "45                                     ÎÇÆ ÏÇ∞Ï±ÖÏùÄ Íº≠ Ï∂îÏ≤úÌï©ÎãàÎã§!    4.0           1.0  \n",
       "46                           Ï†ÄÎäî ÏÇ¨Í≥ºÎÇò Ïö∞Ïú†Ïóê ÌîÑÎ°úÌã¥ ÌÉÄÏÑú Î®πÏñ¥Ïöî „Öé„Öé    0.0           0.0  \n",
       "47                             ÎÇ¥Ïö©ÎèÑ Í∑∏Î†áÍ≥† Î≥ÑÎ°ú ÎßòÏóê Îì§ÏßÄ ÏïäÎäîÎã§..    0.0           0.0  \n",
       "48                                  Í∏∞Ï¥àÏÉùÌôúÏàòÍ∏âÏûê Ïû•Ïï†Ïù∏ÏùÑ ÏúÑÌïòÏó¨.    1.0           0.0  \n",
       "49                       Íµ≠ÌöåÏùòÏõê, ÎèÑÏùòÏõê, ÏãúÏùòÏõê Îì§ 3ÏÑ† Ïù¥ÏÉÅ Ï∂úÎßà Í∏àÏßÄ    0.8           0.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_temp: train row 10Í∞úÎßå ÎΩëÏïÑÏÑú ÌÖåÏä§Ìä∏\n",
    "train_temp = train[20:50]\n",
    "train_temp = preprocess(train_temp)\n",
    "train_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: 2caa5e5496fed85692709081d304c63cf6eaa3bc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell:   0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/heejun-base/lib/python3.9/site-packages/pandas/core/indexes/range.py:414\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_temp \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_spell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_temp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_temp\n",
      "Cell \u001b[0;32mIn[8], line 80\u001b[0m, in \u001b[0;36mcheck_spell\u001b[0;34m(dataframe)\u001b[0m\n\u001b[1;32m     77\u001b[0m passport_key \u001b[38;5;241m=\u001b[39m get_passport_key()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_1\u001b[39m\u001b[38;5;124m'\u001b[39m])), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheck_spell\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 80\u001b[0m     dataframe\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39munescape(check(\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, passport_key))\n\u001b[1;32m     81\u001b[0m     dataframe\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39munescape(check(dataframe\u001b[38;5;241m.\u001b[39mloc[i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_2\u001b[39m\u001b[38;5;124m'\u001b[39m], passport_key))        \n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataframe\n",
      "File \u001b[0;32m~/miniconda3/envs/heejun-base/lib/python3.9/site-packages/pandas/core/indexing.py:1146\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m-> 1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/heejun-base/lib/python3.9/site-packages/pandas/core/frame.py:4012\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   4006\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[1;32m   4008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[1;32m   4009\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[1;32m   4010\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[1;32m   4011\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[0;32m-> 4012\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[1;32m   4015\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[1;32m   4016\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/heejun-base/lib/python3.9/site-packages/pandas/core/indexes/range.py:416\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 416\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "train_temp = check_spell(train_temp)\n",
    "train_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original     : Ïó¨Ïö¥Ïù¥ ÎÇ®ÎäîÎã§ÎùºÎäî ÌëúÌòÑÏùÑ Ïù¥Îü¥Îïå Ïì∞ÎäîÍ≤ÉÏù¥Ï£†!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'repeat_normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# s2 = 'ÏàòÎßéÏùÄÎ∞òÎ°ÄÏùÑÏù¥Í≤®ÎÉáÎî∞'\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal     : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m s2_r \u001b[38;5;241m=\u001b[39m \u001b[43mprep_repeats\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter repeats: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms2_r\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m s2_rs \u001b[38;5;241m=\u001b[39m prep_spacing(s2_r)\n",
      "Cell \u001b[0;32mIn[1], line 96\u001b[0m, in \u001b[0;36mprep_repeats\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     93\u001b[0m     normalized_text \u001b[38;5;241m=\u001b[39m repeat_normalize(text, num_repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m normalized_text\n\u001b[0;32m---> 96\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# ÎäêÎÇåÌëú, Î¨ºÏùåÌëú, Ï†ê(.) Îì±Ïùò Î¨∏Ïû• Î∂ÄÌò∏ Î∞òÎ≥µÏùÑ ÏµúÎåÄ 2Í∞úÎ°ú Ï§ÑÏûÑ\u001b[39;00m\n\u001b[1;32m     99\u001b[0m text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([!?.])\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m2,}\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "Cell \u001b[0;32mIn[1], line 93\u001b[0m, in \u001b[0;36mprep_repeats.<locals>.normalize_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize_text\u001b[39m(text):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Î∞òÎ≥µÎêòÎäî Í∏ÄÏûê(Í∞êÌÉÑÏÇ¨, ÏõÉÏùåÏÜåÎ¶¨ Îì±)Î•º ÏµúÎåÄ 2Í∞úÎ°ú Ï§ÑÏûÑ\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     normalized_text \u001b[38;5;241m=\u001b[39m \u001b[43mrepeat_normalize\u001b[49m(text, num_repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m normalized_text\n",
      "\u001b[0;31mNameError\u001b[0m: name 'repeat_normalize' is not defined"
     ]
    }
   ],
   "source": [
    "n = 120\n",
    "n = n-2\n",
    "s2 = test['sentence_1'][n]\n",
    "\n",
    "# s2 = 'ÏàòÎßéÏùÄÎ∞òÎ°ÄÏùÑÏù¥Í≤®ÎÉáÎî∞'\n",
    "\n",
    "print(f'original     : {s2}')\n",
    "s2_r = prep_repeats(s2)\n",
    "print(f'after repeats: {s2_r}')\n",
    "s2_rs = prep_spacing(s2_r)\n",
    "print(f'after spacing: {s2_rs}')\n",
    "s2_rsn = prep_naver(s2_rs)\n",
    "print(f'after naver  : {s2_rsn}')\n",
    "\n",
    "print()\n",
    "\n",
    "s3 = s2\n",
    "print(f'original     : {s3}')\n",
    "s3_r = prep_repeats(s3)\n",
    "print(f'after repeats: {s3_r}')\n",
    "s3_rn = prep_naver(s3_r)\n",
    "print(f'after naver  : {s3_rn}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ïò§Ïò§!! ÏïÑÏïÑ Ï∫¨Ï∫¨!! ÎÑàÎ¨¥ ÏõÉÍ≤®„Öã„Öã!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ÏòàÏãú Î¨∏Ïû•\n",
    "text = \"Ïò§Ïò§Ïò§Ïò§!! ÏïÑÏïÑÏïÑÏïÑ Ï∫¨Ï∫¨Ï∫¨Ï∫¨!! ÎÑàÎ¨¥ ÏõÉÍ≤®„Öã„Öã„Öã„Öã!!\"\n",
    "normalized_text = normalize_text(text)\n",
    "print(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ïã§Ìóò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"klue/roberta-base\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Failed to fetch the page, status code: 403",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataframe\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ÏòàÏãúÎ°ú test Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê ÎåÄÌï¥ Ï†ÅÏö©\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m test_tokenized \u001b[38;5;241m=\u001b[39m tokenizing(tokenizer, \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[132], line 119\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    117\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(prep_repeats)\n\u001b[1;32m    118\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(prep_repeats)\n\u001b[0;32m--> 119\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep_naver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(prep_naver)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/miniconda3/envs/heejun-base/lib/python3.9/site-packages/pandas/core/series.py:4753\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4636\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4754\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heejun-base/lib/python3.9/site-packages/pandas/core/apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heejun-base/lib/python3.9/site-packages/pandas/core/apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/miniconda3/envs/heejun-base/lib/python3.9/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/heejun-base/lib/python3.9/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2920\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[132], line 82\u001b[0m, in \u001b[0;36mprep_naver\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m passport_key \u001b[38;5;241m=\u001b[39m \u001b[43mget_passport_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m text \u001b[38;5;241m=\u001b[39m html\u001b[38;5;241m.\u001b[39munescape(check(text, passport_key))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Cell \u001b[0;32mIn[132], line 37\u001b[0m, in \u001b[0;36mprep_naver.<locals>.get_passport_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassportKey not found in the HTML response.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch the page, status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionError\u001b[0m: Failed to fetch the page, status code: 403"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def tokenizing(tokenizer, dataframe):\n",
    "    # ÏÉà Ïó¥ÏùÑ Ï∂îÍ∞ÄÌï† Ï§ÄÎπÑ\n",
    "    dataframe['s1_tokens'] = None\n",
    "    dataframe['s2_tokens'] = None\n",
    "    \n",
    "    for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "        # sentence_1 ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï\n",
    "        s1_outputs = tokenizer(\n",
    "            item['sentence_1'],\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "        # sentence_2 ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï\n",
    "        s2_outputs = tokenizer(\n",
    "            item['sentence_2'],\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        )\n",
    "        \n",
    "        # ÌÜ†ÌÅ¨ÎÇòÏù¥ÏßïÎêú Í≤∞Í≥ºÎ•º Í∞ÅÍ∞Å ÏÉà Ïó¥Ïóê Ï†ÄÏû•\n",
    "        dataframe.at[idx, 's1_tokens'] = tokenizer.convert_ids_to_tokens(s1_outputs['input_ids'])\n",
    "        dataframe.at[idx, 's2_tokens'] = tokenizer.convert_ids_to_tokens(s2_outputs['input_ids'])\n",
    "    \n",
    "    return dataframe\n",
    "        \n",
    "# ÏòàÏãúÎ°ú test Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê ÎåÄÌï¥ Ï†ÅÏö©\n",
    "test_tokenized = tokenizing(tokenizer, preprocess(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 rows with [UNK] tokens\n"
     ]
    }
   ],
   "source": [
    "# find [UNK] tokens in s1_tokens and s2_tokens\n",
    "# make a new df 'test_unk' that contains rows with [UNK] tokens\n",
    "def find_unk(df):\n",
    "    count = 0\n",
    "    unk_indices = []\n",
    "    for idx, item in df.iterrows():\n",
    "        if '[UNK]' in item['s1_tokens'] or '[UNK]' in item['s2_tokens']:\n",
    "            unk_indices.append(idx)\n",
    "            count += 1\n",
    "    print(f'Found {count} rows with [UNK] tokens')\n",
    "    return df.loc[unk_indices]\n",
    "\n",
    "test_unk = find_unk(test_tokenized)\n",
    "# test_unk.to_csv('a_unk.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print 2nd row of test_unk\n",
    "\n",
    "with open('slang.txt', 'w') as f:\n",
    "    for n in range(len(test_unk)):\n",
    "        if '[UNK]' in test_unk.iloc[n]['s1_tokens']:\n",
    "            f.write(f'{test_unk.iloc[n][\"sentence_1\"]}\\n')\n",
    "            f.write(f'{test_unk.iloc[n][\"s1_tokens\"]}\\n')\n",
    "        if '[UNK]' in test_unk.iloc[n]['s2_tokens']:\n",
    "            f.write(f'{test_unk.iloc[n][\"sentence_2\"]}\\n')\n",
    "            f.write(f'{test_unk.iloc[n][\"s2_tokens\"]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slang = {\n",
    "    'Ïó¨Ï≠ôÎèÑÎ°ù': 'Î¨ºÏñ¥Î≥¥ÎèÑÎ°ù',\n",
    "    'ÎøÖÍ∞ÑÎã§': 'Î©ãÏûàÎã§',\n",
    "    'ÏûàÎÇòÏöØ„Öé„Öé': 'ÏûàÎÇòÏöî„Öé„Öé',\n",
    "    'Í∂ÅÍµºÌï©ÎãàÎã§': 'Í∂ÅÍ∏àÌï©ÎãàÎã§',\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '[PAD]', '[SEP]']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id to token example\n",
    "id = [0, 1, 2]\n",
    "token = tokenizer.convert_ids_to_tokens(id)\n",
    "token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Í∞ÄÏÉÅ',\n",
       " '##Ìôî',\n",
       " '##Ìèê',\n",
       " '##Í±∞ÎûòÏÜå',\n",
       " 'ÌèêÏáÑ',\n",
       " '##Ìïò',\n",
       " '##ÏßÄ',\n",
       " 'Îßê',\n",
       " '##Í≥†',\n",
       " '[SEP]',\n",
       " 'Í∞ÄÏÉÅ',\n",
       " '##Ìôî',\n",
       " '##Ìèê',\n",
       " 'Í±∞ÎûòÏÜå',\n",
       " 'ÌèêÏáÑ',\n",
       " 'Î∞òÎåÄ',\n",
       " '##Ìï©ÎãàÎã§',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id to token\n",
    "tokens = tokenizer.convert_ids_to_tokens(test_tokenized[0][0])\n",
    "tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
