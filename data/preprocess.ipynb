{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open train.csv as pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "valid = pd.read_csv('dev.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9324, 6), (550, 6), (1100, 4))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10974"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[0] + valid.shape[0] + test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10974, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.concat([train, valid, test], ignore_index=True)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                 0\n",
       "source             0\n",
       "sentence_1         0\n",
       "sentence_2         0\n",
       "label           1100\n",
       "binary-label    1100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boostcamp-sts-v1-train-000</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>ìŠ¤ë¦´ë„ìˆê³  ë°˜ì „ë„ ìˆê³  ì—¬ëŠ í•œêµ­ì˜í™” ì“°ë ˆê¸°ë“¤í•˜ê³ ëŠ” ì°¨ì›ì´ ë‹¤ë¥´ë„¤ìš”~</td>\n",
       "      <td>ë°˜ì „ë„ ìˆê³ ,ì‚¬ë‘ë„ ìˆê³ ì¬ë¯¸ë„ìˆë„¤ìš”.</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-train-001</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>ì•— ì œê°€ ì ‘ê·¼ê¶Œí•œì´ ì—†ë‹¤ê³  ëœ¹ë‹ˆë‹¤;;</td>\n",
       "      <td>ì˜¤, ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ ì—†ë‹¤ê³  í•©ë‹ˆë‹¤.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boostcamp-sts-v1-train-002</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>ì£¼íƒì²­ì•½ì¡°ê±´ ë³€ê²½í•´ì£¼ì„¸ìš”.</td>\n",
       "      <td>ì£¼íƒì²­ì•½ ë¬´ì£¼íƒê¸°ì¤€ ë³€ê²½í•´ì£¼ì„¸ìš”.</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boostcamp-sts-v1-train-003</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>ì…ì‚¬í›„ ì²˜ìŒ ëŒ€ë©´ìœ¼ë¡œ ë§Œë‚˜ ë°˜ê°€ì› ìŠµë‹ˆë‹¤.</td>\n",
       "      <td>í™”ìƒìœ¼ë¡œë§Œ ë³´ë‹¤ê°€ ë¦¬ì–¼ë¡œ ë§Œë‚˜ë‹ˆ ì •ë§ ë°˜ê°€ì› ìŠµë‹ˆë‹¤.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boostcamp-sts-v1-train-004</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>ë¿Œë“¯ë¿Œë“¯ í•˜ë„¤ìš”!!</td>\n",
       "      <td>ê¼¬ì˜¥ ì‹¤ì œë¡œ í•œë²ˆ ëµˆì–´ìš” ë¿Œë¿Œë¿Œ~!~!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id            source  \\\n",
       "0  boostcamp-sts-v1-train-000      nsmc-sampled   \n",
       "1  boostcamp-sts-v1-train-001         slack-rtt   \n",
       "2  boostcamp-sts-v1-train-002  petition-sampled   \n",
       "3  boostcamp-sts-v1-train-003     slack-sampled   \n",
       "4  boostcamp-sts-v1-train-004     slack-sampled   \n",
       "\n",
       "                               sentence_1                    sentence_2  \\\n",
       "0  ìŠ¤ë¦´ë„ìˆê³  ë°˜ì „ë„ ìˆê³  ì—¬ëŠ í•œêµ­ì˜í™” ì“°ë ˆê¸°ë“¤í•˜ê³ ëŠ” ì°¨ì›ì´ ë‹¤ë¥´ë„¤ìš”~          ë°˜ì „ë„ ìˆê³ ,ì‚¬ë‘ë„ ìˆê³ ì¬ë¯¸ë„ìˆë„¤ìš”.   \n",
       "1                    ì•— ì œê°€ ì ‘ê·¼ê¶Œí•œì´ ì—†ë‹¤ê³  ëœ¹ë‹ˆë‹¤;;           ì˜¤, ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ ì—†ë‹¤ê³  í•©ë‹ˆë‹¤.   \n",
       "2                          ì£¼íƒì²­ì•½ì¡°ê±´ ë³€ê²½í•´ì£¼ì„¸ìš”.            ì£¼íƒì²­ì•½ ë¬´ì£¼íƒê¸°ì¤€ ë³€ê²½í•´ì£¼ì„¸ìš”.   \n",
       "3                  ì…ì‚¬í›„ ì²˜ìŒ ëŒ€ë©´ìœ¼ë¡œ ë§Œë‚˜ ë°˜ê°€ì› ìŠµë‹ˆë‹¤.  í™”ìƒìœ¼ë¡œë§Œ ë³´ë‹¤ê°€ ë¦¬ì–¼ë¡œ ë§Œë‚˜ë‹ˆ ì •ë§ ë°˜ê°€ì› ìŠµë‹ˆë‹¤.   \n",
       "4                              ë¿Œë“¯ë¿Œë“¯ í•˜ë„¤ìš”!!         ê¼¬ì˜¥ ì‹¤ì œë¡œ í•œë²ˆ ëµˆì–´ìš” ë¿Œë¿Œë¿Œ~!~!   \n",
       "\n",
       "   label  binary-label  \n",
       "0    2.2           0.0  \n",
       "1    4.2           1.0  \n",
       "2    2.4           0.0  \n",
       "3    3.0           1.0  \n",
       "4    0.0           0.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/miniconda3/envs/heejun-base/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/ephemeral/home/miniconda3/envs/heejun-base/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10974/10974 [00:06<00:00, 1744.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('klue/roberta-base', max_length=128)\n",
    "\n",
    "def tokenizing(self, dataframe):\n",
    "    data = []\n",
    "    tokens_data = []  # í† í° ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "        # ë‘ ì…ë ¥ ë¬¸ì¥ì„ [SEP] í† í°ìœ¼ë¡œ ì´ì–´ë¶™ì—¬ì„œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
    "        text = '[SEP]'.join([item[text_column] for text_column in ['sentence_1', 'sentence_2']])\n",
    "        outputs = tokenizer(text, add_special_tokens=True, padding='max_length', truncation=True)\n",
    "        data.append(outputs['input_ids'])\n",
    "        tokens = tokenizer.convert_ids_to_tokens(outputs['input_ids'])  # token idë¥¼ tokensë¡œ ë³€í™˜\n",
    "        tokens_data.append(tokens)  # í† í° ë¦¬ìŠ¤íŠ¸ ì¶”ê°€\n",
    "    return data, tokens_data\n",
    "\n",
    "\n",
    "data, tokens = tokenizing(tokenizer, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new column 'tokens' in the dataframe\n",
    "dataset['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10974, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boostcamp-sts-v1-train-000</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>ìŠ¤ë¦´ë„ìˆê³  ë°˜ì „ë„ ìˆê³  ì—¬ëŠ í•œêµ­ì˜í™” ì“°ë ˆê¸°ë“¤í•˜ê³ ëŠ” ì°¨ì›ì´ ë‹¤ë¥´ë„¤ìš”~</td>\n",
       "      <td>ë°˜ì „ë„ ìˆê³ ,ì‚¬ë‘ë„ ìˆê³ ì¬ë¯¸ë„ìˆë„¤ìš”.</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], ìŠ¤ë¦´, ##ë„, ##ìˆ, ##ê³ , ë°˜ì „, ##ë„, ìˆ, ##ê³ , ì—¬ëŠ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-train-001</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>ì•— ì œê°€ ì ‘ê·¼ê¶Œí•œì´ ì—†ë‹¤ê³  ëœ¹ë‹ˆë‹¤;;</td>\n",
       "      <td>ì˜¤, ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ ì—†ë‹¤ê³  í•©ë‹ˆë‹¤.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], ì•—, ì œ, ##ê°€, ì ‘ê·¼, ##ê¶Œ, ##í•œ, ##ì´, ì—†, ##ë‹¤ê³ ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boostcamp-sts-v1-train-002</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>ì£¼íƒì²­ì•½ì¡°ê±´ ë³€ê²½í•´ì£¼ì„¸ìš”.</td>\n",
       "      <td>ì£¼íƒì²­ì•½ ë¬´ì£¼íƒê¸°ì¤€ ë³€ê²½í•´ì£¼ì„¸ìš”.</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], ì£¼íƒ, ##ì²­, ##ì•½, ##ì¡°ê±´, ë³€ê²½, ##í•´, ##ì£¼, ##ì„¸ìš”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boostcamp-sts-v1-train-003</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>ì…ì‚¬í›„ ì²˜ìŒ ëŒ€ë©´ìœ¼ë¡œ ë§Œë‚˜ ë°˜ê°€ì› ìŠµë‹ˆë‹¤.</td>\n",
       "      <td>í™”ìƒìœ¼ë¡œë§Œ ë³´ë‹¤ê°€ ë¦¬ì–¼ë¡œ ë§Œë‚˜ë‹ˆ ì •ë§ ë°˜ê°€ì› ìŠµë‹ˆë‹¤.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], ì…ì‚¬, ##í›„, ì²˜ìŒ, ëŒ€ë©´, ##ìœ¼ë¡œ, ë§Œë‚˜, ë°˜ê°€ì› , ##ìŠµ, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boostcamp-sts-v1-train-004</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>ë¿Œë“¯ë¿Œë“¯ í•˜ë„¤ìš”!!</td>\n",
       "      <td>ê¼¬ì˜¥ ì‹¤ì œë¡œ í•œë²ˆ ëµˆì–´ìš” ë¿Œë¿Œë¿Œ~!~!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], ë¿Œë“¯, ##ë¿Œ, ##ë“¯, í•˜, ##ë„¤, ##ìš”, !, !, [SEP]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>boostcamp-sts-v1-train-005</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>ì˜¤ë§ˆì´ê°€ëœ¨ì§€ì ¸ìŠ¤í¬ë¡¸ì´ìŠ¤íŠ¸íœ</td>\n",
       "      <td>ì˜¤ ë§ˆì´ ê°“ ì§€ì €ìŠ¤ ìŠ¤í¬ë¡  ì´ìŠ¤íŠ¸ íŒ¬</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], [UNK], [SEP], ì˜¤, ë§ˆì´, ê°“, ì§€ì €, ##ìŠ¤, ìŠ¤í¬, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>boostcamp-sts-v1-train-006</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>ì „ ì•”ë§Œ ì°ì–´ë„ ê¹Œë§Œ í•˜ëŠ˜.. ã… ã… </td>\n",
       "      <td>ì•”ë§Œ ì°ì–´ë„ í•˜ëŠ˜ì€ ê¹Œë§£ë‹¤.. ã… ã… </td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], ì „, ì•”, ##ë§Œ, ì°, ##ì–´ë„, ê¹Œë§Œ, í•˜ëŠ˜, ., ., ã… ã… , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>boostcamp-sts-v1-train-007</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>ì´ë ‡ê²Œ ê·€ì—¬ìš´ ì¥ë“¤ì€ ì²˜ìŒì´ë„¤ìš”.ã…ã…ã…</td>\n",
       "      <td>ì´ë ‡ê²Œ ì§€ê²¨ìš´ ê³µí¬ì˜í™”ëŠ” ì²˜ìŒ..</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], ì´ë ‡ê²Œ, ê·€ì—¬ìš´, ì¥, ##ë“¤, ##ì€, ì²˜ìŒ, ##ì´, ##ë„¤, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boostcamp-sts-v1-train-008</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>ë¯¸ì„¸ë¨¼ì§€ í•´ê²°ì´ ê°€ì¥ ì‹œê¸‰í•œ ë¬¸ì œì…ë‹ˆë‹¤!</td>\n",
       "      <td>ê°€ì¥ ì‹œê¸‰í•œ ê²ƒì´ ì‹ ìƒì•„ì‹¤ ê´€ë¦¬ì…ë‹ˆë‹¤!!!</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], ë¯¸ì„¸ë¨¼ì§€, í•´ê²°, ##ì´, ê°€ì¥, ì‹œê¸‰, ##í•œ, ë¬¸ì œ, ##ì…ë‹ˆë‹¤,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>boostcamp-sts-v1-train-009</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>í¬ë¦¼í•˜ìš°ìŠ¤ í™˜ë¶ˆì¡°ì¹˜í•´ì£¼ì„¸ìš”.</td>\n",
       "      <td>í¬ë¦¼í•˜ìš°ìŠ¤ í™˜ë¶ˆì¡°ì¹˜í•  ìˆ˜ ìˆë„ë¡í•´ì£¼ì„¸ì—¬</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], í¬ë¦¼, ##í•˜ìš°ìŠ¤, í™˜ë¶ˆ, ##ì¡°ì¹˜, ##í•´, ##ì£¼, ##ì„¸ìš”, ....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id            source  \\\n",
       "0  boostcamp-sts-v1-train-000      nsmc-sampled   \n",
       "1  boostcamp-sts-v1-train-001         slack-rtt   \n",
       "2  boostcamp-sts-v1-train-002  petition-sampled   \n",
       "3  boostcamp-sts-v1-train-003     slack-sampled   \n",
       "4  boostcamp-sts-v1-train-004     slack-sampled   \n",
       "5  boostcamp-sts-v1-train-005          nsmc-rtt   \n",
       "6  boostcamp-sts-v1-train-006         slack-rtt   \n",
       "7  boostcamp-sts-v1-train-007      nsmc-sampled   \n",
       "8  boostcamp-sts-v1-train-008  petition-sampled   \n",
       "9  boostcamp-sts-v1-train-009  petition-sampled   \n",
       "\n",
       "                               sentence_1                    sentence_2  \\\n",
       "0  ìŠ¤ë¦´ë„ìˆê³  ë°˜ì „ë„ ìˆê³  ì—¬ëŠ í•œêµ­ì˜í™” ì“°ë ˆê¸°ë“¤í•˜ê³ ëŠ” ì°¨ì›ì´ ë‹¤ë¥´ë„¤ìš”~          ë°˜ì „ë„ ìˆê³ ,ì‚¬ë‘ë„ ìˆê³ ì¬ë¯¸ë„ìˆë„¤ìš”.   \n",
       "1                    ì•— ì œê°€ ì ‘ê·¼ê¶Œí•œì´ ì—†ë‹¤ê³  ëœ¹ë‹ˆë‹¤;;           ì˜¤, ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ ì—†ë‹¤ê³  í•©ë‹ˆë‹¤.   \n",
       "2                          ì£¼íƒì²­ì•½ì¡°ê±´ ë³€ê²½í•´ì£¼ì„¸ìš”.            ì£¼íƒì²­ì•½ ë¬´ì£¼íƒê¸°ì¤€ ë³€ê²½í•´ì£¼ì„¸ìš”.   \n",
       "3                  ì…ì‚¬í›„ ì²˜ìŒ ëŒ€ë©´ìœ¼ë¡œ ë§Œë‚˜ ë°˜ê°€ì› ìŠµë‹ˆë‹¤.  í™”ìƒìœ¼ë¡œë§Œ ë³´ë‹¤ê°€ ë¦¬ì–¼ë¡œ ë§Œë‚˜ë‹ˆ ì •ë§ ë°˜ê°€ì› ìŠµë‹ˆë‹¤.   \n",
       "4                              ë¿Œë“¯ë¿Œë“¯ í•˜ë„¤ìš”!!         ê¼¬ì˜¥ ì‹¤ì œë¡œ í•œë²ˆ ëµˆì–´ìš” ë¿Œë¿Œë¿Œ~!~!   \n",
       "5                          ì˜¤ë§ˆì´ê°€ëœ¨ì§€ì ¸ìŠ¤í¬ë¡¸ì´ìŠ¤íŠ¸íœ          ì˜¤ ë§ˆì´ ê°“ ì§€ì €ìŠ¤ ìŠ¤í¬ë¡  ì´ìŠ¤íŠ¸ íŒ¬   \n",
       "6                     ì „ ì•”ë§Œ ì°ì–´ë„ ê¹Œë§Œ í•˜ëŠ˜.. ã… ã…            ì•”ë§Œ ì°ì–´ë„ í•˜ëŠ˜ì€ ê¹Œë§£ë‹¤.. ã… ã…    \n",
       "7                   ì´ë ‡ê²Œ ê·€ì—¬ìš´ ì¥ë“¤ì€ ì²˜ìŒì´ë„¤ìš”.ã…ã…ã…            ì´ë ‡ê²Œ ì§€ê²¨ìš´ ê³µí¬ì˜í™”ëŠ” ì²˜ìŒ..   \n",
       "8                  ë¯¸ì„¸ë¨¼ì§€ í•´ê²°ì´ ê°€ì¥ ì‹œê¸‰í•œ ë¬¸ì œì…ë‹ˆë‹¤!       ê°€ì¥ ì‹œê¸‰í•œ ê²ƒì´ ì‹ ìƒì•„ì‹¤ ê´€ë¦¬ì…ë‹ˆë‹¤!!!   \n",
       "9                         í¬ë¦¼í•˜ìš°ìŠ¤ í™˜ë¶ˆì¡°ì¹˜í•´ì£¼ì„¸ìš”.         í¬ë¦¼í•˜ìš°ìŠ¤ í™˜ë¶ˆì¡°ì¹˜í•  ìˆ˜ ìˆë„ë¡í•´ì£¼ì„¸ì—¬   \n",
       "\n",
       "   label  binary-label                                             tokens  \n",
       "0    2.2           0.0  [[CLS], ìŠ¤ë¦´, ##ë„, ##ìˆ, ##ê³ , ë°˜ì „, ##ë„, ìˆ, ##ê³ , ì—¬ëŠ...  \n",
       "1    4.2           1.0  [[CLS], ì•—, ì œ, ##ê°€, ì ‘ê·¼, ##ê¶Œ, ##í•œ, ##ì´, ì—†, ##ë‹¤ê³ ,...  \n",
       "2    2.4           0.0  [[CLS], ì£¼íƒ, ##ì²­, ##ì•½, ##ì¡°ê±´, ë³€ê²½, ##í•´, ##ì£¼, ##ì„¸ìš”...  \n",
       "3    3.0           1.0  [[CLS], ì…ì‚¬, ##í›„, ì²˜ìŒ, ëŒ€ë©´, ##ìœ¼ë¡œ, ë§Œë‚˜, ë°˜ê°€ì› , ##ìŠµ, #...  \n",
       "4    0.0           0.0  [[CLS], ë¿Œë“¯, ##ë¿Œ, ##ë“¯, í•˜, ##ë„¤, ##ìš”, !, !, [SEP]...  \n",
       "5    2.6           1.0  [[CLS], [UNK], [SEP], ì˜¤, ë§ˆì´, ê°“, ì§€ì €, ##ìŠ¤, ìŠ¤í¬, #...  \n",
       "6    3.6           1.0  [[CLS], ì „, ì•”, ##ë§Œ, ì°, ##ì–´ë„, ê¹Œë§Œ, í•˜ëŠ˜, ., ., ã… ã… , ...  \n",
       "7    0.6           0.0  [[CLS], ì´ë ‡ê²Œ, ê·€ì—¬ìš´, ì¥, ##ë“¤, ##ì€, ì²˜ìŒ, ##ì´, ##ë„¤, #...  \n",
       "8    0.4           0.0  [[CLS], ë¯¸ì„¸ë¨¼ì§€, í•´ê²°, ##ì´, ê°€ì¥, ì‹œê¸‰, ##í•œ, ë¬¸ì œ, ##ì…ë‹ˆë‹¤,...  \n",
       "9    4.2           1.0  [[CLS], í¬ë¦¼, ##í•˜ìš°ìŠ¤, í™˜ë¶ˆ, ##ì¡°ì¹˜, ##í•´, ##ì£¼, ##ì„¸ìš”, ....  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-train-001</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>ì•— ì œê°€ ì ‘ê·¼ê¶Œí•œì´ ì—†ë‹¤ê³  ëœ¹ë‹ˆë‹¤;;</td>\n",
       "      <td>ì˜¤, ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ ì—†ë‹¤ê³  í•©ë‹ˆë‹¤.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], ì•—, ì œ, ##ê°€, ì ‘ê·¼, ##ê¶Œ, ##í•œ, ##ì´, ì—†, ##ë‹¤ê³ ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>boostcamp-sts-v1-train-005</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>ì˜¤ë§ˆì´ê°€ëœ¨ì§€ì ¸ìŠ¤í¬ë¡¸ì´ìŠ¤íŠ¸íœ</td>\n",
       "      <td>ì˜¤ ë§ˆì´ ê°“ ì§€ì €ìŠ¤ ìŠ¤í¬ë¡  ì´ìŠ¤íŠ¸ íŒ¬</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], [UNK], [SEP], ì˜¤, ë§ˆì´, ê°“, ì§€ì €, ##ìŠ¤, ìŠ¤í¬, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>boostcamp-sts-v1-train-158</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>ì²˜ìŒ ëµŒ ë¶„ë“¤ê³¼ ë¹ ë¥´ê²Œ ì¹œí•´ì§ˆ ìˆ˜ ìˆì„ ê²ƒ ê°™ì€ ëŠë‚Œ!</td>\n",
       "      <td>ì²˜ìŒ ë§Œë‚˜ëŠ” ì‚¬ëŒë“¤ê³¼ ê¸ˆë°© ì¹œí•´ì§ˆ ìˆ˜ ìˆëŠ” ê²ƒ ê°™ì•„ìš”!</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[CLS], ì²˜ìŒ, [UNK], ë¶„, ##ë“¤, ##ê³¼, ë¹ ë¥´, ##ê²Œ, ì¹œí•´, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>boostcamp-sts-v1-train-187</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>ì™€ì•„ì•„ì•„ì•ˆì „ ì¢‹ì•„ìš”ì˜¤ì˜¤</td>\n",
       "      <td>êº„ì˜¤ì˜¤ì˜¬!!!!! í™˜ì˜í•©ë‹ˆë‹¤ì•„ì•„ì•„</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], ì™€, ##ì•„ì•„, ##ì•„, ##ì•ˆì „, ì¢‹ì•„, ##ìš”, ##ì˜¤, ##ì˜¤,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>boostcamp-sts-v1-train-227</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>ìˆ˜ì§€ ëª©ì†Œë¦¬ ë„ˆë¬´ì¢‹ì•„ã…œã…œã…œ</td>\n",
       "      <td>ì†Œì†Œí•œì¬ë¯¸ã…œã…‹ì—¬íƒ¯ê»ì‚¶ì„ë‹¤ì‹œëŒì•„ë³´ê²Œí•˜ëŠ”ì˜í™”ã…œ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[CLS], ìˆ˜ì§€, ëª©ì†Œë¦¬, ë„ˆë¬´, ##ì¢‹, ##ì•„, ##ã…œã…œ, ##ã…œ, [SEP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10907</th>\n",
       "      <td>boostcamp-sts-v1-test-1033</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>ì‚¼ê°€ê³ ì¸ì˜ ëª…ë³µì„ ë¹•ë‹ˆë‹¤</td>\n",
       "      <td>ì‚¼ê°€ ê³ ì¸ì˜ ëª…ë³µì„ ë¹•ë‹ˆë‹¤</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[CLS], ì‚¼ê°€, ##ê³ , ##ì¸, ##ì˜, ëª…ë³µ, ##ì„, [UNK], [SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10911</th>\n",
       "      <td>boostcamp-sts-v1-test-1037</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>ë‹´ì—ëŠ” ì˜¤í”„ì—ì„œ ì–¼êµ´ ëµ ìˆ˜ ìˆê¸°ë¥¼</td>\n",
       "      <td>ì œ ë¨¸ë¦¬ê°€ ì–´ê¹¨ì— ë‹¿ê¸° ì „ì—ëŠ” í•œë²ˆ ëµ ìˆ˜ ìˆê¸¸</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[CLS], ë‹´, ##ì—, ##ëŠ”, ì˜¤í”„, ##ì—ì„œ, ì–¼êµ´, [UNK], ìˆ˜, ìˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10934</th>\n",
       "      <td>boostcamp-sts-v1-test-1060</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>ì •ë§ ì¬ë°Œê²Œë´¤ìŠµë‹ˆë‹¤ ^^</td>\n",
       "      <td>ì •ë§ ì¬ë°Œê²Œë´£ìŠµë‹ˆë‹¤^_^</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[CLS], ì •ë§, ì¬ë°Œ, ##ê²Œ, ##ë´¤, ##ìŠµ, ##ë‹ˆë‹¤, ^, ^, [SE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>boostcamp-sts-v1-test-1063</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>í•­ìƒ ìš”ë§˜ë•Œì¯¤ ë¹„ê°€ì™€ì„œ ì•„ì‰¬ì› ëŠ”ë° ì´ë²ˆ ë´„ì€ ë²šê½ƒ ê°œë‚˜ë¦¬ ì§„ë‹¬ë˜ê°€ ëª¨ë‘ í•œìë¦¬ì— ëª¨...</td>\n",
       "      <td>ì´ë§˜ë•Œì¯¤ ë¹„ê°€ ì™€ì„œ ì•„ì‰¬ì› ëŠ”ë°, ì´ë²ˆ ë´„, ë²šê½ƒ, ê°œë‚˜ë¦¬, ë§Œê°œí•œ ë²šê½ƒì´ ë§Œë°œí•˜ëŠ” ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[CLS], í•­ìƒ, ìš”, ##ë§˜, ##ë•Œ, ##ì¯¤, ë¹„, ##ê°€ì™€, ##ì„œ, ì•„ì‰¬...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10972</th>\n",
       "      <td>boostcamp-sts-v1-test-1098</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>ìš”ì¦˜ ì¬ë¯¸ê°€ í›… ë–¨ì–´ì§...</td>\n",
       "      <td>ìš”ì¦˜ ì¬ë¯¸ê°€ ì‚¬ë¼ì¡Œë‹¤...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[CLS], ìš”ì¦˜, ì¬ë¯¸, ##ê°€, [UNK], ë–¨ì–´, ##ì§, ., ., ., ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>470 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id         source  \\\n",
       "1      boostcamp-sts-v1-train-001      slack-rtt   \n",
       "5      boostcamp-sts-v1-train-005       nsmc-rtt   \n",
       "158    boostcamp-sts-v1-train-158      slack-rtt   \n",
       "187    boostcamp-sts-v1-train-187  slack-sampled   \n",
       "227    boostcamp-sts-v1-train-227   nsmc-sampled   \n",
       "...                           ...            ...   \n",
       "10907  boostcamp-sts-v1-test-1033   nsmc-sampled   \n",
       "10911  boostcamp-sts-v1-test-1037  slack-sampled   \n",
       "10934  boostcamp-sts-v1-test-1060   nsmc-sampled   \n",
       "10937  boostcamp-sts-v1-test-1063      slack-rtt   \n",
       "10972  boostcamp-sts-v1-test-1098       nsmc-rtt   \n",
       "\n",
       "                                              sentence_1  \\\n",
       "1                                   ì•— ì œê°€ ì ‘ê·¼ê¶Œí•œì´ ì—†ë‹¤ê³  ëœ¹ë‹ˆë‹¤;;   \n",
       "5                                         ì˜¤ë§ˆì´ê°€ëœ¨ì§€ì ¸ìŠ¤í¬ë¡¸ì´ìŠ¤íŠ¸íœ   \n",
       "158                       ì²˜ìŒ ëµŒ ë¶„ë“¤ê³¼ ë¹ ë¥´ê²Œ ì¹œí•´ì§ˆ ìˆ˜ ìˆì„ ê²ƒ ê°™ì€ ëŠë‚Œ!   \n",
       "187                                         ì™€ì•„ì•„ì•„ì•ˆì „ ì¢‹ì•„ìš”ì˜¤ì˜¤   \n",
       "227                                       ìˆ˜ì§€ ëª©ì†Œë¦¬ ë„ˆë¬´ì¢‹ì•„ã…œã…œã…œ   \n",
       "...                                                  ...   \n",
       "10907                                      ì‚¼ê°€ê³ ì¸ì˜ ëª…ë³µì„ ë¹•ë‹ˆë‹¤   \n",
       "10911                                ë‹´ì—ëŠ” ì˜¤í”„ì—ì„œ ì–¼êµ´ ëµ ìˆ˜ ìˆê¸°ë¥¼   \n",
       "10934                                      ì •ë§ ì¬ë°Œê²Œë´¤ìŠµë‹ˆë‹¤ ^^   \n",
       "10937  í•­ìƒ ìš”ë§˜ë•Œì¯¤ ë¹„ê°€ì™€ì„œ ì•„ì‰¬ì› ëŠ”ë° ì´ë²ˆ ë´„ì€ ë²šê½ƒ ê°œë‚˜ë¦¬ ì§„ë‹¬ë˜ê°€ ëª¨ë‘ í•œìë¦¬ì— ëª¨...   \n",
       "10972                                    ìš”ì¦˜ ì¬ë¯¸ê°€ í›… ë–¨ì–´ì§...   \n",
       "\n",
       "                                              sentence_2  label  binary-label  \\\n",
       "1                                    ì˜¤, ì•¡ì„¸ìŠ¤ ê¶Œí•œì´ ì—†ë‹¤ê³  í•©ë‹ˆë‹¤.    4.2           1.0   \n",
       "5                                   ì˜¤ ë§ˆì´ ê°“ ì§€ì €ìŠ¤ ìŠ¤í¬ë¡  ì´ìŠ¤íŠ¸ íŒ¬    2.6           1.0   \n",
       "158                       ì²˜ìŒ ë§Œë‚˜ëŠ” ì‚¬ëŒë“¤ê³¼ ê¸ˆë°© ì¹œí•´ì§ˆ ìˆ˜ ìˆëŠ” ê²ƒ ê°™ì•„ìš”!    3.4           1.0   \n",
       "187                                   êº„ì˜¤ì˜¤ì˜¬!!!!! í™˜ì˜í•©ë‹ˆë‹¤ì•„ì•„ì•„    0.0           0.0   \n",
       "227                              ì†Œì†Œí•œì¬ë¯¸ã…œã…‹ì—¬íƒ¯ê»ì‚¶ì„ë‹¤ì‹œëŒì•„ë³´ê²Œí•˜ëŠ”ì˜í™”ã…œ    0.0           0.0   \n",
       "...                                                  ...    ...           ...   \n",
       "10907                                     ì‚¼ê°€ ê³ ì¸ì˜ ëª…ë³µì„ ë¹•ë‹ˆë‹¤    NaN           NaN   \n",
       "10911                         ì œ ë¨¸ë¦¬ê°€ ì–´ê¹¨ì— ë‹¿ê¸° ì „ì—ëŠ” í•œë²ˆ ëµ ìˆ˜ ìˆê¸¸    NaN           NaN   \n",
       "10934                                      ì •ë§ ì¬ë°Œê²Œë´£ìŠµë‹ˆë‹¤^_^    NaN           NaN   \n",
       "10937  ì´ë§˜ë•Œì¯¤ ë¹„ê°€ ì™€ì„œ ì•„ì‰¬ì› ëŠ”ë°, ì´ë²ˆ ë´„, ë²šê½ƒ, ê°œë‚˜ë¦¬, ë§Œê°œí•œ ë²šê½ƒì´ ë§Œë°œí•˜ëŠ” ...    NaN           NaN   \n",
       "10972                                     ìš”ì¦˜ ì¬ë¯¸ê°€ ì‚¬ë¼ì¡Œë‹¤...    NaN           NaN   \n",
       "\n",
       "                                                  tokens  \n",
       "1      [[CLS], ì•—, ì œ, ##ê°€, ì ‘ê·¼, ##ê¶Œ, ##í•œ, ##ì´, ì—†, ##ë‹¤ê³ ,...  \n",
       "5      [[CLS], [UNK], [SEP], ì˜¤, ë§ˆì´, ê°“, ì§€ì €, ##ìŠ¤, ìŠ¤í¬, #...  \n",
       "158    [[CLS], ì²˜ìŒ, [UNK], ë¶„, ##ë“¤, ##ê³¼, ë¹ ë¥´, ##ê²Œ, ì¹œí•´, #...  \n",
       "187    [[CLS], ì™€, ##ì•„ì•„, ##ì•„, ##ì•ˆì „, ì¢‹ì•„, ##ìš”, ##ì˜¤, ##ì˜¤,...  \n",
       "227    [[CLS], ìˆ˜ì§€, ëª©ì†Œë¦¬, ë„ˆë¬´, ##ì¢‹, ##ì•„, ##ã…œã…œ, ##ã…œ, [SEP...  \n",
       "...                                                  ...  \n",
       "10907  [[CLS], ì‚¼ê°€, ##ê³ , ##ì¸, ##ì˜, ëª…ë³µ, ##ì„, [UNK], [SE...  \n",
       "10911  [[CLS], ë‹´, ##ì—, ##ëŠ”, ì˜¤í”„, ##ì—ì„œ, ì–¼êµ´, [UNK], ìˆ˜, ìˆ...  \n",
       "10934  [[CLS], ì •ë§, ì¬ë°Œ, ##ê²Œ, ##ë´¤, ##ìŠµ, ##ë‹ˆë‹¤, ^, ^, [SE...  \n",
       "10937  [[CLS], í•­ìƒ, ìš”, ##ë§˜, ##ë•Œ, ##ì¯¤, ë¹„, ##ê°€ì™€, ##ì„œ, ì•„ì‰¬...  \n",
       "10972  [[CLS], ìš”ì¦˜, ì¬ë¯¸, ##ê°€, [UNK], ë–¨ì–´, ##ì§, ., ., ., ...  \n",
       "\n",
       "[470 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find '[UNK]' token in dataset['tokens'] and make a new df\n",
    "unk_list = []\n",
    "for i in range(len(dataset['tokens'])):\n",
    "    if '[UNK]' in dataset['tokens'][i]:\n",
    "        unk_list.append(i)\n",
    "        \n",
    "unk_df = dataset.loc[unk_list]\n",
    "unk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a txt file with unk_df 'tokens' data\n",
    "with open('unk.txt', 'w') as f:\n",
    "    for i in range(len(unk_df)):\n",
    "        f.write(str(unk_df['sentence_1'].values[i]) + ' @ ' + str(unk_df['sentence_2'].values[i]) + '\\n' + str(unk_df['tokens'].values[i]) + '\\n' + '****************************************************************************************************' + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#ì˜¤ìš´ì™„ #ì–´ì©Œêµ¬ #ì €ì©Œêµ¬ &lt;&lt; ì´ìë¦¬ì— í•´ì‹œíƒœê·¸ê°€ ìˆì—ˆìŒ</td>\n",
       "      <td>()()() () &lt;&lt; ì´ìë¦¬ì— ë¹ˆê´„í˜¸ ìˆì—ˆìŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë„ì–´ì“°ê¸°ë¥¼ì•ˆí•´ë²„ë¦¬ê¸°</td>\n",
       "      <td>ë„ì–´ì“°ê¸°ë¥¼      ë‘ë²ˆì´ìƒ í•´ë„    í•œë²ˆë§Œ          ë„ì–´ì“°ê¸°ê°€ ëœë‹¤ê³ ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë§ì¶¤ë²• í‹€ë¦¬ë©´ ì™¸ ì•Šë˜? ì“°ê³ ì‹¶ì€ëŒ€ë¡œì“°ë©´ë¼ì§€</td>\n",
       "      <td>ë§Ÿì¶¥ë»¡ í‹€ë ¤ë²„ë¦¬ê¸°~</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sentence_1  \\\n",
       "0  #ì˜¤ìš´ì™„ #ì–´ì©Œêµ¬ #ì €ì©Œêµ¬ << ì´ìë¦¬ì— í•´ì‹œíƒœê·¸ê°€ ìˆì—ˆìŒ   \n",
       "1                        ë„ì–´ì“°ê¸°ë¥¼ì•ˆí•´ë²„ë¦¬ê¸°   \n",
       "2          ë§ì¶¤ë²• í‹€ë¦¬ë©´ ì™¸ ì•Šë˜? ì“°ê³ ì‹¶ì€ëŒ€ë¡œì“°ë©´ë¼ì§€   \n",
       "\n",
       "                                       sentence_2  \n",
       "0                       ()()() () << ì´ìë¦¬ì— ë¹ˆê´„í˜¸ ìˆì—ˆìŒ  \n",
       "1  ë„ì–´ì“°ê¸°ë¥¼      ë‘ë²ˆì´ìƒ í•´ë„    í•œë²ˆë§Œ          ë„ì–´ì“°ê¸°ê°€ ëœë‹¤ê³ ?   \n",
       "2                                      ë§Ÿì¶¥ë»¡ í‹€ë ¤ë²„ë¦¬ê¸°~  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame({\n",
    "    'sentence_1': ['#ì˜¤ìš´ì™„ #ì–´ì©Œêµ¬ #ì €ì©Œêµ¬ << ì´ìë¦¬ì— í•´ì‹œíƒœê·¸ê°€ ìˆì—ˆìŒ', 'ë„ì–´ì“°ê¸°ë¥¼ì•ˆí•´ë²„ë¦¬ê¸°', 'ë§ì¶¤ë²• í‹€ë¦¬ë©´ ì™¸ ì•Šë˜? ì“°ê³ ì‹¶ì€ëŒ€ë¡œì“°ë©´ë¼ì§€'],\n",
    "    'sentence_2': ['()()() () << ì´ìë¦¬ì— ë¹ˆê´„í˜¸ ìˆì—ˆìŒ', 'ë„ì–´ì“°ê¸°ë¥¼      ë‘ë²ˆì´ìƒ í•´ë„    í•œë²ˆë§Œ          ë„ì–´ì“°ê¸°ê°€ ëœë‹¤ê³ ? ', 'ë§Ÿì¶¥ë»¡ í‹€ë ¤ë²„ë¦¬ê¸°~']\n",
    "})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def delete_etcs(dataframe):\n",
    "#     for i in range(len(dataframe['sentence_1'])):\n",
    "#         dataframe['sentence_1'][i] = re.sub(r'#', '', dataframe['sentence_1'][i])\n",
    "#         dataframe['sentence_2'][i] = re.sub(r'#', '', dataframe['sentence_2'][i])\n",
    "#         dataframe['sentence_1'][i] = re.sub(r'\\(\\)', '', dataframe['sentence_1'][i])\n",
    "#         dataframe['sentence_2'][i] = re.sub(r'\\(\\)', '', dataframe['sentence_2'][i])\n",
    "#     return dataframe\n",
    "\n",
    "# delete_etcs(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  6.41it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#ì˜¤ìš´ ì™„ #ì–´ì©Œêµ¬ #ì €ì©Œêµ¬ &lt;&lt; ì´ ìë¦¬ì— í•´ì‹œ íƒœê·¸ê°€ ìˆì—ˆìŒ</td>\n",
       "      <td>()()() () &lt;&lt; ì´ ìë¦¬ì— ë¹ˆê´„í˜¸ ìˆì—ˆìŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë„ì–´ì“°ê¸°ë¥¼ ì•ˆ í•´ë²„ë¦¬ê¸°</td>\n",
       "      <td>ë„ì–´ì“°ê¸°ë¥¼ ë‘ ë²ˆ ì´ìƒ í•´ë„ í•œ ë²ˆë§Œ ë„ì–´ì“°ê¸°ê°€ ëœë‹¤ê³ ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë§ì¶¤ë²• í‹€ë¦¬ë©´ ì™¸ ì•Šë˜? ì“°ê³  ì‹¶ì€ ëŒ€ë¡œ ì“°ë©´ ë¼ì§€</td>\n",
       "      <td>ë§Ÿì¶¥ë»¡ í‹€ë ¤ë²„ë¦¬ê¸°~</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            sentence_1                       sentence_2\n",
       "0  #ì˜¤ìš´ ì™„ #ì–´ì©Œêµ¬ #ì €ì©Œêµ¬ << ì´ ìë¦¬ì— í•´ì‹œ íƒœê·¸ê°€ ìˆì—ˆìŒ       ()()() () << ì´ ìë¦¬ì— ë¹ˆê´„í˜¸ ìˆì—ˆìŒ\n",
       "1                         ë„ì–´ì“°ê¸°ë¥¼ ì•ˆ í•´ë²„ë¦¬ê¸°  ë„ì–´ì“°ê¸°ë¥¼ ë‘ ë²ˆ ì´ìƒ í•´ë„ í•œ ë²ˆë§Œ ë„ì–´ì“°ê¸°ê°€ ëœë‹¤ê³ ?\n",
       "2         ë§ì¶¤ë²• í‹€ë¦¬ë©´ ì™¸ ì•Šë˜? ì“°ê³  ì‹¶ì€ ëŒ€ë¡œ ì“°ë©´ ë¼ì§€                       ë§Ÿì¶¥ë»¡ í‹€ë ¤ë²„ë¦¬ê¸°~"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for each values in train['sentence_1'] or train['sentence_2], apply spacing\n",
    "from pykospacing import Spacing\n",
    "\n",
    "def apply_spacing(dataframe):\n",
    "    spacing = Spacing()\n",
    "    for i in tqdm(range(len(dataframe['sentence_1'])), desc='apply_spacing'):\n",
    "        dataframe.loc[i, 'sentence_1'] = spacing(dataframe.loc[i, 'sentence_1'])\n",
    "        dataframe.loc[i, 'sentence_2'] = spacing(dataframe.loc[i, 'sentence_2'])\n",
    "    return dataframe\n",
    "\n",
    "apply_spacing(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:05<00:00,  1.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#ì˜¤ìš´ ì™„ #ì–´ì©Œê³  #ì €ì©Œê³  &lt;&lt; ì´ ìë¦¬ì— í•´ì‹œ íƒœê·¸ê°€ ìˆì—ˆìŒ</td>\n",
       "      <td>()()() () &lt;&lt; ì´ ìë¦¬ì— ë¹ˆ ê´„í˜¸ ìˆì—ˆìŒ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë„ì–´ì“°ê¸°ë¥¼ ì•ˆ í•´ë²„ë¦¬ê¸°</td>\n",
       "      <td>ë„ì–´ì“°ê¸°ë¥¼ ë‘ ë²ˆ ì´ìƒ í•´ë„ í•œ ë²ˆë§Œ ë„ì–´ì“°ê¸°ê°€ ëœë‹¤ê³ ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë§ì¶¤ë²• í‹€ë¦¬ë©´ ì™œ ì•ˆë¼? ì“°ê³  ì‹¶ì€ ëŒ€ë¡œ ì“°ë©´ ë˜ì§€</td>\n",
       "      <td>ë§Ÿì¶¥ë»¡ í‹€ë ¤ë²„ë¦¬ê¸°~</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            sentence_1                       sentence_2\n",
       "0  #ì˜¤ìš´ ì™„ #ì–´ì©Œê³  #ì €ì©Œê³  << ì´ ìë¦¬ì— í•´ì‹œ íƒœê·¸ê°€ ìˆì—ˆìŒ      ()()() () << ì´ ìë¦¬ì— ë¹ˆ ê´„í˜¸ ìˆì—ˆìŒ\n",
       "1                         ë„ì–´ì“°ê¸°ë¥¼ ì•ˆ í•´ë²„ë¦¬ê¸°  ë„ì–´ì“°ê¸°ë¥¼ ë‘ ë²ˆ ì´ìƒ í•´ë„ í•œ ë²ˆë§Œ ë„ì–´ì“°ê¸°ê°€ ëœë‹¤ê³ ?\n",
       "2         ë§ì¶¤ë²• í‹€ë¦¬ë©´ ì™œ ì•ˆë¼? ì“°ê³  ì‹¶ì€ ëŒ€ë¡œ ì“°ë©´ ë˜ì§€                       ë§Ÿì¶¥ë»¡ í‹€ë ¤ë²„ë¦¬ê¸°~"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ë„¤ì´ë²„ ë§ì¶¤ë²• api ì‚¬ìš©\n",
    "- ì´ëª¨í‹°ì½˜ ì‚¬ë¼ì§: 'ğŸ‘ŒğŸ‘ŒğŸ‘Œ' -> ''\n",
    "- ì‹ ì¡°ì–´ ë“±ì€ ì•ˆë°”ë€œ: 'ê°¬ì„±' -> 'ê°¬ì„±', 'íˆì´ì•¼' -> 'íˆì´ì•¼' ë“±\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "\n",
    "def check_spell(dataframe):\n",
    "    def get_passport_key():\n",
    "        url = \"https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=0&ie=utf8&query=%EB%A7%9E%EC%B6%A4%EB%B2%95%EA%B2%80%EC%82%AC%EA%B8%B0\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            match = re.search(r'passportKey=([a-zA-Z0-9-_]+)', html)\n",
    "            if match:\n",
    "                passport_key = match.group(1)\n",
    "                print(f\"passportKey found: {passport_key}\")\n",
    "                return passport_key\n",
    "            else:\n",
    "                raise ValueError(\"passportKey not found in the HTML response.\")\n",
    "        else:\n",
    "            raise ConnectionError(f\"Failed to fetch the page, status code: {response.status_code}\")\n",
    "\n",
    "    # ë§ì¶¤ë²• ê²€ì‚¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜\n",
    "    def _spell_check_request(text, passport_key):\n",
    "        payload = {\n",
    "            'passportKey': passport_key,\n",
    "            '_callback': passport_key,\n",
    "            'q': text,\n",
    "            'color_blindness': '0'\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',\n",
    "            'referer': 'https://search.naver.com/',\n",
    "        }\n",
    "\n",
    "        start_time = time.time()\n",
    "        r = requests.get(\"https://m.search.naver.com/p/csearch/ocontent/util/SpellerProxy\", params=payload, headers=headers)\n",
    "        passed_time = time.time() - start_time\n",
    "\n",
    "        json_match = re.search(r'\\{.*\\}', r.text)\n",
    "        if json_match:\n",
    "            json_data = json_match.group(0)\n",
    "            data = json.loads(json_data)\n",
    "            html = data['message']['result']['html']\n",
    "            return _remove_tags(html)\n",
    "        else:\n",
    "            raise ValueError(\"No JSON data found in the response.\")\n",
    "\n",
    "    def _remove_tags(text):\n",
    "        text = '<content>{}</content>'.format(text).replace('<br>','')\n",
    "        result = ''.join(re.sub(r'<[^>]+>', '', text))\n",
    "        return result\n",
    "\n",
    "    def check(text, passport_key):\n",
    "        try:\n",
    "            return _spell_check_request(text, passport_key)\n",
    "        except ValueError as e:\n",
    "            if 'No JSON data found in the response' in str(e):\n",
    "                print(\"passport_key expired, fetching a new one.\")\n",
    "                passport_key = get_passport_key()  # ìƒˆë¡œìš´ passport_key ê°€ì ¸ì˜¤ê¸°\n",
    "                return _spell_check_request(text, passport_key)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    passport_key = get_passport_key()\n",
    "\n",
    "    for i in tqdm(range(len(dataframe['sentence_1'])), desc='check_spell'):\n",
    "        dataframe.loc[i, 'sentence_1'] = html.unescape(check(dataframe.loc[i, 'sentence_1'], passport_key))\n",
    "        dataframe.loc[i, 'sentence_2'] = html.unescape(check(dataframe.loc[i, 'sentence_2'], passport_key))        \n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:00<00:00, 18.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>ë§ì•œã…‹ã…‹ã…‹ã…‹ë­”ë°ìš¬ã…‹ã…‹ã…‹</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ê°¬ì„± ìˆë„¤</td>\n",
       "      <td>ì§„ã…‰ ã…? ì •ë§ë¡œ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ë„µ ì•Œê² ìŠµë‹ˆë‹¤ ã…‹</td>\n",
       "      <td>ë¬¸ì¬ì¸ ì •ë¶€</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>íˆì´ì•¼</td>\n",
       "      <td>ë´¬ìš”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì €ë²ˆì— ëµŒ ê±° ê°™ì€ë°</td>\n",
       "      <td>ì­ˆë¼›ì­ˆë¼›</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence_1    sentence_2\n",
       "0               ë§ì•œã…‹ã…‹ã…‹ã…‹ë­”ë°ìš¬ã…‹ã…‹ã…‹\n",
       "1        ê°¬ì„± ìˆë„¤    ì§„ã…‰ ã…? ì •ë§ë¡œ?\n",
       "2    ë„µ ì•Œê² ìŠµë‹ˆë‹¤ ã…‹        ë¬¸ì¬ì¸ ì •ë¶€\n",
       "3          íˆì´ì•¼            ë´¬ìš”\n",
       "4  ì €ë²ˆì— ëµŒ ê±° ê°™ì€ë°          ì­ˆë¼›ì­ˆë¼›"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_test = pd.DataFrame({\n",
    "    'sentence_1': ['ğŸ‘Œ', 'ê°¬ì„±ìˆë„¤', 'ë„µ ì•Œê² ìŠµë‹ˆë‹¤ ã…‹', 'íˆì´ì•¼', 'ì €ë²ˆì— ëµŒê±°ê°™ì€ë°'],\n",
    "    'sentence_2': ['ë§ì•œã…‹ã…‹ã…‹ã…‹ë­”ë°ìš¬ã…‹ã…‹ã…‹', 'ì§„ã…‰ ã…? ì •ë§ë¡œ?', 'ë¬¸ì¬ì¸ì •ë¶€', 'ë´¬ìš©', 'ì­ˆë¼›ì­ˆë¼›']\n",
    "})\n",
    "\n",
    "check_spell(local_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 2535.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ë§ì•œã…‹ã…‹</td>\n",
       "      <td>ë§ì•œã…‹ã…‹!!!!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì™€í•˜í•˜</td>\n",
       "      <td>ìº¬ìº¬</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_1        sentence_2\n",
       "0       ë§ì•œã…‹ã…‹  ë§ì•œã…‹ã…‹!!!!!!!!!!!!\n",
       "1        ì™€í•˜í•˜                ìº¬ìº¬"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use soynlp to correct spelling\n",
    "from soynlp.normalizer import repeat_normalize as r_n\n",
    "\n",
    "def repeat_normalize(dataframe):\n",
    "    for i in tqdm(range(len(dataframe['sentence_1'])), desc='repeat_normalize'):\n",
    "        dataframe.loc[i, 'sentence_1'] = r_n(dataframe.loc[i, 'sentence_1'], num_repeats=2)\n",
    "        dataframe.loc[i, 'sentence_2'] = r_n(dataframe.loc[i, 'sentence_2'], num_repeats=2)\n",
    "    return dataframe\n",
    "\n",
    "local_test = pd.DataFrame({\n",
    "    'sentence_1': ['ë§ì•œã…‹ã…‹ã…‹ã…‹', 'ì™€í•˜í•˜í•˜í•˜í•˜í•˜'],\n",
    "    'sentence_2': ['ë§ì•œã…‹ã…‹ã…‹ã…‹!!!!!!!!!!!!', 'ìº¬ìº¬ìº¬ìº¬ìº¬ìº¬ìº¬']\n",
    "})\n",
    "    \n",
    "repeat_normalize(local_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10974, 7)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # copy dataset\n",
    "# dataset_filter_1 = dataset.copy()\n",
    "# dataset_filter_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10974/10974 [09:26<00:00, 19.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10974/10974 [12:05<00:00, 15.13it/s]\n",
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10974/10974 [00:04<00:00, 2196.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# # apply all the functions\n",
    "# # dataset_filter_1 = delete_etcs(dataset_filter_1)\n",
    "# dataset_filter_1 = apply_spacing(dataset_filter_1)\n",
    "# dataset_filter_1 = check_spell(dataset_filter_1)\n",
    "# dataset_filter_1 = repeat_normalize(dataset_filter_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10974/10974 [00:06<00:00, 1809.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# data, tokens = tokenizing(tokenizer, dataset_filter_1)\n",
    "# dataset_filter_1['tokens'] = tokens\n",
    "\n",
    "# # find '[UNK]' token in dataset_filter_1['tokens'] and make a new df\n",
    "# unk_list = []\n",
    "# for i in range(len(dataset_filter_1['tokens'])):\n",
    "#     if '[UNK]' in dataset_filter_1['tokens'][i]:\n",
    "#         unk_list.append(i)\n",
    "        \n",
    "# unk_df = dataset_filter_1.loc[unk_list]\n",
    "\n",
    "# # make a txt file with unk_df 'tokens' data\n",
    "# with open('unk_filter_1.txt', 'w') as f:\n",
    "#     for i in range(len(unk_df)):\n",
    "#         f.write(str(unk_df['sentence_1'].values[i]) + ' @ ' + str(unk_df['sentence_2'].values[i]) + '\\n' + str(unk_df['tokens'].values[i]) + '\\n' + '****************************************************************************************************' + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINAL: preprocess train.csv to train_preprocessed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9324/9324 [07:59<00:00, 19.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9324/9324 [10:26<00:00, 14.87it/s]\n",
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9324/9324 [00:03<00:00, 2450.75it/s]\n",
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9324/9324 [08:03<00:00, 19.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9324/9324 [09:30<00:00, 16.33it/s]\n",
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9324/9324 [00:03<00:00, 2458.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# # load train.csv\n",
    "# train = pd.read_csv('train.csv')\n",
    "\n",
    "# # spacing\n",
    "# train_spacing = apply_spacing(train)\n",
    "# train_spacing.to_csv('train_spacing.csv', index=False)\n",
    "\n",
    "# # spell check\n",
    "# train_spell = check_spell(train)\n",
    "# train_spell.to_csv('train_spell.csv', index=False)\n",
    "\n",
    "# # repeat normalize\n",
    "# train_repeat = repeat_normalize(train)\n",
    "# train_repeat.to_csv('train_repeat.csv', index=False)\n",
    "\n",
    "# train_spacing_spell = apply_spacing(train)\n",
    "# train_spacing_spell = check_spell(train_spacing_spell)\n",
    "# train_spacing_spell.to_csv('train_spacing_spell.csv', index=False)\n",
    "\n",
    "# train_spacing_repeat = apply_spacing(train)\n",
    "# train_spacing_repeat = repeat_normalize(train_spacing_repeat)\n",
    "# train_spacing_repeat.to_csv('train_spacing_repeat.csv', index=False)\n",
    "\n",
    "# train_spell_repeat = check_spell(train)\n",
    "# train_spell_repeat = repeat_normalize(train_spell_repeat)\n",
    "# train_spell_repeat.to_csv('train_spell_repeat.csv', index=False)\n",
    "\n",
    "# train_preproc_all = apply_spacing(train)\n",
    "# train_preproc_all = check_spell(train_preproc_all)\n",
    "# train_preproc_all = repeat_normalize(train_preproc_all)\n",
    "# train_preproc_all.to_csv('train_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:27<00:00, 19.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:37<00:00, 14.77it/s]\n",
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:00<00:00, 3496.96it/s]\n",
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:28<00:00, 19.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:35<00:00, 15.38it/s]\n",
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:27<00:00, 19.79it/s]\n",
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:00<00:00, 3578.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:30<00:00, 18.13it/s]\n",
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:00<00:00, 3504.34it/s]\n",
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:27<00:00, 19.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:30<00:00, 17.98it/s]\n",
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 550/550 [00:00<00:00, 3439.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# dev = pd.read_csv('dev.csv')\n",
    "\n",
    "# dev_spacing = apply_spacing(dev)\n",
    "# dev_spacing.to_csv('dev_spacing.csv', index=False)\n",
    "\n",
    "# dev_spell = check_spell(dev)\n",
    "# dev_spell.to_csv('dev_spell.csv', index=False)\n",
    "\n",
    "# dev_repeat = repeat_normalize(dev)\n",
    "# dev_repeat.to_csv('dev_repeat.csv', index=False)\n",
    "\n",
    "# dev_spacing_spell = apply_spacing(dev)\n",
    "# dev_spacing_spell = check_spell(dev_spacing_spell)\n",
    "# dev_spacing_spell.to_csv('dev_spacing_spell.csv', index=False)\n",
    "\n",
    "# dev_spacing_repeat = apply_spacing(dev)\n",
    "# dev_spacing_repeat = repeat_normalize(dev_spacing_repeat)\n",
    "# dev_spacing_repeat.to_csv('dev_spacing_repeat.csv', index=False)\n",
    "\n",
    "# dev_spell_repeat = check_spell(dev)\n",
    "# dev_spell_repeat = repeat_normalize(dev_spell_repeat)\n",
    "# dev_spell_repeat.to_csv('dev_spell_repeat.csv', index=False)\n",
    "\n",
    "# dev_preproc_all = apply_spacing(dev)\n",
    "# dev_preproc_all = check_spell(dev_preproc_all)\n",
    "# dev_preproc_all = repeat_normalize(dev_preproc_all)\n",
    "# dev_preproc_all.to_csv('dev_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [00:55<00:00, 19.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [01:09<00:00, 15.93it/s]\n",
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [00:00<00:00, 4957.67it/s]\n",
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [00:55<00:00, 19.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: d97f0b07208e9fd39556aee101e1a171867d6ab4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [01:04<00:00, 17.03it/s]\n",
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [00:56<00:00, 19.55it/s]\n",
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [00:00<00:00, 4930.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: b7584e2dbf34edd27f75b6430787c04eb65feb52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [01:06<00:00, 16.55it/s]\n",
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [00:00<00:00, 4853.54it/s]\n",
      "apply_spacing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [01:00<00:00, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passportKey found: b7584e2dbf34edd27f75b6430787c04eb65feb52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "check_spell: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [01:05<00:00, 16.67it/s]\n",
      "repeat_normalize: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1100/1100 [00:00<00:00, 4954.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# test = pd.read_csv('test.csv')\n",
    "\n",
    "# test_spacing = apply_spacing(test)\n",
    "# test_spacing.to_csv('test_spacing.csv', index=False)\n",
    "\n",
    "# test_spell = check_spell(test)\n",
    "# test_spell.to_csv('test_spell.csv', index=False)\n",
    "\n",
    "# test_repeat = repeat_normalize(test)\n",
    "# test_repeat.to_csv('test_repeat.csv', index=False)\n",
    "\n",
    "# test_spacing_spell = apply_spacing(test)\n",
    "# test_spacing_spell = check_spell(test_spacing_spell)\n",
    "# test_spacing_spell.to_csv('test_spacing_spell.csv', index=False)\n",
    "\n",
    "# test_spacing_repeat = apply_spacing(test)\n",
    "# test_spacing_repeat = repeat_normalize(test_spacing_repeat)\n",
    "# test_spacing_repeat.to_csv('test_spacing_repeat.csv', index=False)\n",
    "\n",
    "# test_spell_repeat = check_spell(test)\n",
    "# test_spell_repeat = repeat_normalize(test_spell_repeat)\n",
    "# test_spell_repeat.to_csv('test_spell_repeat.csv', index=False)\n",
    "\n",
    "# test_preproc_all = apply_spacing(test)\n",
    "# test_preproc_all = check_spell(test_preproc_all)\n",
    "# test_preproc_all = repeat_normalize(test_preproc_all)\n",
    "# test_preproc_all.to_csv('test_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizer: add_token() -> add <PERSON>, <ADDRESS> tokens\n",
    "# tokenizer.add_tokens(['<PERSON>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_slang(text):\n",
    "#     # ì¤„ì„ë§ -> ì •ì‹ í‘œí˜„ìœ¼ë¡œ ë³€í™˜\n",
    "#     slang_dict = {\n",
    "#         'ë„µ': 'ë„¤',\n",
    "#         ' í›… ': ' ê°‘ìê¸° '\n",
    "#     }\n",
    "\n",
    "#     # ì¤„ì„ë§ ì‚¬ì „ ê¸°ë°˜ìœ¼ë¡œ ì •ê·œí™”\n",
    "#     for slang, formal in slang_dict.items():\n",
    "#         text = text.replace(slang, formal)\n",
    "    \n",
    "#     return text\n",
    "\n",
    "# # ì˜ˆì‹œ ë¬¸ì¥\n",
    "# text = \"ë„˜ ì¡¸ê·€íƒ± ã…‹ã…‹ ì§„ì§œ ì§±ì´ì•¼ ã… ã… \"\n",
    "\n",
    "# # ì¤„ì„ë§ ì •ê·œí™” ì ìš©\n",
    "# normalized_text = normalize_slang(text)\n",
    "# print(normalized_text)  # \"ë„ˆë¬´ ì¡¸ë¼ ê·€ì—½ë‹¤ ì›ƒìŒ ì§„ì§œ ì •ë§ì´ì•¼ ìŠ¬í””\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
