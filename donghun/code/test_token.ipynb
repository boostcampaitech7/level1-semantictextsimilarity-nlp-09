{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/miniconda3/envs/hun_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boostcamp-sts-v1-train-000</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~</td>\n",
       "      <td>반전도 있고,사랑도 있고재미도있네요.</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-train-001</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>앗 제가 접근권한이 없다고 뜹니다;;</td>\n",
       "      <td>오, 액세스 권한이 없다고 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boostcamp-sts-v1-train-002</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>주택청약조건 변경해주세요.</td>\n",
       "      <td>주택청약 무주택기준 변경해주세요.</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boostcamp-sts-v1-train-003</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>입사후 처음 대면으로 만나 반가웠습니다.</td>\n",
       "      <td>화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boostcamp-sts-v1-train-004</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>뿌듯뿌듯 하네요!!</td>\n",
       "      <td>꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id            source  \\\n",
       "0  boostcamp-sts-v1-train-000      nsmc-sampled   \n",
       "1  boostcamp-sts-v1-train-001         slack-rtt   \n",
       "2  boostcamp-sts-v1-train-002  petition-sampled   \n",
       "3  boostcamp-sts-v1-train-003     slack-sampled   \n",
       "4  boostcamp-sts-v1-train-004     slack-sampled   \n",
       "\n",
       "                               sentence_1                    sentence_2  \\\n",
       "0  스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~          반전도 있고,사랑도 있고재미도있네요.   \n",
       "1                    앗 제가 접근권한이 없다고 뜹니다;;           오, 액세스 권한이 없다고 합니다.   \n",
       "2                          주택청약조건 변경해주세요.            주택청약 무주택기준 변경해주세요.   \n",
       "3                  입사후 처음 대면으로 만나 반가웠습니다.  화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.   \n",
       "4                              뿌듯뿌듯 하네요!!         꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!   \n",
       "\n",
       "   label  binary-label  \n",
       "0    2.2           0.0  \n",
       "1    4.2           1.0  \n",
       "2    2.4           0.0  \n",
       "3    3.0           1.0  \n",
       "4    0.0           0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/miniconda3/envs/hun_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at snunlp/KR-ELECTRA-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"snunlp/KR-ELECTRA-discriminator\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, max_length=160)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraTokenizerFast(name_or_path='snunlp/KR-ELECTRA-discriminator', vocab_size=30000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElectraForSequenceClassification(\n",
       "  (electra): ElectraModel(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): ElectraClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): GELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testText = df['sentence_1'][0]\n",
    "testText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['스릴',\n",
       " '##도',\n",
       " '##있',\n",
       " '##고',\n",
       " '반전',\n",
       " '##도',\n",
       " '있',\n",
       " '##고',\n",
       " '여느',\n",
       " '한국',\n",
       " '##영화',\n",
       " '쓰레기',\n",
       " '##들',\n",
       " '##하고',\n",
       " '##는',\n",
       " '차원',\n",
       " '##이',\n",
       " '다르',\n",
       " '##네요',\n",
       " '~']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(testText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets=[]):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    # 학습 및 추론 과정에서 데이터를 1개씩 꺼내오는 곳\n",
    "    def __getitem__(self, idx):\n",
    "        # 정답이 있다면 else문을, 없다면 if문을 수행합니다\n",
    "        if len(self.targets) == 0:\n",
    "            return torch.tensor(self.inputs[idx])\n",
    "        else:\n",
    "            return torch.tensor(self.inputs[idx]), torch.tensor(self.targets[idx])\n",
    "\n",
    "    # 입력하는 개수만큼 데이터를 사용합니다\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader(pl.LightningDataModule):\n",
    "    def __init__(self, model_name, batch_size, shuffle, train_path, dev_path, test_path, predict_path):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.train_path = train_path\n",
    "        self.dev_path = dev_path\n",
    "        self.test_path = test_path\n",
    "        self.predict_path = predict_path\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.predict_dataset = None\n",
    "\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, max_length=160)\n",
    "        self.target_columns = ['label']\n",
    "        self.delete_columns = ['id']\n",
    "        self.text_columns = ['sentence_1', 'sentence_2']\n",
    "\n",
    "    def tokenizing(self, dataframe):\n",
    "        data = []\n",
    "        for idx, item in tqdm(dataframe.iterrows(), desc='tokenizing', total=len(dataframe)):\n",
    "            # 두 입력 문장을 [SEP] 토큰으로 이어붙여서 전처리합니다.\n",
    "            text = '[SEP]'.join([item[text_column] for text_column in self.text_columns])\n",
    "            outputs = self.tokenizer(text, add_special_tokens=True, padding='max_length', truncation=True)\n",
    "            data.append(outputs['input_ids'])\n",
    "            print(text, outputs,sep='\\n')\n",
    "        return data\n",
    "\n",
    "    def preprocessing(self, data):\n",
    "        # 안쓰는 컬럼을 삭제합니다.\n",
    "        data = data.drop(columns=self.delete_columns)\n",
    "\n",
    "        # 타겟 데이터가 없으면 빈 배열을 리턴합니다.\n",
    "        try:\n",
    "            targets = data[self.target_columns].values.tolist()\n",
    "        except:\n",
    "            targets = []\n",
    "        # 텍스트 데이터를 전처리합니다.\n",
    "        inputs = self.tokenizing(data)\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "    def setup(self, stage='fit'):\n",
    "        if stage == 'fit':\n",
    "            # 학습 데이터와 검증 데이터셋을 호출합니다\n",
    "            train_data = pd.read_csv(self.train_path)\n",
    "            val_data = pd.read_csv(self.dev_path)\n",
    "\n",
    "            # 학습데이터 준비\n",
    "            train_inputs, train_targets = self.preprocessing(train_data)\n",
    "\n",
    "            # 검증데이터 준비\n",
    "            val_inputs, val_targets = self.preprocessing(val_data)\n",
    "\n",
    "            # train 데이터만 shuffle을 적용해줍니다, 필요하다면 val, test 데이터에도 shuffle을 적용할 수 있습니다\n",
    "            self.train_dataset = Dataset(train_inputs, train_targets)\n",
    "            self.val_dataset = Dataset(val_inputs, val_targets)\n",
    "        else:\n",
    "            # 평가데이터 준비\n",
    "            test_data = pd.read_csv(self.test_path)\n",
    "            test_inputs, test_targets = self.preprocessing(test_data)\n",
    "            self.test_dataset = Dataset(test_inputs, test_targets)\n",
    "\n",
    "            predict_data = pd.read_csv(self.predict_path)\n",
    "            predict_inputs, predict_targets = self.preprocessing(predict_data)\n",
    "            self.predict_dataset = Dataset(predict_inputs, [])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=args.shuffle)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.predict_dataset, batch_size=self.batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/miniconda3/envs/hun_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloader = Dataloader(\"google-t5/t5-base\", 16, True, None, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3, 45, 5005, 17732, 90, 24703, 86, 15472, 17460, 10920, 18, 3]\n"
     ]
    }
   ],
   "source": [
    "text = \"[CLS] [UNK] [SEP] It is very rewarding. [SEP]\"\n",
    "token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing:   0%|          | 0/10 [00:00<?, ?it/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "tokenizing: 100%|██████████| 10/10 [00:00<00:00, 2095.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~[SEP]반전도 있고,사랑도 있고재미도있네요.\n",
      "{'input_ids': [3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 6306, 134, 8569, 908, 2, 3, 2, 6, 2, 3, 2, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "앗 제가 접근권한이 없다고 뜹니다;;[SEP]오, 액세스 권한이 없다고 합니다.\n",
      "{'input_ids': [3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 117, 117, 6306, 134, 8569, 908, 2, 6, 3, 2, 3, 2, 3, 2, 3, 2, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "주택청약조건 변경해주세요.[SEP]주택청약 무주택기준 변경해주세요.\n",
      "{'input_ids': [3, 2, 3, 2, 5, 6306, 134, 8569, 908, 2, 3, 2, 3, 2, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "입사후 처음 대면으로 만나 반가웠습니다.[SEP]화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.\n",
      "{'input_ids': [3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 5, 6306, 134, 8569, 908, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "뿌듯뿌듯 하네요!![SEP]꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!\n",
      "{'input_ids': [3, 2, 3, 2, 1603, 6306, 134, 8569, 908, 2, 3, 2, 3, 2, 3, 2, 3, 2, 55, 2, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "오마이가뜨지져스크롸이스트휏[SEP]오 마이 갓 지저스 스크론 이스트 팬\n",
      "{'input_ids': [3, 2, 6306, 134, 8569, 908, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "전 암만 찍어도 까만 하늘.. ㅠㅠ[SEP]암만 찍어도 하늘은 까맣다.. ㅠㅠ\n",
      "{'input_ids': [3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 5, 5, 3, 2, 6306, 134, 8569, 908, 2, 3, 2, 3, 2, 3, 2, 5, 5, 3, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "이렇게 귀여운 쥐들은 처음이네요.ㅎㅎㅎ[SEP]이렇게 지겨운 공포영화는 처음..\n",
      "{'input_ids': [3, 2, 3, 2, 3, 2, 3, 2, 5, 2, 6306, 134, 8569, 908, 2, 3, 2, 3, 2, 3, 2, 5, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "미세먼지 해결이 가장 시급한 문제입니다![SEP]가장 시급한 것이 신생아실 관리입니다!!!\n",
      "{'input_ids': [3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 55, 6306, 134, 8569, 908, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3158, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "크림하우스 환불조치해주세요.[SEP]크림하우스 환불조치할 수 있도록해주세여\n",
      "{'input_ids': [3, 2, 3, 2, 5, 6306, 134, 8569, 908, 2, 3, 2, 3, 2, 3, 2, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  6306,\n",
       "  134,\n",
       "  8569,\n",
       "  908,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  6,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  1],\n",
       " [3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  117,\n",
       "  117,\n",
       "  6306,\n",
       "  134,\n",
       "  8569,\n",
       "  908,\n",
       "  2,\n",
       "  6,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  1],\n",
       " [3, 2, 3, 2, 5, 6306, 134, 8569, 908, 2, 3, 2, 3, 2, 5, 1],\n",
       " [3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  6306,\n",
       "  134,\n",
       "  8569,\n",
       "  908,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  1],\n",
       " [3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  1603,\n",
       "  6306,\n",
       "  134,\n",
       "  8569,\n",
       "  908,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  55,\n",
       "  2,\n",
       "  55,\n",
       "  1],\n",
       " [3, 2, 6306, 134, 8569, 908, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 1],\n",
       " [3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  3,\n",
       "  2,\n",
       "  6306,\n",
       "  134,\n",
       "  8569,\n",
       "  908,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  3,\n",
       "  2,\n",
       "  1],\n",
       " [3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  2,\n",
       "  6306,\n",
       "  134,\n",
       "  8569,\n",
       "  908,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  5,\n",
       "  5,\n",
       "  1],\n",
       " [3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  55,\n",
       "  6306,\n",
       "  134,\n",
       "  8569,\n",
       "  908,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  3158,\n",
       "  1],\n",
       " [3, 2, 3, 2, 5, 6306, 134, 8569, 908, 2, 3, 2, 3, 2, 3, 2, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.tokenizing(df[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
